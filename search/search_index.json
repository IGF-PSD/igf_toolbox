{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"IGF python toolbox","text":""},{"location":"#objectives","title":"Objectives","text":"<p>This package contains a set of Python utilities for :</p> <ul> <li>easy data loading ;</li> <li>descriptive statistics iteration ;</li> <li>implementation of econometric methods.</li> </ul>"},{"location":"#organization","title":"Organization","text":"<p>The repository is organized as follow :</p> <ul> <li>the <code>s3</code> module contains a set of functions for importing or exporting files in various formats from or to an s3 bucket on the Nubonyxia platform;</li> <li>the <code>stats_des</code> module contains functions to complement those integrated by default in pandas for descriptive statistics (addition of totals, weighted statistics, verification of statistical confidentiality, etc.).</li> <li>the <code>preprocessing</code> module contains a set of classes that can be integrated into a <code>sklearn.pipeline</code> to perform various data transformation operations</li> <li>the <code>model_selection</code> module contains functions for training a prediction model or estimating a regression model</li> <li>the <code>estimators</code> module contains  econometric models that can be integrated into a <code>sklearn.pipeline</code></li> <li>the <code>utils</code> module contains  a set of utility functions on which other functions in this module depend, or for calculating weighted statistics, for example</li> </ul>"},{"location":"#installation","title":"Installation","text":""},{"location":"#package-and-dependencies","title":"Package and dependencies","text":"<pre><code>git clone &lt;repo_url&gt;\npip install -e igf_toolbox\n</code></pre> <p>The package can then be used like any other Python package.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>To visualize the documentation :</p> <pre><code>mkdocs build --port 5000\n</code></pre>"},{"location":"#license","title":"License","text":"<p>The package is licensed under the MIT License.</p>"},{"location":"api/AddConstante/","title":"AddConstante","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>Add a constant column to a DataFrame.</p> <p>This transformer appends a constant column named 'Constante' with value 1 to a DataFrame. It can be useful in certain statistical models where an intercept term is needed.</p> <p>Methods: - fit(X, y=None): Returns self. - transform(X, y=None): Adds the constant column to the data. - fit_transform(X, y=None): Fit to data and then transform it.</p> <p>Example:</p> <p>adder = AddConstante() df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}) df_transformed = adder.transform(df) print(df_transformed)    A  B  Constante 0  1  4          1 1  2  5          1 2  3  6          1</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>class AddConstante(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Add a constant column to a DataFrame.\n\n    This transformer appends a constant column named 'Constante' with value 1 to a DataFrame.\n    It can be useful in certain statistical models where an intercept term is needed.\n\n    Methods:\n    - fit(X, y=None): Returns self.\n    - transform(X, y=None): Adds the constant column to the data.\n    - fit_transform(X, y=None): Fit to data and then transform it.\n\n    Example:\n    &gt;&gt;&gt; adder = AddConstante()\n    &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n    &gt;&gt;&gt; df_transformed = adder.transform(df)\n    &gt;&gt;&gt; print(df_transformed)\n       A  B  Constante\n    0  1  4          1\n    1  2  5          1\n    2  3  6          1\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Initialize the AddConstante transformer.\n        \"\"\"\n        pass\n\n    def fit(self, X, y=None) -&gt; None:\n        \"\"\"\n        Return self.\n\n        The fit method is implemented for compatibility with sklearn's TransformerMixin,\n        but doesn't perform any actual computation.\n\n        Parameters:\n        - X (pd.DataFrame): The input data. Not used, only needed for compatibility.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - self: The instance itself.\n        \"\"\"\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Add a constant column to the input DataFrame.\n\n        Parameters:\n        - X (pd.DataFrame): The input data to transform.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - pd.DataFrame: The transformed data with an additional constant column.\n        \"\"\"\n        data_res = X.copy()\n        data_res[\"Constante\"] = 1\n\n        return data_res\n\n    def fit_transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Fit to data and then transform it.\n\n        Parameters:\n        - X (pd.DataFrame): The input data to transform.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - pd.DataFrame: The transformed data.\n        \"\"\"\n        self.fit(X=X, y=y)\n        return self.transform(X=X, y=y)\n</code></pre>"},{"location":"api/AddConstante/#igf_toolbox.preprocessing.transformers.AddConstante.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the AddConstante transformer.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initialize the AddConstante transformer.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/AddConstante/#igf_toolbox.preprocessing.transformers.AddConstante.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Return self.</p> <p>The fit method is implemented for compatibility with sklearn's TransformerMixin, but doesn't perform any actual computation.</p> <p>Parameters: - X (pd.DataFrame): The input data. Not used, only needed for compatibility. - y (ignored): This parameter is ignored.</p> <p>Returns: - self: The instance itself.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def fit(self, X, y=None) -&gt; None:\n    \"\"\"\n    Return self.\n\n    The fit method is implemented for compatibility with sklearn's TransformerMixin,\n    but doesn't perform any actual computation.\n\n    Parameters:\n    - X (pd.DataFrame): The input data. Not used, only needed for compatibility.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - self: The instance itself.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/AddConstante/#igf_toolbox.preprocessing.transformers.AddConstante.fit_transform","title":"<code>fit_transform(X, y=None)</code>","text":"<p>Fit to data and then transform it.</p> <p>Parameters: - X (pd.DataFrame): The input data to transform. - y (ignored): This parameter is ignored.</p> <p>Returns: - pd.DataFrame: The transformed data.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def fit_transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Fit to data and then transform it.\n\n    Parameters:\n    - X (pd.DataFrame): The input data to transform.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - pd.DataFrame: The transformed data.\n    \"\"\"\n    self.fit(X=X, y=y)\n    return self.transform(X=X, y=y)\n</code></pre>"},{"location":"api/AddConstante/#igf_toolbox.preprocessing.transformers.AddConstante.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Add a constant column to the input DataFrame.</p> <p>Parameters: - X (pd.DataFrame): The input data to transform. - y (ignored): This parameter is ignored.</p> <p>Returns: - pd.DataFrame: The transformed data with an additional constant column.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Add a constant column to the input DataFrame.\n\n    Parameters:\n    - X (pd.DataFrame): The input data to transform.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - pd.DataFrame: The transformed data with an additional constant column.\n    \"\"\"\n    data_res = X.copy()\n    data_res[\"Constante\"] = 1\n\n    return data_res\n</code></pre>"},{"location":"api/AddFixedEffect/","title":"AddFixedEffect","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>Add fixed effect dummy variables to a DataFrame.</p> <p>This transformer appends dummy variables derived from a categorical column specified by <code>x_categorie</code> to the DataFrame. If the <code>no_trap</code> parameter is provided, the specified columns will be dropped to avoid the dummy variable trap.</p> <p>Attributes: - x_categorie (str): Name of the column from which to generate the dummy variables. - no_trap (list or str, optional): Name(s) of the dummy column(s) to drop for avoiding dummy variable trap.</p> <p>Methods: - fit(X, y=None): Returns self. - transform(X, y=None): Adds the dummy columns derived from <code>x_categorie</code> to the data. - fit_transform(X, y=None): Fit to data and then transform it.</p> <p>Example:</p> <p>transformer = AddFixedEffect(x_categorie='Color', no_trap='Red') df = pd.DataFrame({'Color': ['Red', 'Green', 'Blue'], 'Value': [1, 2, 3]}) df_transformed = transformer.transform(df) print(df_transformed)    Color  Value  Green  Blue 0    Red      1      0     0 1  Green      2      1     0 2   Blue      3      0     1</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>class AddFixedEffect(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Add fixed effect dummy variables to a DataFrame.\n\n    This transformer appends dummy variables derived from a categorical column specified by `x_categorie`\n    to the DataFrame. If the `no_trap` parameter is provided, the specified columns will be dropped\n    to avoid the dummy variable trap.\n\n    Attributes:\n    - x_categorie (str): Name of the column from which to generate the dummy variables.\n    - no_trap (list or str, optional): Name(s) of the dummy column(s) to drop for avoiding dummy variable trap.\n\n    Methods:\n    - fit(X, y=None): Returns self.\n    - transform(X, y=None): Adds the dummy columns derived from `x_categorie` to the data.\n    - fit_transform(X, y=None): Fit to data and then transform it.\n\n    Example:\n    &gt;&gt;&gt; transformer = AddFixedEffect(x_categorie='Color', no_trap='Red')\n    &gt;&gt;&gt; df = pd.DataFrame({'Color': ['Red', 'Green', 'Blue'], 'Value': [1, 2, 3]})\n    &gt;&gt;&gt; df_transformed = transformer.transform(df)\n    &gt;&gt;&gt; print(df_transformed)\n       Color  Value  Green  Blue\n    0    Red      1      0     0\n    1  Green      2      1     0\n    2   Blue      3      0     1\n    \"\"\"\n\n    def __init__(\n        self, x_categorie: str, no_trap: Optional[Union[str, List[str], None]] = None\n    ) -&gt; None:\n        \"\"\"\n        Initialize the AddFixedEffect transformer.\n\n        Parameters:\n        - x_categorie (str): The categorical column to convert into dummy variables.\n        - no_trap (list or str, optional): Dummy column(s) to drop to avoid the dummy variable trap.\n        \"\"\"\n        # Initialisation des valeurs des param\u00e8tres\n        self.x_categorie = x_categorie\n        self.no_trap = no_trap\n\n    def fit(self, X, y=None) -&gt; None:\n        \"\"\"\n        Return self.\n\n        The fit method is implemented for compatibility with sklearn's TransformerMixin,\n        but doesn't perform any actual computation.\n\n        Parameters:\n        - X (pd.DataFrame): The input data. Not used, only needed for compatibility.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - self: The instance itself.\n        \"\"\"\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Add dummy variables to the input DataFrame.\n\n        This method converts the specified `x_categorie` column to dummy variables and appends them to\n        the input DataFrame. If the `no_trap` attribute is set, the specified dummy column(s) will be dropped.\n\n        Parameters:\n        - X (pd.DataFrame): The input data to transform.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - pd.DataFrame: The transformed data with added dummy variables.\n        \"\"\"\n        # Cr\u00e9ation et ajout du jeu de donn\u00e9es d'indicatrices\n        data_res = pd.concat(\n            [X, pd.get_dummies(data=X[self.x_categorie])], axis=1, join=\"outer\"\n        )  # .drop(self.x_categorie, axis=1)\n\n        # Supression des colonnes superflues\n        if self.no_trap is not None:\n            data_res.drop(self.no_trap, axis=1, inplace=True)\n\n        return data_res\n\n    def fit_transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Fit to data and then transform it.\n\n        Parameters:\n        - X (pd.DataFrame): The input data to transform.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - pd.DataFrame: The transformed data.\n        \"\"\"\n        self.fit(X=X, y=y)\n        return self.transform(X=X, y=y)\n</code></pre>"},{"location":"api/AddFixedEffect/#igf_toolbox.preprocessing.transformers.AddFixedEffect.__init__","title":"<code>__init__(x_categorie, no_trap=None)</code>","text":"<p>Initialize the AddFixedEffect transformer.</p> <p>Parameters: - x_categorie (str): The categorical column to convert into dummy variables. - no_trap (list or str, optional): Dummy column(s) to drop to avoid the dummy variable trap.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def __init__(\n    self, x_categorie: str, no_trap: Optional[Union[str, List[str], None]] = None\n) -&gt; None:\n    \"\"\"\n    Initialize the AddFixedEffect transformer.\n\n    Parameters:\n    - x_categorie (str): The categorical column to convert into dummy variables.\n    - no_trap (list or str, optional): Dummy column(s) to drop to avoid the dummy variable trap.\n    \"\"\"\n    # Initialisation des valeurs des param\u00e8tres\n    self.x_categorie = x_categorie\n    self.no_trap = no_trap\n</code></pre>"},{"location":"api/AddFixedEffect/#igf_toolbox.preprocessing.transformers.AddFixedEffect.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Return self.</p> <p>The fit method is implemented for compatibility with sklearn's TransformerMixin, but doesn't perform any actual computation.</p> <p>Parameters: - X (pd.DataFrame): The input data. Not used, only needed for compatibility. - y (ignored): This parameter is ignored.</p> <p>Returns: - self: The instance itself.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def fit(self, X, y=None) -&gt; None:\n    \"\"\"\n    Return self.\n\n    The fit method is implemented for compatibility with sklearn's TransformerMixin,\n    but doesn't perform any actual computation.\n\n    Parameters:\n    - X (pd.DataFrame): The input data. Not used, only needed for compatibility.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - self: The instance itself.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/AddFixedEffect/#igf_toolbox.preprocessing.transformers.AddFixedEffect.fit_transform","title":"<code>fit_transform(X, y=None)</code>","text":"<p>Fit to data and then transform it.</p> <p>Parameters: - X (pd.DataFrame): The input data to transform. - y (ignored): This parameter is ignored.</p> <p>Returns: - pd.DataFrame: The transformed data.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def fit_transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Fit to data and then transform it.\n\n    Parameters:\n    - X (pd.DataFrame): The input data to transform.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - pd.DataFrame: The transformed data.\n    \"\"\"\n    self.fit(X=X, y=y)\n    return self.transform(X=X, y=y)\n</code></pre>"},{"location":"api/AddFixedEffect/#igf_toolbox.preprocessing.transformers.AddFixedEffect.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Add dummy variables to the input DataFrame.</p> <p>This method converts the specified <code>x_categorie</code> column to dummy variables and appends them to the input DataFrame. If the <code>no_trap</code> attribute is set, the specified dummy column(s) will be dropped.</p> <p>Parameters: - X (pd.DataFrame): The input data to transform. - y (ignored): This parameter is ignored.</p> <p>Returns: - pd.DataFrame: The transformed data with added dummy variables.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Add dummy variables to the input DataFrame.\n\n    This method converts the specified `x_categorie` column to dummy variables and appends them to\n    the input DataFrame. If the `no_trap` attribute is set, the specified dummy column(s) will be dropped.\n\n    Parameters:\n    - X (pd.DataFrame): The input data to transform.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - pd.DataFrame: The transformed data with added dummy variables.\n    \"\"\"\n    # Cr\u00e9ation et ajout du jeu de donn\u00e9es d'indicatrices\n    data_res = pd.concat(\n        [X, pd.get_dummies(data=X[self.x_categorie])], axis=1, join=\"outer\"\n    )  # .drop(self.x_categorie, axis=1)\n\n    # Supression des colonnes superflues\n    if self.no_trap is not None:\n        data_res.drop(self.no_trap, axis=1, inplace=True)\n\n    return data_res\n</code></pre>"},{"location":"api/AddInteraction/","title":"AddInteraction","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>Add interaction variables to a DataFrame based on the specified categorical columns.</p> <p>This transformer creates interaction dummy variables based on the given list of categorical columns. If the <code>list_no_trap</code> parameter is provided, the specified columns will be dropped to avoid the dummy variable trap.</p> <p>Attributes: - list_x_categorie (list): List of columns from which to generate the interaction dummy variables. - list_no_trap (list, optional): List of names of the dummy columns to drop for avoiding dummy variable trap.</p> <p>Methods: - fit(X, y=None): Returns self. - transform(X, y=None): Adds interaction dummy variables derived from <code>list_x_categorie</code> to the data. - fit_transform(X, y=None): Fit to data and then transform it.</p> <p>Example:</p> <p>transformer = AddInteraction(list_x_categorie=['Color', 'Shape'], list_no_trap=['Red']) df = pd.DataFrame({'Color': ['Red', 'Green'], 'Shape': ['Circle', 'Square'], 'Value': [1, 2]}) df_transformed = transformer.transform(df) print(df_transformed)    Color   Shape  Value  Green - Circle  Green - Square 0    Red  Circle      1              0               0 1  Green  Square      2              0               1</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>class AddInteraction(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Add interaction variables to a DataFrame based on the specified categorical columns.\n\n    This transformer creates interaction dummy variables based on the given list of categorical columns.\n    If the `list_no_trap` parameter is provided, the specified columns will be dropped\n    to avoid the dummy variable trap.\n\n    Attributes:\n    - list_x_categorie (list): List of columns from which to generate the interaction dummy variables.\n    - list_no_trap (list, optional): List of names of the dummy columns to drop for avoiding dummy variable trap.\n\n    Methods:\n    - fit(X, y=None): Returns self.\n    - transform(X, y=None): Adds interaction dummy variables derived from `list_x_categorie` to the data.\n    - fit_transform(X, y=None): Fit to data and then transform it.\n\n    Example:\n    &gt;&gt;&gt; transformer = AddInteraction(list_x_categorie=['Color', 'Shape'], list_no_trap=['Red'])\n    &gt;&gt;&gt; df = pd.DataFrame({'Color': ['Red', 'Green'], 'Shape': ['Circle', 'Square'], 'Value': [1, 2]})\n    &gt;&gt;&gt; df_transformed = transformer.transform(df)\n    &gt;&gt;&gt; print(df_transformed)\n       Color   Shape  Value  Green - Circle  Green - Square\n    0    Red  Circle      1              0               0\n    1  Green  Square      2              0               1\n    \"\"\"\n\n    def __init__(\n        self,\n        list_x_categorie: List[str],\n        list_no_trap: Optional[Union[List[str], None]] = None,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the AddInteraction transformer.\n\n        Parameters:\n        - list_x_categorie (list): The list of categorical columns to convert into interaction dummy variables.\n        - list_no_trap (list, optional): Dummy columns to drop to avoid the dummy variable trap.\n        \"\"\"\n        # Initialisation des valeurs des param\u00e8tres\n        # Variables concern\u00e9es\n        self.list_x_categorie = list_x_categorie\n        # Liste \u00e9num\u00e9rant les modalit\u00e9s \u00e0 retirer\n        self.list_no_trap = list_no_trap\n\n    def fit(self, X, y=None) -&gt; None:\n        \"\"\"\n        Return self.\n\n        The fit method is implemented for compatibility with sklearn's TransformerMixin,\n        but doesn't perform any actual computation.\n\n        Parameters:\n        - X (pd.DataFrame): The input data. Not used, only needed for compatibility.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - self: The instance itself.\n        \"\"\"\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Add interaction dummy variables to the input DataFrame.\n\n        This method converts the specified columns in `list_x_categorie` into interaction dummy variables\n        and appends them to the input DataFrame. If the `list_no_trap` attribute is set,\n        the specified dummy column(s) will be dropped.\n\n        Parameters:\n        - X (pd.DataFrame): The input data to transform.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - pd.DataFrame: The transformed data with added interaction dummy variables.\n        \"\"\"\n        # Cr\u00e9ation et ajout du jeu de donn\u00e9es d'indicatrices\n        data_res = pd.concat(\n            [X, pd.get_dummies(data=X[self.list_x_categorie], prefix=\"interact\")],\n            axis=1,\n            join=\"outer\",\n        )  # .drop(self.list_x_categorie, axis=1)\n\n        # Supression des colonnes superflues\n        if self.list_no_trap is not None:\n            data_res.drop(\n                [\"interact_\" + no_trap for no_trap in self.list_no_trap],\n                axis=1,\n                inplace=True,\n            )\n\n        # Combinaison des indicatrices et cr\u00e9ation des int\u00e9ractions\n\n        # Cr\u00e9ation des combinaisons possibles\n        list_modalite = [\n            np.setdiff1d(X[x_categorie].unique(), self.list_no_trap).tolist()\n            for x_categorie in self.list_x_categorie\n        ]\n        list_combinaison = list(product(*list_modalite))\n\n        # Ajout des interactions\n        for combinaison in list_combinaison:\n            # Nom de la colonne d'int\u00e9raction\n            nom_col = \" - \".join(combinaison)\n            # Initialisation de la colonne d'interaction\n            data_res[nom_col] = 1\n            for modalite in combinaison:\n                # Mise \u00e0 jour de la variable d'interaction\n                data_res[nom_col] *= data_res[\"interact_\" + modalite]\n\n        # Suppression des indicatrices non interagies\n        list_col_drop = [\n            \"interact_\" + col\n            for col in np.unique(\n                np.setdiff1d(\n                    X[self.list_x_categorie].values.reshape(-1), self.list_no_trap\n                )\n            )\n        ]\n        data_res.drop(list_col_drop, axis=1, inplace=True)\n\n        return data_res\n\n    def fit_transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Fit to data and then transform it.\n\n        Parameters:\n        - X (pd.DataFrame): The input data to transform.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - pd.DataFrame: The transformed data.\n        \"\"\"\n        self.fit(X=X, y=y)\n        return self.transform(X=X, y=y)\n</code></pre>"},{"location":"api/AddInteraction/#igf_toolbox.preprocessing.transformers.AddInteraction.__init__","title":"<code>__init__(list_x_categorie, list_no_trap=None)</code>","text":"<p>Initialize the AddInteraction transformer.</p> <p>Parameters: - list_x_categorie (list): The list of categorical columns to convert into interaction dummy variables. - list_no_trap (list, optional): Dummy columns to drop to avoid the dummy variable trap.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def __init__(\n    self,\n    list_x_categorie: List[str],\n    list_no_trap: Optional[Union[List[str], None]] = None,\n) -&gt; None:\n    \"\"\"\n    Initialize the AddInteraction transformer.\n\n    Parameters:\n    - list_x_categorie (list): The list of categorical columns to convert into interaction dummy variables.\n    - list_no_trap (list, optional): Dummy columns to drop to avoid the dummy variable trap.\n    \"\"\"\n    # Initialisation des valeurs des param\u00e8tres\n    # Variables concern\u00e9es\n    self.list_x_categorie = list_x_categorie\n    # Liste \u00e9num\u00e9rant les modalit\u00e9s \u00e0 retirer\n    self.list_no_trap = list_no_trap\n</code></pre>"},{"location":"api/AddInteraction/#igf_toolbox.preprocessing.transformers.AddInteraction.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Return self.</p> <p>The fit method is implemented for compatibility with sklearn's TransformerMixin, but doesn't perform any actual computation.</p> <p>Parameters: - X (pd.DataFrame): The input data. Not used, only needed for compatibility. - y (ignored): This parameter is ignored.</p> <p>Returns: - self: The instance itself.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def fit(self, X, y=None) -&gt; None:\n    \"\"\"\n    Return self.\n\n    The fit method is implemented for compatibility with sklearn's TransformerMixin,\n    but doesn't perform any actual computation.\n\n    Parameters:\n    - X (pd.DataFrame): The input data. Not used, only needed for compatibility.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - self: The instance itself.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/AddInteraction/#igf_toolbox.preprocessing.transformers.AddInteraction.fit_transform","title":"<code>fit_transform(X, y=None)</code>","text":"<p>Fit to data and then transform it.</p> <p>Parameters: - X (pd.DataFrame): The input data to transform. - y (ignored): This parameter is ignored.</p> <p>Returns: - pd.DataFrame: The transformed data.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def fit_transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Fit to data and then transform it.\n\n    Parameters:\n    - X (pd.DataFrame): The input data to transform.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - pd.DataFrame: The transformed data.\n    \"\"\"\n    self.fit(X=X, y=y)\n    return self.transform(X=X, y=y)\n</code></pre>"},{"location":"api/AddInteraction/#igf_toolbox.preprocessing.transformers.AddInteraction.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Add interaction dummy variables to the input DataFrame.</p> <p>This method converts the specified columns in <code>list_x_categorie</code> into interaction dummy variables and appends them to the input DataFrame. If the <code>list_no_trap</code> attribute is set, the specified dummy column(s) will be dropped.</p> <p>Parameters: - X (pd.DataFrame): The input data to transform. - y (ignored): This parameter is ignored.</p> <p>Returns: - pd.DataFrame: The transformed data with added interaction dummy variables.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Add interaction dummy variables to the input DataFrame.\n\n    This method converts the specified columns in `list_x_categorie` into interaction dummy variables\n    and appends them to the input DataFrame. If the `list_no_trap` attribute is set,\n    the specified dummy column(s) will be dropped.\n\n    Parameters:\n    - X (pd.DataFrame): The input data to transform.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - pd.DataFrame: The transformed data with added interaction dummy variables.\n    \"\"\"\n    # Cr\u00e9ation et ajout du jeu de donn\u00e9es d'indicatrices\n    data_res = pd.concat(\n        [X, pd.get_dummies(data=X[self.list_x_categorie], prefix=\"interact\")],\n        axis=1,\n        join=\"outer\",\n    )  # .drop(self.list_x_categorie, axis=1)\n\n    # Supression des colonnes superflues\n    if self.list_no_trap is not None:\n        data_res.drop(\n            [\"interact_\" + no_trap for no_trap in self.list_no_trap],\n            axis=1,\n            inplace=True,\n        )\n\n    # Combinaison des indicatrices et cr\u00e9ation des int\u00e9ractions\n\n    # Cr\u00e9ation des combinaisons possibles\n    list_modalite = [\n        np.setdiff1d(X[x_categorie].unique(), self.list_no_trap).tolist()\n        for x_categorie in self.list_x_categorie\n    ]\n    list_combinaison = list(product(*list_modalite))\n\n    # Ajout des interactions\n    for combinaison in list_combinaison:\n        # Nom de la colonne d'int\u00e9raction\n        nom_col = \" - \".join(combinaison)\n        # Initialisation de la colonne d'interaction\n        data_res[nom_col] = 1\n        for modalite in combinaison:\n            # Mise \u00e0 jour de la variable d'interaction\n            data_res[nom_col] *= data_res[\"interact_\" + modalite]\n\n    # Suppression des indicatrices non interagies\n    list_col_drop = [\n        \"interact_\" + col\n        for col in np.unique(\n            np.setdiff1d(\n                X[self.list_x_categorie].values.reshape(-1), self.list_no_trap\n            )\n        )\n    ]\n    data_res.drop(list_col_drop, axis=1, inplace=True)\n\n    return data_res\n</code></pre>"},{"location":"api/ClusteringTransformer/","title":"ClusteringTransformer","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>A transformer for adding cluster labels to a dataset using a clustering estimator.</p>"},{"location":"api/ClusteringTransformer/#igf_toolbox.preprocessing.transformers.ClusteringTransformer--parameters","title":"Parameters","text":"<p>estimator : estimator object     The clustering estimator (e.g., KMeans, DBSCAN) to use for labeling. sample_weight : array-like or None, default=None     Sample weights to apply during the clustering. If None, no sample weights will be used.</p>"},{"location":"api/ClusteringTransformer/#igf_toolbox.preprocessing.transformers.ClusteringTransformer--attributes","title":"Attributes","text":"<p>labels_ : pandas.Series     The cluster labels assigned to each sample in the fitted dataset.</p>"},{"location":"api/ClusteringTransformer/#igf_toolbox.preprocessing.transformers.ClusteringTransformer--methods","title":"Methods","text":"<p>fit(X, y=None)     Fit the clustering estimator to the input data and assign cluster labels. predict(X)     Predict cluster labels for the input data. transform(X, y=None)     Transform the input data by adding cluster labels as a new column.</p>"},{"location":"api/ClusteringTransformer/#igf_toolbox.preprocessing.transformers.ClusteringTransformer--notes","title":"Notes","text":"<p>This transformer is designed to work with clustering estimators from scikit-learn.</p>"},{"location":"api/ClusteringTransformer/#igf_toolbox.preprocessing.transformers.ClusteringTransformer--examples","title":"Examples","text":"<p>from sklearn.cluster import KMeans from sklearn.datasets import make_blobs data, _ = make_blobs(n_samples=100, centers=3, random_state=42) estimator = KMeans(n_clusters=3, random_state=42) transformer = ClusteringTransformer(estimator) transformed_data = transformer.fit_transform(data)</p> <p>The resulting <code>transformed_data</code> DataFrame will include a 'labels' column with cluster assignments.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>class ClusteringTransformer(TransformerMixin, BaseEstimator):\n    \"\"\"\n    A transformer for adding cluster labels to a dataset using a clustering estimator.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        The clustering estimator (e.g., KMeans, DBSCAN) to use for labeling.\n    sample_weight : array-like or None, default=None\n        Sample weights to apply during the clustering. If None, no sample weights will be used.\n\n    Attributes\n    ----------\n    labels_ : pandas.Series\n        The cluster labels assigned to each sample in the fitted dataset.\n\n    Methods\n    -------\n    fit(X, y=None)\n        Fit the clustering estimator to the input data and assign cluster labels.\n    predict(X)\n        Predict cluster labels for the input data.\n    transform(X, y=None)\n        Transform the input data by adding cluster labels as a new column.\n\n    Notes\n    -----\n    This transformer is designed to work with clustering estimators from scikit-learn.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from sklearn.cluster import KMeans\n    &gt;&gt;&gt; from sklearn.datasets import make_blobs\n    &gt;&gt;&gt; data, _ = make_blobs(n_samples=100, centers=3, random_state=42)\n    &gt;&gt;&gt; estimator = KMeans(n_clusters=3, random_state=42)\n    &gt;&gt;&gt; transformer = ClusteringTransformer(estimator)\n    &gt;&gt;&gt; transformed_data = transformer.fit_transform(data)\n\n    The resulting `transformed_data` DataFrame will include a 'labels' column with cluster assignments.\n    \"\"\"\n\n    def __init__(self, estimator, sample_weight: Union[pd.Series, None] = None) -&gt; None:\n\n        # Initialisation de l'estimateur\n        self.estimator = estimator\n        # Initialisation des poids d'entrainement\n        self.sample_weight = sample_weight\n\n    def fit(self, X: pd.DataFrame, y=None) -&gt; None:\n        \"\"\"\n        Fit the clustering estimator to the input data and assign cluster labels.\n\n        Parameters\n        ----------\n        X : pd.DataFrame, shape (n_samples, n_features)\n            The input data.\n        y : array-like, shape (n_samples,), default=None\n            The target labels (ignored in unsupervised clustering).\n\n        Returns\n        -------\n        self : object\n            Returns the instance itself.\n        \"\"\"\n        # Restriction aux observations qui n'ont pas \u00e9t\u00e9 supprim\u00e9es jusqu'alors\n        if self.sample_weight is not None:\n            sample_weight_reduced = self.sample_weight.loc[\n                self.sample_weight.index.isin(X.index)\n            ]\n            # Entrainement du mod\u00e8le\n            try:\n                self.estimator.fit(X=X, y=y, sample_weight=sample_weight_reduced)\n            except:\n                self.estimator.fit(X=X, y=y)\n                pass\n        else:\n            # Entrainement du mod\u00e8le\n            self.estimator.fit(X=X, y=y)\n\n        # Extraction des labels d'assignation \u00e0 chaque cluster\n        self.labels = pd.DataFrame(\n            data=self.estimator.labels_, index=X.index, columns=[\"labels\"]\n        )\n\n        return self\n\n    def predict(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n        \"\"\"\n        Predict cluster labels for the input data.\n\n        Parameters\n        ----------\n        X : array-like or pd.DataFrame, shape (n_samples, n_features)\n            The input data for which to predict cluster labels.\n\n        Returns\n        -------\n        data_pred : pd.DataFrame, shape (n_samples, 1)\n            A DataFrame with cluster labels as a 'labels' column.\n        \"\"\"\n        # Pr\u00e9diction de l'array r\u00e9sultat\n        array_pred = self.estimator.predict(X=X)\n        # Conversion en DataFrame\n        data_pred = pd.DataFrame(data=array_pred, index=X.index, columns=[\"labels\"])\n\n        return data_pred\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; None:\n        \"\"\"\n        Transform the input data by adding cluster labels as a new column.\n\n        Parameters\n        ----------\n        X : array-like or pd.DataFrame, shape (n_samples, n_features)\n            The input data to transform.\n\n        Returns\n        -------\n        X_transformed : pd.DataFrame, shape (n_samples, n_features + 1)\n            The input data with an additional 'labels' column containing cluster assignments.\n        \"\"\"\n        # Une mani\u00e8re plus g\u00e9n\u00e9rale serait de pr\u00e9dire les labels dans un premier temps sur les X\n        X_transformed = pd.concat([self.labels, X], axis=1, join=\"outer\")\n\n        return X_transformed\n</code></pre>"},{"location":"api/ClusteringTransformer/#igf_toolbox.preprocessing.transformers.ClusteringTransformer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the clustering estimator to the input data and assign cluster labels.</p>"},{"location":"api/ClusteringTransformer/#igf_toolbox.preprocessing.transformers.ClusteringTransformer.fit--parameters","title":"Parameters","text":"<p>X : pd.DataFrame, shape (n_samples, n_features)     The input data. y : array-like, shape (n_samples,), default=None     The target labels (ignored in unsupervised clustering).</p>"},{"location":"api/ClusteringTransformer/#igf_toolbox.preprocessing.transformers.ClusteringTransformer.fit--returns","title":"Returns","text":"<p>self : object     Returns the instance itself.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def fit(self, X: pd.DataFrame, y=None) -&gt; None:\n    \"\"\"\n    Fit the clustering estimator to the input data and assign cluster labels.\n\n    Parameters\n    ----------\n    X : pd.DataFrame, shape (n_samples, n_features)\n        The input data.\n    y : array-like, shape (n_samples,), default=None\n        The target labels (ignored in unsupervised clustering).\n\n    Returns\n    -------\n    self : object\n        Returns the instance itself.\n    \"\"\"\n    # Restriction aux observations qui n'ont pas \u00e9t\u00e9 supprim\u00e9es jusqu'alors\n    if self.sample_weight is not None:\n        sample_weight_reduced = self.sample_weight.loc[\n            self.sample_weight.index.isin(X.index)\n        ]\n        # Entrainement du mod\u00e8le\n        try:\n            self.estimator.fit(X=X, y=y, sample_weight=sample_weight_reduced)\n        except:\n            self.estimator.fit(X=X, y=y)\n            pass\n    else:\n        # Entrainement du mod\u00e8le\n        self.estimator.fit(X=X, y=y)\n\n    # Extraction des labels d'assignation \u00e0 chaque cluster\n    self.labels = pd.DataFrame(\n        data=self.estimator.labels_, index=X.index, columns=[\"labels\"]\n    )\n\n    return self\n</code></pre>"},{"location":"api/ClusteringTransformer/#igf_toolbox.preprocessing.transformers.ClusteringTransformer.predict","title":"<code>predict(X)</code>","text":"<p>Predict cluster labels for the input data.</p>"},{"location":"api/ClusteringTransformer/#igf_toolbox.preprocessing.transformers.ClusteringTransformer.predict--parameters","title":"Parameters","text":"<p>X : array-like or pd.DataFrame, shape (n_samples, n_features)     The input data for which to predict cluster labels.</p>"},{"location":"api/ClusteringTransformer/#igf_toolbox.preprocessing.transformers.ClusteringTransformer.predict--returns","title":"Returns","text":"<p>data_pred : pd.DataFrame, shape (n_samples, 1)     A DataFrame with cluster labels as a 'labels' column.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def predict(self, X: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Predict cluster labels for the input data.\n\n    Parameters\n    ----------\n    X : array-like or pd.DataFrame, shape (n_samples, n_features)\n        The input data for which to predict cluster labels.\n\n    Returns\n    -------\n    data_pred : pd.DataFrame, shape (n_samples, 1)\n        A DataFrame with cluster labels as a 'labels' column.\n    \"\"\"\n    # Pr\u00e9diction de l'array r\u00e9sultat\n    array_pred = self.estimator.predict(X=X)\n    # Conversion en DataFrame\n    data_pred = pd.DataFrame(data=array_pred, index=X.index, columns=[\"labels\"])\n\n    return data_pred\n</code></pre>"},{"location":"api/ClusteringTransformer/#igf_toolbox.preprocessing.transformers.ClusteringTransformer.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transform the input data by adding cluster labels as a new column.</p>"},{"location":"api/ClusteringTransformer/#igf_toolbox.preprocessing.transformers.ClusteringTransformer.transform--parameters","title":"Parameters","text":"<p>X : array-like or pd.DataFrame, shape (n_samples, n_features)     The input data to transform.</p>"},{"location":"api/ClusteringTransformer/#igf_toolbox.preprocessing.transformers.ClusteringTransformer.transform--returns","title":"Returns","text":"<p>X_transformed : pd.DataFrame, shape (n_samples, n_features + 1)     The input data with an additional 'labels' column containing cluster assignments.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; None:\n    \"\"\"\n    Transform the input data by adding cluster labels as a new column.\n\n    Parameters\n    ----------\n    X : array-like or pd.DataFrame, shape (n_samples, n_features)\n        The input data to transform.\n\n    Returns\n    -------\n    X_transformed : pd.DataFrame, shape (n_samples, n_features + 1)\n        The input data with an additional 'labels' column containing cluster assignments.\n    \"\"\"\n    # Une mani\u00e8re plus g\u00e9n\u00e9rale serait de pr\u00e9dire les labels dans un premier temps sur les X\n    X_transformed = pd.concat([self.labels, X], axis=1, join=\"outer\")\n\n    return X_transformed\n</code></pre>"},{"location":"api/ColumnExcluder/","title":"ColumnExcluder","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>Transformer that allows for the exclusion of one or multiple columns from a DataFrame.</p> <p>Provides methods for fitting to data and transforming data by dropping specified columns.</p>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder--attributes","title":"Attributes:","text":"<p>list_col_drop : list     List of column names to be dropped.</p>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder--methods","title":"Methods:","text":"<p>fit(X, y=None) :     Fits the transformer to the data. For this transformer, it's essentially a no-op but maintains consistency.</p> <p>transform(X, y=None) :     Transforms the data by dropping the specified columns.</p> <p>fit_transform(X, y=None) :     Fits and then transforms the data.</p>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder--examples","title":"Examples:","text":"<p>col_excluder = ColumnExcluder(list_col_drop=['col1', 'col2']) reduced_data = col_excluder.fit_transform(data)</p>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder--notes","title":"Notes:","text":"<p>It's important to ensure that the columns specified in <code>list_col_drop</code> exist in the DataFrame. Otherwise, it may raise a KeyError.</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>class ColumnExcluder(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Transformer that allows for the exclusion of one or multiple columns from a DataFrame.\n\n    Provides methods for fitting to data and transforming data by dropping specified columns.\n\n    Attributes:\n    -----------\n    list_col_drop : list\n        List of column names to be dropped.\n\n    Methods:\n    --------\n    fit(X, y=None) :\n        Fits the transformer to the data. For this transformer, it's essentially a no-op but maintains consistency.\n\n    transform(X, y=None) :\n        Transforms the data by dropping the specified columns.\n\n    fit_transform(X, y=None) :\n        Fits and then transforms the data.\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; col_excluder = ColumnExcluder(list_col_drop=['col1', 'col2'])\n    &gt;&gt;&gt; reduced_data = col_excluder.fit_transform(data)\n\n    Notes:\n    ------\n    It's important to ensure that the columns specified in `list_col_drop` exist in the DataFrame. Otherwise, it may raise a KeyError.\n    \"\"\"\n\n    def __init__(self, list_col_drop: List[str]) -&gt; None:\n        \"\"\"\n        Initializes the ColumnExcluder class.\n\n        Parameters:\n        -----------\n        list_col_drop : list\n            List of column names to be dropped from the DataFrame.\n        \"\"\"\n        # Initialisation des param\u00e8tres\n        self.list_col_drop = list_col_drop\n\n    def fit(self, X, y=None) -&gt; None:\n        \"\"\"\n        Fits the transformer to the data. For this transformer, it's a no-op but is included for consistency.\n\n        Parameters:\n        -----------\n        X : pd.DataFrame\n            The data to be transformed.\n\n        y : Ignored\n            This parameter exists only for compatibility with scikit-learn pipeline and is not used.\n\n        Returns:\n        --------\n        self\n        \"\"\"\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Transforms the data by dropping the specified columns.\n\n        Parameters:\n        -----------\n        X : pd.DataFrame\n            The data to be transformed.\n\n        y : Ignored\n            This parameter exists only for compatibility with scikit-learn pipeline and is not used.\n\n        Returns:\n        --------\n        pd.DataFrame\n            Transformed data with specified columns dropped.\n        \"\"\"\n        return X.drop(self.list_col_drop, axis=1)\n\n    def fit_transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Fits and then transforms the data.\n\n        Parameters:\n        -----------\n        X : pd.DataFrame\n            The data to be transformed.\n\n        y : Ignored\n            This parameter exists only for compatibility with scikit-learn pipeline and is not used.\n\n        Returns:\n        --------\n        pd.DataFrame\n            Transformed data with specified columns dropped.\n        \"\"\"\n        self.fit(X=X, y=y)\n        return self.transform(X=X, y=y)\n</code></pre>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder.__init__","title":"<code>__init__(list_col_drop)</code>","text":"<p>Initializes the ColumnExcluder class.</p>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder.__init__--parameters","title":"Parameters:","text":"<p>list_col_drop : list     List of column names to be dropped from the DataFrame.</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>def __init__(self, list_col_drop: List[str]) -&gt; None:\n    \"\"\"\n    Initializes the ColumnExcluder class.\n\n    Parameters:\n    -----------\n    list_col_drop : list\n        List of column names to be dropped from the DataFrame.\n    \"\"\"\n    # Initialisation des param\u00e8tres\n    self.list_col_drop = list_col_drop\n</code></pre>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fits the transformer to the data. For this transformer, it's a no-op but is included for consistency.</p>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder.fit--parameters","title":"Parameters:","text":"<p>X : pd.DataFrame     The data to be transformed.</p> Ignored <p>This parameter exists only for compatibility with scikit-learn pipeline and is not used.</p>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder.fit--returns","title":"Returns:","text":"<p>self</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>def fit(self, X, y=None) -&gt; None:\n    \"\"\"\n    Fits the transformer to the data. For this transformer, it's a no-op but is included for consistency.\n\n    Parameters:\n    -----------\n    X : pd.DataFrame\n        The data to be transformed.\n\n    y : Ignored\n        This parameter exists only for compatibility with scikit-learn pipeline and is not used.\n\n    Returns:\n    --------\n    self\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder.fit_transform","title":"<code>fit_transform(X, y=None)</code>","text":"<p>Fits and then transforms the data.</p>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder.fit_transform--parameters","title":"Parameters:","text":"<p>X : pd.DataFrame     The data to be transformed.</p> Ignored <p>This parameter exists only for compatibility with scikit-learn pipeline and is not used.</p>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder.fit_transform--returns","title":"Returns:","text":"<p>pd.DataFrame     Transformed data with specified columns dropped.</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>def fit_transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Fits and then transforms the data.\n\n    Parameters:\n    -----------\n    X : pd.DataFrame\n        The data to be transformed.\n\n    y : Ignored\n        This parameter exists only for compatibility with scikit-learn pipeline and is not used.\n\n    Returns:\n    --------\n    pd.DataFrame\n        Transformed data with specified columns dropped.\n    \"\"\"\n    self.fit(X=X, y=y)\n    return self.transform(X=X, y=y)\n</code></pre>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Transforms the data by dropping the specified columns.</p>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder.transform--parameters","title":"Parameters:","text":"<p>X : pd.DataFrame     The data to be transformed.</p> Ignored <p>This parameter exists only for compatibility with scikit-learn pipeline and is not used.</p>"},{"location":"api/ColumnExcluder/#igf_toolbox.preprocessing.excluders.ColumnExcluder.transform--returns","title":"Returns:","text":"<p>pd.DataFrame     Transformed data with specified columns dropped.</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Transforms the data by dropping the specified columns.\n\n    Parameters:\n    -----------\n    X : pd.DataFrame\n        The data to be transformed.\n\n    y : Ignored\n        This parameter exists only for compatibility with scikit-learn pipeline and is not used.\n\n    Returns:\n    --------\n    pd.DataFrame\n        Transformed data with specified columns dropped.\n    \"\"\"\n    return X.drop(self.list_col_drop, axis=1)\n</code></pre>"},{"location":"api/ForestOutliersIsolation/","title":"ForestOutliersIsolation","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>A transformer that uses Isolation Forest to identify and exclude outliers from a dataset.</p> <p>The Isolation Forest is an unsupervised machine learning algorithm that works by isolating observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of that selected feature. It's primarily used for anomaly detection.</p> <p>Parameters: - n_estimators (int, optional): The number of base estimators in the ensemble. Default is 100. - random_state (int, optional): Seed used by the random number generator. Default is 42.</p> <p>Attributes: - model (IsolationForest): The trained Isolation Forest model.</p> <p>Methods: - fit(X, y=None): Fit the Isolation Forest model. - transform(X, y=None): Transform the data by excluding detected anomalies.</p> <p>Example:</p> <p>fo = ForestOutliersIsolation() X = pd.DataFrame({'A': [1, 2, 3, 100], 'B': [1, 2, 3, 100]}) fo.fit(X) print(fo.transform(X))    A  B 0  1  1 1  2  2 2  3  3</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>class ForestOutliersIsolation(TransformerMixin, BaseEstimator):\n    \"\"\"\n    A transformer that uses Isolation Forest to identify and exclude outliers from a dataset.\n\n    The Isolation Forest is an unsupervised machine learning algorithm that works by\n    isolating observations by randomly selecting a feature and then randomly selecting\n    a split value between the maximum and minimum values of that selected feature. It's\n    primarily used for anomaly detection.\n\n    Parameters:\n    - n_estimators (int, optional): The number of base estimators in the ensemble. Default is 100.\n    - random_state (int, optional): Seed used by the random number generator. Default is 42.\n\n    Attributes:\n    - model (IsolationForest): The trained Isolation Forest model.\n\n    Methods:\n    - fit(X, y=None): Fit the Isolation Forest model.\n    - transform(X, y=None): Transform the data by excluding detected anomalies.\n\n    Example:\n    &gt;&gt;&gt; fo = ForestOutliersIsolation()\n    &gt;&gt;&gt; X = pd.DataFrame({'A': [1, 2, 3, 100], 'B': [1, 2, 3, 100]})\n    &gt;&gt;&gt; fo.fit(X)\n    &gt;&gt;&gt; print(fo.transform(X))\n       A  B\n    0  1  1\n    1  2  2\n    2  3  3\n    \"\"\"\n\n    def __init__(self, n_estimators: int = 100, random_state: int = 42) -&gt; None:\n        # Initialisation des param\u00e8tres\n        self.n_estimators = n_estimators\n        self.random_state = random_state\n\n    def fit(self, X, y=None):\n        \"\"\"\n        Fit the Isolation Forest model to the data.\n\n        Parameters:\n        - X (pd.DataFrame): The input data.\n        - y (ignored): This parameter is ignored as Isolation Forest is an unsupervised method.\n\n        Returns:\n        - self: The fitted transformer.\n        \"\"\"\n        self.model = IsolationForest(\n            n_estimators=self.n_estimators, random_state=self.random_state\n        ).fit(X=X, y=y)\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Remove detected anomalies from the data using the trained Isolation Forest model.\n\n        Parameters:\n        - X (pd.DataFrame): The input data.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - pd.DataFrame: The transformed data with detected anomalies excluded.\n        \"\"\"\n        # Copie ind\u00e9pendante du jeu de donn\u00e9es\n        X_res = X.copy()\n\n        # Ajout de la pr\u00e9diction des anomalies\n        X_res[\"anomalies\"] = self.model.predict(X_res)\n\n        # Suppression des lignes concern\u00e9es\n        list_drop = X_res.loc[X_res[\"anomalies\"] == -1].index.tolist()\n        X_res.drop(list_drop, axis=0, inplace=True)\n\n        # Suppression de la colonne 'anomalies'\n        X_res.drop(\"anomalies\", axis=1, inplace=True)\n\n        return X_res\n</code></pre>"},{"location":"api/ForestOutliersIsolation/#igf_toolbox.preprocessing.excluders.ForestOutliersIsolation.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the Isolation Forest model to the data.</p> <p>Parameters: - X (pd.DataFrame): The input data. - y (ignored): This parameter is ignored as Isolation Forest is an unsupervised method.</p> <p>Returns: - self: The fitted transformer.</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>def fit(self, X, y=None):\n    \"\"\"\n    Fit the Isolation Forest model to the data.\n\n    Parameters:\n    - X (pd.DataFrame): The input data.\n    - y (ignored): This parameter is ignored as Isolation Forest is an unsupervised method.\n\n    Returns:\n    - self: The fitted transformer.\n    \"\"\"\n    self.model = IsolationForest(\n        n_estimators=self.n_estimators, random_state=self.random_state\n    ).fit(X=X, y=y)\n    return self\n</code></pre>"},{"location":"api/ForestOutliersIsolation/#igf_toolbox.preprocessing.excluders.ForestOutliersIsolation.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Remove detected anomalies from the data using the trained Isolation Forest model.</p> <p>Parameters: - X (pd.DataFrame): The input data. - y (ignored): This parameter is ignored.</p> <p>Returns: - pd.DataFrame: The transformed data with detected anomalies excluded.</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Remove detected anomalies from the data using the trained Isolation Forest model.\n\n    Parameters:\n    - X (pd.DataFrame): The input data.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - pd.DataFrame: The transformed data with detected anomalies excluded.\n    \"\"\"\n    # Copie ind\u00e9pendante du jeu de donn\u00e9es\n    X_res = X.copy()\n\n    # Ajout de la pr\u00e9diction des anomalies\n    X_res[\"anomalies\"] = self.model.predict(X_res)\n\n    # Suppression des lignes concern\u00e9es\n    list_drop = X_res.loc[X_res[\"anomalies\"] == -1].index.tolist()\n    X_res.drop(list_drop, axis=0, inplace=True)\n\n    # Suppression de la colonne 'anomalies'\n    X_res.drop(\"anomalies\", axis=1, inplace=True)\n\n    return X_res\n</code></pre>"},{"location":"api/FrozenDict/","title":"FrozenDict","text":"<p>               Bases: <code>Mapping</code></p> <p>An immutable dictionary that can be hashed.</p> Source code in <code>igf_toolbox/utils/base.py</code> <pre><code>class FrozenDict(Mapping):\n    \"\"\"\n    An immutable dictionary that can be hashed.\n    \"\"\"\n\n    def __init__(self, *args, **kwargs) -&gt; None:\n\n        self._d = dict(*args, **kwargs)\n        self._hash = None\n\n    def __iter__(self):\n        return iter(self._d)\n\n    def __len__(self):\n        return len(self._d)\n\n    def __getitem__(self, key):\n        return self._d[key]\n\n    def __hash__(self):\n        if self._hash is None:\n            hash_ = 0\n            for pair in self.items():\n                hash_ ^= hash(pair)\n            self._hash = hash_\n        return self._hash\n\n    def __setitem__(self, key, value):\n        raise TypeError(\"FrozenDict is immutable; changes are not allowed\")\n\n    def __repr__(self):\n        return f\"FrozenDict({self._d})\"\n</code></pre>"},{"location":"api/LeastSquaresEstimator/","title":"LeastSquaresEstimator","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>A simple wrapper around the statsmodels OLS (Ordinary Least Squares) and WLS (Weighted Least Squares) regression models.</p> <p>Provides methods for fitting a linear regression model, making predictions, and summarizing the results.</p>"},{"location":"api/LeastSquaresEstimator/#igf_toolbox.estimators.regressors.LeastSquaresEstimator--attributes","title":"Attributes:","text":"<p>model : statsmodels regression model     The internal regression model, either OLS or WLS. _coef : pd.Series     The estimated coefficients from the regression model.</p>"},{"location":"api/LeastSquaresEstimator/#igf_toolbox.estimators.regressors.LeastSquaresEstimator--methods","title":"Methods:","text":"<p>fit(X, y, sample_weight=None) :     Fits a regression model to the data.</p> <p>predict(X) :     Uses the fitted model to predict the dependent variable for a new set of independent variables.</p> <p>summary() :     Returns a summary of the estimated regression coefficients, their p-values, and 95% confidence intervals.</p> <p>rsquared() :     Returns the R-squared and adjusted R-squared values for the fitted model.</p>"},{"location":"api/LeastSquaresEstimator/#igf_toolbox.estimators.regressors.LeastSquaresEstimator--examples","title":"Examples:","text":"<p>estimator = LeastSquaresEstimator() estimator.fit(X_train, y_train) predictions = estimator.predict(X_test) summary_results = estimator.summary() r2_values = estimator.rsquared()</p> Source code in <code>igf_toolbox/estimators/regressors.py</code> <pre><code>class LeastSquaresEstimator(BaseEstimator):\n    \"\"\"\n    A simple wrapper around the statsmodels OLS (Ordinary Least Squares) and WLS (Weighted Least Squares) regression models.\n\n    Provides methods for fitting a linear regression model, making predictions, and summarizing the results.\n\n    Attributes:\n    -----------\n    model : statsmodels regression model\n        The internal regression model, either OLS or WLS.\n    _coef : pd.Series\n        The estimated coefficients from the regression model.\n\n    Methods:\n    --------\n    fit(X, y, sample_weight=None) :\n        Fits a regression model to the data.\n\n    predict(X) :\n        Uses the fitted model to predict the dependent variable for a new set of independent variables.\n\n    summary() :\n        Returns a summary of the estimated regression coefficients, their p-values, and 95% confidence intervals.\n\n    rsquared() :\n        Returns the R-squared and adjusted R-squared values for the fitted model.\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; estimator = LeastSquaresEstimator()\n    &gt;&gt;&gt; estimator.fit(X_train, y_train)\n    &gt;&gt;&gt; predictions = estimator.predict(X_test)\n    &gt;&gt;&gt; summary_results = estimator.summary()\n    &gt;&gt;&gt; r2_values = estimator.rsquared()\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"Initializes the LeastSquaresEstimator class.\"\"\"\n\n    def fit(self, X, y, sample_weight: Optional[Union[pd.Series, None]] = None) -&gt; None:\n        \"\"\"\n        Fits a regression model to the data.\n\n        Parameters:\n        -----------\n        X : pd.DataFrame\n            The independent variables (explanatory variables).\n\n        y : pd.Series\n            The dependent variable (response variable).\n\n        sample_weight : pd.Series, optional\n            Optional weights for each observation. If provided, WLS is used; otherwise, OLS is used.\n\n        Returns:\n        --------\n        None\n        \"\"\"\n        # Estimation du mod\u00e8le\n        if sample_weight is None:\n            self.model = OLS(endog=y, exog=X).fit()\n        else:\n            self.model = WLS(endog=y, exog=X, weights=sample_weight).fit()\n\n        self._coef = self.model.params\n\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Uses the fitted model to predict the dependent variable for a new set of independent variables.\n\n        Parameters:\n        -----------\n        X : pd.DataFrame\n            The independent variables for which to predict the dependent variable.\n\n        Returns:\n        --------\n        pd.Series\n            Predicted values of the dependent variable.\n        \"\"\"\n        return self.model.predict(X)\n\n    def summary(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns a summary of the estimated regression coefficients, their p-values, and 95% confidence intervals.\n\n        Returns:\n        --------\n        pd.DataFrame\n            A dataframe with the estimated regression coefficients, their p-values, and 95% confidence intervals.\n        \"\"\"\n\n        # Description des r\u00e9sultats du mod\u00e8le\n        coefs = self.model.params.to_frame().rename({0: \"Coefficients\"}, axis=1)\n        p_values = self.model.pvalues.to_frame().rename({0: \"P-valeurs\"}, axis=1)\n        ic_95 = self.model.conf_int(alpha=0.05).rename({0: \"ICg\", 1: \"ICd\"}, axis=1)\n\n        # Concat\u00e9nation des r\u00e9sultats\n        data_res = pd.concat([coefs, p_values, ic_95], axis=1, join=\"outer\")\n\n        return data_res\n\n    def rsquared(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns the R-squared and adjusted R-squared values for the fitted model.\n\n        Returns:\n        --------\n        pd.DataFrame\n            A dataframe with the R-squared and adjusted R-squared values.\n        \"\"\"\n        # Description de la pertinence du mod\u00e8le\n        data_r2 = pd.DataFrame(\n            [[self.model.rsquared, self.model.rsquared_adj]],\n            index=[0],\n            columns=[\"R2\", \"R2Adj\"],\n        )\n        return data_r2\n</code></pre>"},{"location":"api/LeastSquaresEstimator/#igf_toolbox.estimators.regressors.LeastSquaresEstimator.__init__","title":"<code>__init__()</code>","text":"<p>Initializes the LeastSquaresEstimator class.</p> Source code in <code>igf_toolbox/estimators/regressors.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes the LeastSquaresEstimator class.\"\"\"\n</code></pre>"},{"location":"api/LeastSquaresEstimator/#igf_toolbox.estimators.regressors.LeastSquaresEstimator.fit","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fits a regression model to the data.</p>"},{"location":"api/LeastSquaresEstimator/#igf_toolbox.estimators.regressors.LeastSquaresEstimator.fit--parameters","title":"Parameters:","text":"<p>X : pd.DataFrame     The independent variables (explanatory variables).</p> pd.Series <p>The dependent variable (response variable).</p> pd.Series, optional <p>Optional weights for each observation. If provided, WLS is used; otherwise, OLS is used.</p>"},{"location":"api/LeastSquaresEstimator/#igf_toolbox.estimators.regressors.LeastSquaresEstimator.fit--returns","title":"Returns:","text":"<p>None</p> Source code in <code>igf_toolbox/estimators/regressors.py</code> <pre><code>def fit(self, X, y, sample_weight: Optional[Union[pd.Series, None]] = None) -&gt; None:\n    \"\"\"\n    Fits a regression model to the data.\n\n    Parameters:\n    -----------\n    X : pd.DataFrame\n        The independent variables (explanatory variables).\n\n    y : pd.Series\n        The dependent variable (response variable).\n\n    sample_weight : pd.Series, optional\n        Optional weights for each observation. If provided, WLS is used; otherwise, OLS is used.\n\n    Returns:\n    --------\n    None\n    \"\"\"\n    # Estimation du mod\u00e8le\n    if sample_weight is None:\n        self.model = OLS(endog=y, exog=X).fit()\n    else:\n        self.model = WLS(endog=y, exog=X, weights=sample_weight).fit()\n\n    self._coef = self.model.params\n\n    return self\n</code></pre>"},{"location":"api/LeastSquaresEstimator/#igf_toolbox.estimators.regressors.LeastSquaresEstimator.predict","title":"<code>predict(X)</code>","text":"<p>Uses the fitted model to predict the dependent variable for a new set of independent variables.</p>"},{"location":"api/LeastSquaresEstimator/#igf_toolbox.estimators.regressors.LeastSquaresEstimator.predict--parameters","title":"Parameters:","text":"<p>X : pd.DataFrame     The independent variables for which to predict the dependent variable.</p>"},{"location":"api/LeastSquaresEstimator/#igf_toolbox.estimators.regressors.LeastSquaresEstimator.predict--returns","title":"Returns:","text":"<p>pd.Series     Predicted values of the dependent variable.</p> Source code in <code>igf_toolbox/estimators/regressors.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Uses the fitted model to predict the dependent variable for a new set of independent variables.\n\n    Parameters:\n    -----------\n    X : pd.DataFrame\n        The independent variables for which to predict the dependent variable.\n\n    Returns:\n    --------\n    pd.Series\n        Predicted values of the dependent variable.\n    \"\"\"\n    return self.model.predict(X)\n</code></pre>"},{"location":"api/LeastSquaresEstimator/#igf_toolbox.estimators.regressors.LeastSquaresEstimator.rsquared","title":"<code>rsquared()</code>","text":"<p>Returns the R-squared and adjusted R-squared values for the fitted model.</p>"},{"location":"api/LeastSquaresEstimator/#igf_toolbox.estimators.regressors.LeastSquaresEstimator.rsquared--returns","title":"Returns:","text":"<p>pd.DataFrame     A dataframe with the R-squared and adjusted R-squared values.</p> Source code in <code>igf_toolbox/estimators/regressors.py</code> <pre><code>def rsquared(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns the R-squared and adjusted R-squared values for the fitted model.\n\n    Returns:\n    --------\n    pd.DataFrame\n        A dataframe with the R-squared and adjusted R-squared values.\n    \"\"\"\n    # Description de la pertinence du mod\u00e8le\n    data_r2 = pd.DataFrame(\n        [[self.model.rsquared, self.model.rsquared_adj]],\n        index=[0],\n        columns=[\"R2\", \"R2Adj\"],\n    )\n    return data_r2\n</code></pre>"},{"location":"api/LeastSquaresEstimator/#igf_toolbox.estimators.regressors.LeastSquaresEstimator.summary","title":"<code>summary()</code>","text":"<p>Returns a summary of the estimated regression coefficients, their p-values, and 95% confidence intervals.</p>"},{"location":"api/LeastSquaresEstimator/#igf_toolbox.estimators.regressors.LeastSquaresEstimator.summary--returns","title":"Returns:","text":"<p>pd.DataFrame     A dataframe with the estimated regression coefficients, their p-values, and 95% confidence intervals.</p> Source code in <code>igf_toolbox/estimators/regressors.py</code> <pre><code>def summary(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns a summary of the estimated regression coefficients, their p-values, and 95% confidence intervals.\n\n    Returns:\n    --------\n    pd.DataFrame\n        A dataframe with the estimated regression coefficients, their p-values, and 95% confidence intervals.\n    \"\"\"\n\n    # Description des r\u00e9sultats du mod\u00e8le\n    coefs = self.model.params.to_frame().rename({0: \"Coefficients\"}, axis=1)\n    p_values = self.model.pvalues.to_frame().rename({0: \"P-valeurs\"}, axis=1)\n    ic_95 = self.model.conf_int(alpha=0.05).rename({0: \"ICg\", 1: \"ICd\"}, axis=1)\n\n    # Concat\u00e9nation des r\u00e9sultats\n    data_res = pd.concat([coefs, p_values, ic_95], axis=1, join=\"outer\")\n\n    return data_res\n</code></pre>"},{"location":"api/LogTransformer/","title":"LogTransformer","text":"<p>               Bases: <code>BaseEstimator</code>, <code>TransformerMixin</code></p> <p>Transformer to apply logarithmic transformation to specified columns.</p> <p>This transformer applies a natural logarithm transformation to specified columns in the feature matrix. It's useful when working with skewed data or when the relationship between the variables is multiplicative in nature.</p>"},{"location":"api/LogTransformer/#igf_toolbox.preprocessing.transformers.LogTransformer--parameters","title":"Parameters:","text":"<p>list_col : list of str     A list of column names in the DataFrame which will undergo the logarithm transformation. replace_inf : bool or numeric     The value to replace the infinity values with.</p>"},{"location":"api/LogTransformer/#igf_toolbox.preprocessing.transformers.LogTransformer--attributes","title":"Attributes:","text":"<p>list_col : list of str     Stored list of column names for transformation.</p>"},{"location":"api/LogTransformer/#igf_toolbox.preprocessing.transformers.LogTransformer--examples","title":"Examples:","text":"<p>import pandas as pd data = pd.DataFrame({\"A\": [1, 2, 3, 4, 5], \"B\": [6, 7, 8, 9, 10]}) transformer = LogTransformer(list_col=[\"A\", \"B\"]) transformed_data = transformer.fit_transform(data)</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>class LogTransformer(BaseEstimator, TransformerMixin):\n    \"\"\"\n    Transformer to apply logarithmic transformation to specified columns.\n\n    This transformer applies a natural logarithm transformation to specified\n    columns in the feature matrix. It's useful when working with skewed data\n    or when the relationship between the variables is multiplicative in nature.\n\n    Parameters:\n    -----------\n    list_col : list of str\n        A list of column names in the DataFrame which will undergo the logarithm transformation.\n    replace_inf : bool or numeric\n        The value to replace the infinity values with.\n\n    Attributes:\n    -----------\n    list_col : list of str\n        Stored list of column names for transformation.\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; data = pd.DataFrame({\"A\": [1, 2, 3, 4, 5], \"B\": [6, 7, 8, 9, 10]})\n    &gt;&gt;&gt; transformer = LogTransformer(list_col=[\"A\", \"B\"])\n    &gt;&gt;&gt; transformed_data = transformer.fit_transform(data)\n    \"\"\"\n\n    def __init__(self, list_col: List[str], replace_inf: Optional[int] = 0) -&gt; None:\n        # Initialisation des param\u00e8tres\n        self.list_col = list_col\n        self.replace_inf = replace_inf\n\n    def fit(self, X, y=None) -&gt; None:\n        \"\"\"Fit the transformer.\n\n        As this transformer does not require any fitting, the method simply returns self.\n\n        Parameters:\n        -----------\n        X : DataFrame\n            Feature matrix.\n\n        y : Series or array-like, default=None\n            Target variable. Not used in this transformer.\n\n        Returns:\n        --------\n        self : LogTransformer\n            The transformer instance.\n        \"\"\"\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"Apply logarithmic transformation to the specified columns in X.\n\n        Parameters:\n        -----------\n        X : DataFrame\n            Feature matrix.\n\n        y : Series or array-like, default=None\n            Target variable. Not used in this transformer.\n\n        Returns:\n        --------\n        X : DataFrame\n            Transformed feature matrix.\n        \"\"\"\n        # Transformation logarithmique des colonnes d'int\u00e9r\u00eat\n        X[self.list_col] = np.log(X[self.list_col])\n\n        # Remplacement des valeurs inf par la valeur choisie\n        # Le .replace([-np.inf, np.inf], to_replace) ne fonctionne pas\n        if not isinstance(self.replace_inf, bool):\n            for var_log in self.list_col:\n                X.loc[X[var_log].isin([np.inf, -np.inf]), var_log] = self.replace_inf\n\n        return X\n\n    def fit_transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"Fit the transformer and apply the transformation.\n\n        Parameters:\n        -----------\n        X : DataFrame\n            Feature matrix.\n\n        y : Series or array-like, default=None\n            Target variable. Not used in this transformer.\n\n        Returns:\n        --------\n        X : DataFrame\n            Transformed feature matrix.\n        \"\"\"\n        self.fit(X=X, y=y)\n        return self.transform(X=X, y=y)\n</code></pre>"},{"location":"api/LogTransformer/#igf_toolbox.preprocessing.transformers.LogTransformer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the transformer.</p> <p>As this transformer does not require any fitting, the method simply returns self.</p>"},{"location":"api/LogTransformer/#igf_toolbox.preprocessing.transformers.LogTransformer.fit--parameters","title":"Parameters:","text":"<p>X : DataFrame     Feature matrix.</p> Series or array-like, default=None <p>Target variable. Not used in this transformer.</p>"},{"location":"api/LogTransformer/#igf_toolbox.preprocessing.transformers.LogTransformer.fit--returns","title":"Returns:","text":"<p>self : LogTransformer     The transformer instance.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def fit(self, X, y=None) -&gt; None:\n    \"\"\"Fit the transformer.\n\n    As this transformer does not require any fitting, the method simply returns self.\n\n    Parameters:\n    -----------\n    X : DataFrame\n        Feature matrix.\n\n    y : Series or array-like, default=None\n        Target variable. Not used in this transformer.\n\n    Returns:\n    --------\n    self : LogTransformer\n        The transformer instance.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/LogTransformer/#igf_toolbox.preprocessing.transformers.LogTransformer.fit_transform","title":"<code>fit_transform(X, y=None)</code>","text":"<p>Fit the transformer and apply the transformation.</p>"},{"location":"api/LogTransformer/#igf_toolbox.preprocessing.transformers.LogTransformer.fit_transform--parameters","title":"Parameters:","text":"<p>X : DataFrame     Feature matrix.</p> Series or array-like, default=None <p>Target variable. Not used in this transformer.</p>"},{"location":"api/LogTransformer/#igf_toolbox.preprocessing.transformers.LogTransformer.fit_transform--returns","title":"Returns:","text":"<p>X : DataFrame     Transformed feature matrix.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def fit_transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"Fit the transformer and apply the transformation.\n\n    Parameters:\n    -----------\n    X : DataFrame\n        Feature matrix.\n\n    y : Series or array-like, default=None\n        Target variable. Not used in this transformer.\n\n    Returns:\n    --------\n    X : DataFrame\n        Transformed feature matrix.\n    \"\"\"\n    self.fit(X=X, y=y)\n    return self.transform(X=X, y=y)\n</code></pre>"},{"location":"api/LogTransformer/#igf_toolbox.preprocessing.transformers.LogTransformer.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Apply logarithmic transformation to the specified columns in X.</p>"},{"location":"api/LogTransformer/#igf_toolbox.preprocessing.transformers.LogTransformer.transform--parameters","title":"Parameters:","text":"<p>X : DataFrame     Feature matrix.</p> Series or array-like, default=None <p>Target variable. Not used in this transformer.</p>"},{"location":"api/LogTransformer/#igf_toolbox.preprocessing.transformers.LogTransformer.transform--returns","title":"Returns:","text":"<p>X : DataFrame     Transformed feature matrix.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"Apply logarithmic transformation to the specified columns in X.\n\n    Parameters:\n    -----------\n    X : DataFrame\n        Feature matrix.\n\n    y : Series or array-like, default=None\n        Target variable. Not used in this transformer.\n\n    Returns:\n    --------\n    X : DataFrame\n        Transformed feature matrix.\n    \"\"\"\n    # Transformation logarithmique des colonnes d'int\u00e9r\u00eat\n    X[self.list_col] = np.log(X[self.list_col])\n\n    # Remplacement des valeurs inf par la valeur choisie\n    # Le .replace([-np.inf, np.inf], to_replace) ne fonctionne pas\n    if not isinstance(self.replace_inf, bool):\n        for var_log in self.list_col:\n            X.loc[X[var_log].isin([np.inf, -np.inf]), var_log] = self.replace_inf\n\n    return X\n</code></pre>"},{"location":"api/LogitClassifier/","title":"LogitClassifier","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Logit model wrapped to a sklearn class Parameters</p> <p>proba_threshold : float, default=0.5     Threshold to distinguish forecast corresponding to the zero class and to the one class</p> string, default='roc' <p>Score to calculate via the score method</p> <p>regularized : bool, default=True     Whether to add a regularisation constrain to select variables in the minimization process and ensure convergence</p> string, default='l1' <p>Type of regularization to apply</p> float, default=0.01 <p>Regularization parameter</p>"},{"location":"api/LogitClassifier/#igf_toolbox.estimators.classifiers.LogitClassifier--attributes","title":"Attributes","text":"<p>logit : Logit     Logit fitted regressor</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>class LogitClassifier(BaseEstimator):\n    \"\"\"\n    Logit model wrapped to a sklearn class\n    Parameters\n    ----------\n    proba_threshold : float, default=0.5\n        Threshold to distinguish forecast corresponding to the zero class and to the one class\n\n    scoring : string, default='roc'\n        Score to calculate via the score method\n    regularized : bool, default=True\n        Whether to add a regularisation constrain to select variables in the minimization process and ensure convergence\n\n    method : string, default='l1'\n        Type of regularization to apply\n\n    alpha : float, default=0.01\n        Regularization parameter\n\n    Attributes\n    ----------\n    logit : Logit\n        Logit fitted regressor\n    \"\"\"\n\n    def __init__(\n        self,\n        proba_threshold: Optional[float] = 0.5,\n        scoring: Optional[str] = \"roc\",\n        regularized: Optional[bool] = False,\n        method: Optional[str] = \"l1\",\n        alpha: Optional[float] = 0.01,\n    ) -&gt; None:\n        # Initialisation du mod\u00e8le\n        self.scoring = scoring\n        self.regularized = regularized\n        self.method = method\n        self.alpha = alpha\n        self.trim_mode = \"size\"\n        self.estimator_type = \"classifier\"\n        self._estimator_type = \"classifier\"\n        self.proba_threshold = proba_threshold\n\n    def set_params(self, **parameters) -&gt; None:\n        \"\"\"\n        Change the parameters of the model\n        Returns\n        -------\n        self : returns an instance of self\n        \"\"\"\n        # Ajout des param\u00e8tres\n        for parameter, value in parameters.items():\n            setattr(self, parameter, value)\n        return self\n\n    def get_params(self, deep: Optional[bool] = True) -&gt; dict:\n        \"\"\"\n        Return a dictionnary containing the current parameters of the model\n        Parameters\n        ----------\n\n        deep : bool, default=True\n            Whether to return an independent copy of the parameters\n        Returns\n        -------\n\n        DictParams : dictionnary\n            Dictionnary containing the name of the parameter and its corresponding value\n        \"\"\"\n        # Retourne les param\u00e8tres\n        return {\n            \"alpha\": self.alpha,\n            \"scoring\": self.scoring,\n            \"regularized\": self.regularized,\n            \"method\": self.method,\n            \"trim_mode\": self.trim_mode,\n            \"proba_threshold\": self.proba_threshold,\n        }\n\n    def fit(self, X, y):\n        \"\"\"\n        Fit the probit model.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values. Will be cast to X's dtype if necessary\n        Returns\n        -------\n        self : returns an instance of self\n        \"\"\"\n        # Entrainement du mod\u00e8le\n        if self.regularized:\n            self.logit = Logit(y, X).fit_regularized(\n                method=self.method,\n                alpha=self.alpha,\n                trim_mode=self.trim_mode,\n                maxiter=100000,\n            )\n        else:\n            self.logit = Logit(y, X).fit()\n        # Extraction du coefficient et des classes\n        self._coef = self.logit.params\n        self.classes_ = np.unique(y)\n\n        return self\n\n    def predict(self, X):\n        \"\"\"Predict class for X.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n        Returns\n        -------\n        Prediction : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            The predicted classes.\n        \"\"\"\n        # Pr\u00e9diction\n        if self.proba_threshold:\n            return (self.logit.predict(X) &gt;= self.proba_threshold) * 1\n        return self.logit.predict(X)\n\n    def predict_proba(self, X):\n        \"\"\"Predict class associated with the class labelized as one for X.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n        Returns\n        -------\n        Prediction : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            The predicted probabilities.\n        \"\"\"\n        # Pr\u00e9diction de la probabilit\u00e9 d'appartenance \u00e0 chacune des classes\n        proba = []\n        XArray = np.array(X)\n        for i in range(len(XArray)):\n            proba.append(\n                [\n                    1 - self.logit.predict(XArray[i, :])[0],\n                    self.logit.predict(XArray[i, :])[0],\n                ]\n            )\n        return np.array(proba).reshape(-1, 2)\n\n    def score(self, X_test, y_test):\n        \"\"\"Calculate the score for the label predicted by the model for X_test et les vrais labels y_test.\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n        y_test : array-like of shape (n_samples, 1).\n            The test samples\n        Returns\n        -------\n        score : float or array-like\n            The computed score.\n        \"\"\"\n        # Calcul du ROC-AUC ou de l'Accuracy\n        self.y_pred = ((self.predict(X_test) &gt; 0.25) * 1) * True\n        self.y_true = y_test\n        if self.scoring == \"accuracy\":\n            score = np.count_nonzero(\n                np.add(\n                    self.y_pred.tolist(),\n                    np.multiply(self.y_true.iloc[:, 0].tolist(), -1),\n                )\n                == 0\n            ) / len(self.y_pred)\n        if self.scoring == \"roc\":\n            from sklearn.metrics import roc_auc_score\n\n            score = roc_auc_score(self.y_true, self.predict(X_test))\n        return score\n\n    def summary(self):\n        \"\"\"\n        Give information about the estimation of the logit model\n        Returns\n        -------\n        Summary : Information about the coefficients estimated by the model\n        \"\"\"\n        # R\u00e9sum\u00e9 des r\u00e9sultats de l'estimation\n        return self.logit.summary()\n\n    def cov_matrix(self):\n        \"\"\"\n        Give the covariance matrix of the model\n        Returns\n        -------\n        Cov : Covariance matrix of the model\n        \"\"\"\n        # Matrice de variance-covariance\n        return self.logit.cov_params()\n\n    def margeff(self):\n        \"\"\"\n        Give the marignal effects of the exogenous variables in the model\n        Returns\n        -------\n        MargEff : Marginal effects of the model\n        \"\"\"\n        # Effets marginaux\n        return self.logit.get_margeff().summary_frame()\n</code></pre>"},{"location":"api/LogitClassifier/#igf_toolbox.estimators.classifiers.LogitClassifier.cov_matrix","title":"<code>cov_matrix()</code>","text":"<p>Give the covariance matrix of the model Returns</p> <p>Cov : Covariance matrix of the model</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def cov_matrix(self):\n    \"\"\"\n    Give the covariance matrix of the model\n    Returns\n    -------\n    Cov : Covariance matrix of the model\n    \"\"\"\n    # Matrice de variance-covariance\n    return self.logit.cov_params()\n</code></pre>"},{"location":"api/LogitClassifier/#igf_toolbox.estimators.classifiers.LogitClassifier.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the probit model. Parameters</p> <p>X : array-like of shape (n_samples, n_features)     Training data</p> array-like of shape (n_samples,) or (n_samples, n_targets) <p>Target values. Will be cast to X's dtype if necessary</p>"},{"location":"api/LogitClassifier/#igf_toolbox.estimators.classifiers.LogitClassifier.fit--returns","title":"Returns","text":"<p>self : returns an instance of self</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def fit(self, X, y):\n    \"\"\"\n    Fit the probit model.\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training data\n\n    y : array-like of shape (n_samples,) or (n_samples, n_targets)\n        Target values. Will be cast to X's dtype if necessary\n    Returns\n    -------\n    self : returns an instance of self\n    \"\"\"\n    # Entrainement du mod\u00e8le\n    if self.regularized:\n        self.logit = Logit(y, X).fit_regularized(\n            method=self.method,\n            alpha=self.alpha,\n            trim_mode=self.trim_mode,\n            maxiter=100000,\n        )\n    else:\n        self.logit = Logit(y, X).fit()\n    # Extraction du coefficient et des classes\n    self._coef = self.logit.params\n    self.classes_ = np.unique(y)\n\n    return self\n</code></pre>"},{"location":"api/LogitClassifier/#igf_toolbox.estimators.classifiers.LogitClassifier.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Return a dictionnary containing the current parameters of the model Parameters</p> bool, default=True <p>Whether to return an independent copy of the parameters</p>"},{"location":"api/LogitClassifier/#igf_toolbox.estimators.classifiers.LogitClassifier.get_params--returns","title":"Returns","text":"dictionnary <p>Dictionnary containing the name of the parameter and its corresponding value</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def get_params(self, deep: Optional[bool] = True) -&gt; dict:\n    \"\"\"\n    Return a dictionnary containing the current parameters of the model\n    Parameters\n    ----------\n\n    deep : bool, default=True\n        Whether to return an independent copy of the parameters\n    Returns\n    -------\n\n    DictParams : dictionnary\n        Dictionnary containing the name of the parameter and its corresponding value\n    \"\"\"\n    # Retourne les param\u00e8tres\n    return {\n        \"alpha\": self.alpha,\n        \"scoring\": self.scoring,\n        \"regularized\": self.regularized,\n        \"method\": self.method,\n        \"trim_mode\": self.trim_mode,\n        \"proba_threshold\": self.proba_threshold,\n    }\n</code></pre>"},{"location":"api/LogitClassifier/#igf_toolbox.estimators.classifiers.LogitClassifier.margeff","title":"<code>margeff()</code>","text":"<p>Give the marignal effects of the exogenous variables in the model Returns</p> <p>MargEff : Marginal effects of the model</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def margeff(self):\n    \"\"\"\n    Give the marignal effects of the exogenous variables in the model\n    Returns\n    -------\n    MargEff : Marginal effects of the model\n    \"\"\"\n    # Effets marginaux\n    return self.logit.get_margeff().summary_frame()\n</code></pre>"},{"location":"api/LogitClassifier/#igf_toolbox.estimators.classifiers.LogitClassifier.predict","title":"<code>predict(X)</code>","text":"<p>Predict class for X. Parameters</p> <p>X : array-like of shape (n_samples, n_features)     The input samples. Internally, it will be converted to     <code>dtype=np.float32</code> and if a sparse matrix is provided     to a sparse <code>csr_matrix</code>. Returns</p> <p>Prediction : array-like of shape (n_samples,) or (n_samples, n_outputs)     The predicted classes.</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def predict(self, X):\n    \"\"\"Predict class for X.\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples. Internally, it will be converted to\n        ``dtype=np.float32`` and if a sparse matrix is provided\n        to a sparse ``csr_matrix``.\n    Returns\n    -------\n    Prediction : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        The predicted classes.\n    \"\"\"\n    # Pr\u00e9diction\n    if self.proba_threshold:\n        return (self.logit.predict(X) &gt;= self.proba_threshold) * 1\n    return self.logit.predict(X)\n</code></pre>"},{"location":"api/LogitClassifier/#igf_toolbox.estimators.classifiers.LogitClassifier.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict class associated with the class labelized as one for X. Parameters</p> <p>X : array-like of shape (n_samples, n_features)     The input samples. Internally, it will be converted to     <code>dtype=np.float32</code> and if a sparse matrix is provided     to a sparse <code>csr_matrix</code>. Returns</p> <p>Prediction : array-like of shape (n_samples,) or (n_samples, n_outputs)     The predicted probabilities.</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def predict_proba(self, X):\n    \"\"\"Predict class associated with the class labelized as one for X.\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples. Internally, it will be converted to\n        ``dtype=np.float32`` and if a sparse matrix is provided\n        to a sparse ``csr_matrix``.\n    Returns\n    -------\n    Prediction : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        The predicted probabilities.\n    \"\"\"\n    # Pr\u00e9diction de la probabilit\u00e9 d'appartenance \u00e0 chacune des classes\n    proba = []\n    XArray = np.array(X)\n    for i in range(len(XArray)):\n        proba.append(\n            [\n                1 - self.logit.predict(XArray[i, :])[0],\n                self.logit.predict(XArray[i, :])[0],\n            ]\n        )\n    return np.array(proba).reshape(-1, 2)\n</code></pre>"},{"location":"api/LogitClassifier/#igf_toolbox.estimators.classifiers.LogitClassifier.score","title":"<code>score(X_test, y_test)</code>","text":"<p>Calculate the score for the label predicted by the model for X_test et les vrais labels y_test. Parameters</p> <p>X_test : array-like of shape (n_samples, n_features)     The input samples. Internally, it will be converted to     <code>dtype=np.float32</code> and if a sparse matrix is provided     to a sparse <code>csr_matrix</code>. y_test : array-like of shape (n_samples, 1).     The test samples Returns</p> <p>score : float or array-like     The computed score.</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def score(self, X_test, y_test):\n    \"\"\"Calculate the score for the label predicted by the model for X_test et les vrais labels y_test.\n    Parameters\n    ----------\n    X_test : array-like of shape (n_samples, n_features)\n        The input samples. Internally, it will be converted to\n        ``dtype=np.float32`` and if a sparse matrix is provided\n        to a sparse ``csr_matrix``.\n    y_test : array-like of shape (n_samples, 1).\n        The test samples\n    Returns\n    -------\n    score : float or array-like\n        The computed score.\n    \"\"\"\n    # Calcul du ROC-AUC ou de l'Accuracy\n    self.y_pred = ((self.predict(X_test) &gt; 0.25) * 1) * True\n    self.y_true = y_test\n    if self.scoring == \"accuracy\":\n        score = np.count_nonzero(\n            np.add(\n                self.y_pred.tolist(),\n                np.multiply(self.y_true.iloc[:, 0].tolist(), -1),\n            )\n            == 0\n        ) / len(self.y_pred)\n    if self.scoring == \"roc\":\n        from sklearn.metrics import roc_auc_score\n\n        score = roc_auc_score(self.y_true, self.predict(X_test))\n    return score\n</code></pre>"},{"location":"api/LogitClassifier/#igf_toolbox.estimators.classifiers.LogitClassifier.set_params","title":"<code>set_params(**parameters)</code>","text":"<p>Change the parameters of the model Returns</p> <p>self : returns an instance of self</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def set_params(self, **parameters) -&gt; None:\n    \"\"\"\n    Change the parameters of the model\n    Returns\n    -------\n    self : returns an instance of self\n    \"\"\"\n    # Ajout des param\u00e8tres\n    for parameter, value in parameters.items():\n        setattr(self, parameter, value)\n    return self\n</code></pre>"},{"location":"api/LogitClassifier/#igf_toolbox.estimators.classifiers.LogitClassifier.summary","title":"<code>summary()</code>","text":"<p>Give information about the estimation of the logit model Returns</p> <p>Summary : Information about the coefficients estimated by the model</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def summary(self):\n    \"\"\"\n    Give information about the estimation of the logit model\n    Returns\n    -------\n    Summary : Information about the coefficients estimated by the model\n    \"\"\"\n    # R\u00e9sum\u00e9 des r\u00e9sultats de l'estimation\n    return self.logit.summary()\n</code></pre>"},{"location":"api/OaxacaBlinder/","title":"OaxacaBlinder","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Oaxaca-Blinder Decomposition Estimator.</p> <p>This estimator implements the Oaxaca-Blinder decomposition to decompose the difference in means of a dependent variable between two groups into an explained and unexplained component.</p>"},{"location":"api/OaxacaBlinder/#igf_toolbox.estimators.decomposers.OaxacaBlinder--parameters","title":"Parameters:","text":"<p>bifurcate : Series     A binary series indicating the group of each observation.     Observations belonging to the reference group are labeled as 1,     and the other group as 0.</p> {'pooled', 'other'} <p>The method used for coefficient estimation: - 'pooled': Coefficients estimated from the pooled data. - 'other': Coefficients estimated from the reference group.</p>"},{"location":"api/OaxacaBlinder/#igf_toolbox.estimators.decomposers.OaxacaBlinder--attributes","title":"Attributes:","text":"<p>bifurcate : Series     Stored bifurcation series.</p> str <p>Stored method string.</p> DataFrame <p>Features of the reference group.</p> Series <p>Target variable of the reference group.</p> DataFrame <p>Features of the other group.</p> Series <p>Target variable of the other group.</p> LeastSquaresEstimator <p>Estimated linear regression model.</p> Series or array-like <p>Coefficients from the linear regression model.</p>"},{"location":"api/OaxacaBlinder/#igf_toolbox.estimators.decomposers.OaxacaBlinder--examples","title":"Examples:","text":"<p>import pandas as pd data = pd.DataFrame({\"X1\": [1, 2, 3, 4, 5], \"y\": [6, 7, 8, 9, 10], \"group\": [1, 1, 0, 0, 0]}) decomposer = OaxacaBlinder(bifurcate=data[\"group\"], method=\"pooled\") decomposer.fit(data[[\"X1\"]], data[\"y\"]) result = decomposer.decompose()</p> Source code in <code>igf_toolbox/estimators/decomposers.py</code> <pre><code>class OaxacaBlinder(BaseEstimator):\n    \"\"\"\n    Oaxaca-Blinder Decomposition Estimator.\n\n    This estimator implements the Oaxaca-Blinder decomposition to decompose the difference\n    in means of a dependent variable between two groups into an explained and unexplained\n    component.\n\n    Parameters:\n    -----------\n    bifurcate : Series\n        A binary series indicating the group of each observation.\n        Observations belonging to the reference group are labeled as 1,\n        and the other group as 0.\n\n    method : {'pooled', 'other'}\n        The method used for coefficient estimation:\n        - 'pooled': Coefficients estimated from the pooled data.\n        - 'other': Coefficients estimated from the reference group.\n\n    Attributes:\n    -----------\n    bifurcate : Series\n        Stored bifurcation series.\n\n    method : str\n        Stored method string.\n\n    X_group_ref : DataFrame\n        Features of the reference group.\n\n    y_group_ref : Series\n        Target variable of the reference group.\n\n    X_other_group : DataFrame\n        Features of the other group.\n\n    y_other_group : Series\n        Target variable of the other group.\n\n    model : LeastSquaresEstimator\n        Estimated linear regression model.\n\n    _coef : Series or array-like\n        Coefficients from the linear regression model.\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; import pandas as pd\n    &gt;&gt;&gt; data = pd.DataFrame({\"X1\": [1, 2, 3, 4, 5], \"y\": [6, 7, 8, 9, 10], \"group\": [1, 1, 0, 0, 0]})\n    &gt;&gt;&gt; decomposer = OaxacaBlinder(bifurcate=data[\"group\"], method=\"pooled\")\n    &gt;&gt;&gt; decomposer.fit(data[[\"X1\"]], data[\"y\"])\n    &gt;&gt;&gt; result = decomposer.decompose()\n\n    \"\"\"\n\n    def __init__(self, bifurcate: pd.Series, method: str) -&gt; None:\n        # Initialisation des param\u00e8tres\n        # bifurcate est une s\u00e9rie d'indicatrices du jeu de donn\u00e9es parmi les X utilis\u00e9e pour s\u00e9parer les deux groupes\n        # Les observations appartenant au groupe de r\u00e9f\u00e9rence sont not\u00e9e 1\n        self.bifurcate = bifurcate\n        if method in [\"pooled\", \"other\"]:\n            self.method = method\n        else:\n            raise ValueError(\n                \"Unknown method. 'method' should be in ['pooled', 'other']\"\n            )\n\n    def fit(self, X, y) -&gt; None:\n        \"\"\"\n        Fit the OaxacaBlinder decomposer.\n\n        Parameters:\n        -----------\n        X : DataFrame\n            The feature matrix.\n\n        y : Series or array-like\n            The target variable.\n\n        Returns:\n        --------\n        self : OaxacaBlinder\n            The fitted decomposer.\n        \"\"\"\n        # D\u00e9composition des observations\n        self.X_group_ref, self.y_group_ref = (\n            X.loc[self.bifurcate == 1],\n            y.loc[self.bifurcate == 1],\n        )\n        self.X_other_group, self.y_other_group = (\n            X.loc[self.bifurcate == 0],\n            y.loc[self.bifurcate == 0],\n        )\n\n        # Estimation du mod\u00e8le\n        self.model = LeastSquaresEstimator()\n        if self.method == \"pooled\":\n            self.model.fit(X, y)\n        elif self.method == \"other\":\n            self.model.fit(X_group_ref, y_group_ref)\n\n        self._coef = self.model._coef\n\n    def summary(self):\n        \"\"\"\n        Summarize the estimated regression model.\n\n        Returns:\n        --------\n        DataFrame\n            Summary statistics of the estimated regression model.\n        \"\"\"\n        # R\u00e9sum\u00e9 des r\u00e9sultats d'estimation du mod\u00e8le\n        return self.model.summary()\n\n    def rsquared(self):\n        \"\"\"\n        Compute the R-squared statistic.\n\n        Returns:\n        --------\n        DataFrame\n            R-squared and adjusted R-squared values.\n        \"\"\"\n        # R2 du mod\u00e8le\n        return self.model.rsquared()\n\n    def decompose(self, detailed: Optional[bool] = False) -&gt; pd.DataFrame:\n        \"\"\"\n        Apply the Oaxaca-Blinder decomposition.\n\n        Parameters:\n        -----------\n        detailed : bool, default=False\n            If True, the result includes the detailed effect for each feature.\n            If False, only the aggregated effect is returned.\n\n        Returns:\n        --------\n        DataFrame\n            Decomposed effects, either detailed or aggregated, based on the 'detailed' parameter.\n        \"\"\"\n        # Application de la d\u00e9composition de Oaxaca-Blinder\n        if detailed:\n            # Estimation d\u00e9taill\u00e9e de l'effet pour chacun des X\n            # Valoris\u00e9 au niveau du groupe de r\u00e9f\u00e9rence\n            two_fold_decomposition = (\n                (\n                    (self.X_group_ref.mean(axis=0) - self.X_other_group.mean(axis=0))\n                    * self._coef\n                )\n                .to_frame()\n                .rename({0: \"explained effect\"}, axis=1)\n            )\n        else:\n            two_fold_decomposition = [\n                (self.y_group_ref.mean() - self.y_other_group.mean())\n                - (\n                    (self.X_group_ref.mean(axis=0) - self.X_other_group.mean(axis=0))\n                    * self._coef\n                ).sum(),\n                (\n                    (self.X_group_ref.mean(axis=0) - self.X_other_group.mean(axis=0))\n                    * self._coef\n                ).sum(),\n                self.y_group_ref.mean() - self.y_other_group.mean(),\n            ]\n            # Conversion en DataFrame\n            two_fold_decomposition = pd.Series(\n                two_fold_decomposition,\n                index=[\"Unexplained Effect\", \"Explained Effect\", \"Gap\"],\n                name=\"decomposition\",\n            ).to_frame()\n\n        return two_fold_decomposition\n</code></pre>"},{"location":"api/OaxacaBlinder/#igf_toolbox.estimators.decomposers.OaxacaBlinder.decompose","title":"<code>decompose(detailed=False)</code>","text":"<p>Apply the Oaxaca-Blinder decomposition.</p>"},{"location":"api/OaxacaBlinder/#igf_toolbox.estimators.decomposers.OaxacaBlinder.decompose--parameters","title":"Parameters:","text":"<p>detailed : bool, default=False     If True, the result includes the detailed effect for each feature.     If False, only the aggregated effect is returned.</p>"},{"location":"api/OaxacaBlinder/#igf_toolbox.estimators.decomposers.OaxacaBlinder.decompose--returns","title":"Returns:","text":"<p>DataFrame     Decomposed effects, either detailed or aggregated, based on the 'detailed' parameter.</p> Source code in <code>igf_toolbox/estimators/decomposers.py</code> <pre><code>def decompose(self, detailed: Optional[bool] = False) -&gt; pd.DataFrame:\n    \"\"\"\n    Apply the Oaxaca-Blinder decomposition.\n\n    Parameters:\n    -----------\n    detailed : bool, default=False\n        If True, the result includes the detailed effect for each feature.\n        If False, only the aggregated effect is returned.\n\n    Returns:\n    --------\n    DataFrame\n        Decomposed effects, either detailed or aggregated, based on the 'detailed' parameter.\n    \"\"\"\n    # Application de la d\u00e9composition de Oaxaca-Blinder\n    if detailed:\n        # Estimation d\u00e9taill\u00e9e de l'effet pour chacun des X\n        # Valoris\u00e9 au niveau du groupe de r\u00e9f\u00e9rence\n        two_fold_decomposition = (\n            (\n                (self.X_group_ref.mean(axis=0) - self.X_other_group.mean(axis=0))\n                * self._coef\n            )\n            .to_frame()\n            .rename({0: \"explained effect\"}, axis=1)\n        )\n    else:\n        two_fold_decomposition = [\n            (self.y_group_ref.mean() - self.y_other_group.mean())\n            - (\n                (self.X_group_ref.mean(axis=0) - self.X_other_group.mean(axis=0))\n                * self._coef\n            ).sum(),\n            (\n                (self.X_group_ref.mean(axis=0) - self.X_other_group.mean(axis=0))\n                * self._coef\n            ).sum(),\n            self.y_group_ref.mean() - self.y_other_group.mean(),\n        ]\n        # Conversion en DataFrame\n        two_fold_decomposition = pd.Series(\n            two_fold_decomposition,\n            index=[\"Unexplained Effect\", \"Explained Effect\", \"Gap\"],\n            name=\"decomposition\",\n        ).to_frame()\n\n    return two_fold_decomposition\n</code></pre>"},{"location":"api/OaxacaBlinder/#igf_toolbox.estimators.decomposers.OaxacaBlinder.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the OaxacaBlinder decomposer.</p>"},{"location":"api/OaxacaBlinder/#igf_toolbox.estimators.decomposers.OaxacaBlinder.fit--parameters","title":"Parameters:","text":"<p>X : DataFrame     The feature matrix.</p> Series or array-like <p>The target variable.</p>"},{"location":"api/OaxacaBlinder/#igf_toolbox.estimators.decomposers.OaxacaBlinder.fit--returns","title":"Returns:","text":"<p>self : OaxacaBlinder     The fitted decomposer.</p> Source code in <code>igf_toolbox/estimators/decomposers.py</code> <pre><code>def fit(self, X, y) -&gt; None:\n    \"\"\"\n    Fit the OaxacaBlinder decomposer.\n\n    Parameters:\n    -----------\n    X : DataFrame\n        The feature matrix.\n\n    y : Series or array-like\n        The target variable.\n\n    Returns:\n    --------\n    self : OaxacaBlinder\n        The fitted decomposer.\n    \"\"\"\n    # D\u00e9composition des observations\n    self.X_group_ref, self.y_group_ref = (\n        X.loc[self.bifurcate == 1],\n        y.loc[self.bifurcate == 1],\n    )\n    self.X_other_group, self.y_other_group = (\n        X.loc[self.bifurcate == 0],\n        y.loc[self.bifurcate == 0],\n    )\n\n    # Estimation du mod\u00e8le\n    self.model = LeastSquaresEstimator()\n    if self.method == \"pooled\":\n        self.model.fit(X, y)\n    elif self.method == \"other\":\n        self.model.fit(X_group_ref, y_group_ref)\n\n    self._coef = self.model._coef\n</code></pre>"},{"location":"api/OaxacaBlinder/#igf_toolbox.estimators.decomposers.OaxacaBlinder.rsquared","title":"<code>rsquared()</code>","text":"<p>Compute the R-squared statistic.</p>"},{"location":"api/OaxacaBlinder/#igf_toolbox.estimators.decomposers.OaxacaBlinder.rsquared--returns","title":"Returns:","text":"<p>DataFrame     R-squared and adjusted R-squared values.</p> Source code in <code>igf_toolbox/estimators/decomposers.py</code> <pre><code>def rsquared(self):\n    \"\"\"\n    Compute the R-squared statistic.\n\n    Returns:\n    --------\n    DataFrame\n        R-squared and adjusted R-squared values.\n    \"\"\"\n    # R2 du mod\u00e8le\n    return self.model.rsquared()\n</code></pre>"},{"location":"api/OaxacaBlinder/#igf_toolbox.estimators.decomposers.OaxacaBlinder.summary","title":"<code>summary()</code>","text":"<p>Summarize the estimated regression model.</p>"},{"location":"api/OaxacaBlinder/#igf_toolbox.estimators.decomposers.OaxacaBlinder.summary--returns","title":"Returns:","text":"<p>DataFrame     Summary statistics of the estimated regression model.</p> Source code in <code>igf_toolbox/estimators/decomposers.py</code> <pre><code>def summary(self):\n    \"\"\"\n    Summarize the estimated regression model.\n\n    Returns:\n    --------\n    DataFrame\n        Summary statistics of the estimated regression model.\n    \"\"\"\n    # R\u00e9sum\u00e9 des r\u00e9sultats d'estimation du mod\u00e8le\n    return self.model.summary()\n</code></pre>"},{"location":"api/OneHotEncoder/","title":"OneHotEncoder","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>Custom OneHotEncoder for pandas DataFrames with inverse_transform support.</p> <p>This class provides a wrapper around pandas' get_dummies method for one-hot encoding with capabilities to inverse transform the one-hot encoded data back to its original categorical form.</p> <p>Parameters: - columns (iterable of str): Iterable of column names in the DataFrame to be one-hot encoded. - dummy_na (bool) : Boolean indicating whether to encode Nan. - dtype (type) : Type of the encoding dummies columns (should be in [bool, float, int]).</p> <p>Attributes: - list_col_dummy (iterable of str): Iterable of column names after one-hot encoding.</p> <p>Methods: - fit(X, y=None): Fit method. Does nothing in this transformer and returns self. - transform(X, y=None): Perform one-hot encoding on specified columns in the DataFrame. - inverse_transform(X, y=None): Convert one-hot encoded DataFrame back to its original categorical form.</p> <p>Example:</p> <p>encoder = OneHotEncoder(columns=['color']) df = pd.DataFrame({'color': ['red', 'green', 'blue'], 'value': [10, 20, 30]}) encoded_df = encoder.transform(df) original_df = encoder.inverse_transform(encoded_df)</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>class OneHotEncoder(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Custom OneHotEncoder for pandas DataFrames with inverse_transform support.\n\n    This class provides a wrapper around pandas' get_dummies method for one-hot encoding\n    with capabilities to inverse transform the one-hot encoded data back to its original categorical form.\n\n    Parameters:\n    - columns (iterable of str): Iterable of column names in the DataFrame to be one-hot encoded.\n    - dummy_na (bool) : Boolean indicating whether to encode Nan.\n    - dtype (type) : Type of the encoding dummies columns (should be in [bool, float, int]).\n\n    Attributes:\n    - list_col_dummy (iterable of str): Iterable of column names after one-hot encoding.\n\n    Methods:\n    - fit(X, y=None): Fit method. Does nothing in this transformer and returns self.\n    - transform(X, y=None): Perform one-hot encoding on specified columns in the DataFrame.\n    - inverse_transform(X, y=None): Convert one-hot encoded DataFrame back to its original categorical form.\n\n    Example:\n    &gt;&gt;&gt; encoder = OneHotEncoder(columns=['color'])\n    &gt;&gt;&gt; df = pd.DataFrame({'color': ['red', 'green', 'blue'], 'value': [10, 20, 30]})\n    &gt;&gt;&gt; encoded_df = encoder.transform(df)\n    &gt;&gt;&gt; original_df = encoder.inverse_transform(encoded_df)\n    \"\"\"\n\n    def __init__(\n        self, columns: Iterable[str], dummy_na: bool = False, dtype: type = bool\n    ) -&gt; None:\n        # Initialisation des param\u00e8tres\n        self.columns = columns\n        self.dummy_na = dummy_na\n        self.dtype = dtype\n\n    def fit(self, X, y=None) -&gt; None:\n        \"\"\"\n        Fit method for OneHotEncoder.\n\n        Since the encoder is stateless (i.e., does not learn from the data), this method\n        does nothing and simply returns the encoder instance.\n\n        Parameters:\n        - X (pd.DataFrame): The input data.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - self: The encoder instance.\n        \"\"\"\n        # Construction du jeu de donn\u00e9es avec les dummies\n        list_data_dummies = [\n            pd.get_dummies(\n                X[col],\n                prefix=col,\n                prefix_sep=\" - \",\n                dummy_na=self.dummy_na,\n                dtype=self.dtype,\n            )\n            for col in self.columns\n        ]\n\n        if len(list_data_dummies) &gt; 0:\n            # Concat\u00e9nation des variables binaires\n            data_dummies = pd.concat(list_data_dummies, axis=1, join=\"outer\")\n\n            # Sauvegarde des colonnes pour pouvoir inverser la transformation\n            self.list_col_dummy = data_dummies.columns.to_list()\n        else:\n            self.list_col_dummy = []\n\n        return self\n\n    # / ! \\ A revoir car la syntaxe de get_dummies a \u00e9volu\u00e9 et devrait permettre de rendre la fonction plus concise, en particulier en lui donnant directement le DataFrame en argument\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Perform one-hot encoding on the specified columns in the DataFrame.\n\n        This method takes a DataFrame and one-hot encodes the columns specified during\n        the encoder's instantiation. Other columns remain unchanged.\n\n        Parameters:\n        - X (pd.DataFrame): The input data containing columns to be one-hot encoded.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - pd.DataFrame: A DataFrame with the specified columns one-hot encoded.\n        \"\"\"\n        # Construction du jeu de donn\u00e9es avec les dummies\n        list_data_dummies = [\n            pd.get_dummies(\n                X[col],\n                prefix=col,\n                prefix_sep=\" - \",\n                dummy_na=self.dummy_na,\n                dtype=self.dtype,\n            )\n            for col in self.columns\n        ]\n\n        if len(list_data_dummies) &gt; 0:\n            # Concat\u00e9nation des variables binaires\n            data_dummies = pd.concat(list_data_dummies, axis=1, join=\"outer\").reindex(\n                columns=pd.Index(data=self.list_col_dummy), method=None\n            )\n            # Compl\u00e9tion des Nan\n            if self.dtype == bool:\n                data_dummies.fillna(value=bool, inplace=True)\n            elif self.dtype == float:\n                data_dummies.fillna(value=0.0, inplace=True)\n            elif self.dtype == int:\n                data_dummies.fillna(value=0, inplace=True)\n            else:\n                raise ValueError(\n                    f\"Unknown dtype : {self.dtype}, should be in [bool, float, int]\"\n                )\n\n            # Construction du jeu de donn\u00e9es r\u00e9sultat\n            X_transformed = pd.concat(\n                [X.drop(self.columns, axis=1), data_dummies], axis=1, join=\"outer\"\n            )\n        else:\n            # Sauvegarde des colonnes pour pouvoir inverser la transformation\n            self.list_col_dummy = []\n\n            # Construction du jeu de donn\u00e9es r\u00e9sultat\n            X_transformed = X.copy()\n\n        return X_transformed\n\n    def inverse_transform(self, X: pd.DataFrame, y=None) -&gt; None:\n        \"\"\"\n        Convert a one-hot encoded DataFrame back to its original categorical form.\n\n        This method takes a DataFrame with one-hot encoded columns and transforms it\n        back to its original categorical form using the columns specified during\n        the encoder's instantiation.\n\n        Parameters:\n        - X (pd.DataFrame): The one-hot encoded data.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - pd.DataFrame: A DataFrame with the original categorical data.\n        \"\"\"\n        if len(self.list_col_dummy) &gt; 0:\n            # Cr\u00e9ation d'un dataframe d'appariement\n            data_dummies_categories = pd.DataFrame(\n                data=self.list_col_dummy,\n                index=np.arange(len(self.list_col_dummy)),\n                columns=[\"dummies\"],\n            )\n            data_dummies_categories = data_dummies_categories.apply(\n                func=lambda x: _separate_category_modality_from_dummy(value=x), axis=1\n            )\n\n            # Initialisation de la liste r\u00e9sultat\n            list_data_categories = []\n\n            # Recherche de l'index du maximum\n            for category in data_dummies_categories[\"categories\"].unique():\n                # Liste des dummies cr\u00e9es appartenant \u00e0 la cat\u00e9gorie\n                list_col_dummies_category = data_dummies_categories.loc[\n                    data_dummies_categories[\"categories\"] == category, \"dummies\"\n                ].tolist()\n                # D\u00e9termination de la dummy majoritaire pour chaque classe\n                data_major_dummy_category = (\n                    X[list_col_dummies_category]\n                    .idxmax(axis=\"columns\")\n                    .to_frame()\n                    .rename({0: \"dummies_category\"}, axis=1)\n                )\n                # Modification du nom de l'index pour s'assurer de la transformation\n                data_major_dummy_category.index.name = \"index\"\n                # Appariement avec les modalit\u00e9s\n                data_category = (\n                    pd.merge(\n                        left=data_major_dummy_category.reset_index(),\n                        right=data_dummies_categories[[\"dummies\", \"modalities\"]],\n                        left_on=\"dummies_category\",\n                        right_on=\"dummies\",\n                        how=\"left\",\n                        validate=\"many_to_one\",\n                    )\n                    .set_index(\"index\")[[\"modalities\"]]\n                    .rename({\"modalities\": category}, axis=1)\n                )\n                # Ajout \u00e0 la liste r\u00e9sultat\n                list_data_categories.append(data_category)\n\n            # Concat\u00e9nation des variables cat\u00e9gorielles\n            data_categories = pd.concat(list_data_categories, axis=1, join=\"outer\")\n\n            # Construction du jeu de donn\u00e9es r\u00e9sultat\n            X_inverse_transformed = pd.concat(\n                [X.drop(self.list_col_dummy, axis=1), data_categories],\n                axis=1,\n                join=\"outer\",\n            )\n        else:\n            X_inverse_transformed = X.copy()\n\n        return X_inverse_transformed\n</code></pre>"},{"location":"api/OneHotEncoder/#igf_toolbox.preprocessing.transformers.OneHotEncoder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit method for OneHotEncoder.</p> <p>Since the encoder is stateless (i.e., does not learn from the data), this method does nothing and simply returns the encoder instance.</p> <p>Parameters: - X (pd.DataFrame): The input data. - y (ignored): This parameter is ignored.</p> <p>Returns: - self: The encoder instance.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def fit(self, X, y=None) -&gt; None:\n    \"\"\"\n    Fit method for OneHotEncoder.\n\n    Since the encoder is stateless (i.e., does not learn from the data), this method\n    does nothing and simply returns the encoder instance.\n\n    Parameters:\n    - X (pd.DataFrame): The input data.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - self: The encoder instance.\n    \"\"\"\n    # Construction du jeu de donn\u00e9es avec les dummies\n    list_data_dummies = [\n        pd.get_dummies(\n            X[col],\n            prefix=col,\n            prefix_sep=\" - \",\n            dummy_na=self.dummy_na,\n            dtype=self.dtype,\n        )\n        for col in self.columns\n    ]\n\n    if len(list_data_dummies) &gt; 0:\n        # Concat\u00e9nation des variables binaires\n        data_dummies = pd.concat(list_data_dummies, axis=1, join=\"outer\")\n\n        # Sauvegarde des colonnes pour pouvoir inverser la transformation\n        self.list_col_dummy = data_dummies.columns.to_list()\n    else:\n        self.list_col_dummy = []\n\n    return self\n</code></pre>"},{"location":"api/OneHotEncoder/#igf_toolbox.preprocessing.transformers.OneHotEncoder.inverse_transform","title":"<code>inverse_transform(X, y=None)</code>","text":"<p>Convert a one-hot encoded DataFrame back to its original categorical form.</p> <p>This method takes a DataFrame with one-hot encoded columns and transforms it back to its original categorical form using the columns specified during the encoder's instantiation.</p> <p>Parameters: - X (pd.DataFrame): The one-hot encoded data. - y (ignored): This parameter is ignored.</p> <p>Returns: - pd.DataFrame: A DataFrame with the original categorical data.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def inverse_transform(self, X: pd.DataFrame, y=None) -&gt; None:\n    \"\"\"\n    Convert a one-hot encoded DataFrame back to its original categorical form.\n\n    This method takes a DataFrame with one-hot encoded columns and transforms it\n    back to its original categorical form using the columns specified during\n    the encoder's instantiation.\n\n    Parameters:\n    - X (pd.DataFrame): The one-hot encoded data.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - pd.DataFrame: A DataFrame with the original categorical data.\n    \"\"\"\n    if len(self.list_col_dummy) &gt; 0:\n        # Cr\u00e9ation d'un dataframe d'appariement\n        data_dummies_categories = pd.DataFrame(\n            data=self.list_col_dummy,\n            index=np.arange(len(self.list_col_dummy)),\n            columns=[\"dummies\"],\n        )\n        data_dummies_categories = data_dummies_categories.apply(\n            func=lambda x: _separate_category_modality_from_dummy(value=x), axis=1\n        )\n\n        # Initialisation de la liste r\u00e9sultat\n        list_data_categories = []\n\n        # Recherche de l'index du maximum\n        for category in data_dummies_categories[\"categories\"].unique():\n            # Liste des dummies cr\u00e9es appartenant \u00e0 la cat\u00e9gorie\n            list_col_dummies_category = data_dummies_categories.loc[\n                data_dummies_categories[\"categories\"] == category, \"dummies\"\n            ].tolist()\n            # D\u00e9termination de la dummy majoritaire pour chaque classe\n            data_major_dummy_category = (\n                X[list_col_dummies_category]\n                .idxmax(axis=\"columns\")\n                .to_frame()\n                .rename({0: \"dummies_category\"}, axis=1)\n            )\n            # Modification du nom de l'index pour s'assurer de la transformation\n            data_major_dummy_category.index.name = \"index\"\n            # Appariement avec les modalit\u00e9s\n            data_category = (\n                pd.merge(\n                    left=data_major_dummy_category.reset_index(),\n                    right=data_dummies_categories[[\"dummies\", \"modalities\"]],\n                    left_on=\"dummies_category\",\n                    right_on=\"dummies\",\n                    how=\"left\",\n                    validate=\"many_to_one\",\n                )\n                .set_index(\"index\")[[\"modalities\"]]\n                .rename({\"modalities\": category}, axis=1)\n            )\n            # Ajout \u00e0 la liste r\u00e9sultat\n            list_data_categories.append(data_category)\n\n        # Concat\u00e9nation des variables cat\u00e9gorielles\n        data_categories = pd.concat(list_data_categories, axis=1, join=\"outer\")\n\n        # Construction du jeu de donn\u00e9es r\u00e9sultat\n        X_inverse_transformed = pd.concat(\n            [X.drop(self.list_col_dummy, axis=1), data_categories],\n            axis=1,\n            join=\"outer\",\n        )\n    else:\n        X_inverse_transformed = X.copy()\n\n    return X_inverse_transformed\n</code></pre>"},{"location":"api/OneHotEncoder/#igf_toolbox.preprocessing.transformers.OneHotEncoder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Perform one-hot encoding on the specified columns in the DataFrame.</p> <p>This method takes a DataFrame and one-hot encodes the columns specified during the encoder's instantiation. Other columns remain unchanged.</p> <p>Parameters: - X (pd.DataFrame): The input data containing columns to be one-hot encoded. - y (ignored): This parameter is ignored.</p> <p>Returns: - pd.DataFrame: A DataFrame with the specified columns one-hot encoded.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform one-hot encoding on the specified columns in the DataFrame.\n\n    This method takes a DataFrame and one-hot encodes the columns specified during\n    the encoder's instantiation. Other columns remain unchanged.\n\n    Parameters:\n    - X (pd.DataFrame): The input data containing columns to be one-hot encoded.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - pd.DataFrame: A DataFrame with the specified columns one-hot encoded.\n    \"\"\"\n    # Construction du jeu de donn\u00e9es avec les dummies\n    list_data_dummies = [\n        pd.get_dummies(\n            X[col],\n            prefix=col,\n            prefix_sep=\" - \",\n            dummy_na=self.dummy_na,\n            dtype=self.dtype,\n        )\n        for col in self.columns\n    ]\n\n    if len(list_data_dummies) &gt; 0:\n        # Concat\u00e9nation des variables binaires\n        data_dummies = pd.concat(list_data_dummies, axis=1, join=\"outer\").reindex(\n            columns=pd.Index(data=self.list_col_dummy), method=None\n        )\n        # Compl\u00e9tion des Nan\n        if self.dtype == bool:\n            data_dummies.fillna(value=bool, inplace=True)\n        elif self.dtype == float:\n            data_dummies.fillna(value=0.0, inplace=True)\n        elif self.dtype == int:\n            data_dummies.fillna(value=0, inplace=True)\n        else:\n            raise ValueError(\n                f\"Unknown dtype : {self.dtype}, should be in [bool, float, int]\"\n            )\n\n        # Construction du jeu de donn\u00e9es r\u00e9sultat\n        X_transformed = pd.concat(\n            [X.drop(self.columns, axis=1), data_dummies], axis=1, join=\"outer\"\n        )\n    else:\n        # Sauvegarde des colonnes pour pouvoir inverser la transformation\n        self.list_col_dummy = []\n\n        # Construction du jeu de donn\u00e9es r\u00e9sultat\n        X_transformed = X.copy()\n\n    return X_transformed\n</code></pre>"},{"location":"api/PanelLeastSquaresEstimator/","title":"PanelLeastSquaresEstimator","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>An estimator for panel data using Ordinary Least Squares (OLS).</p>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator--parameters","title":"Parameters","text":"<p>entity_effects : bool     Whether to include entity (fixed) effects in the model. time_effects : bool     Whether to include time effects in the model. drop_absorbed : bool, optional (default=False)     If true, drops variables that are fully absorbed by the entity or time effects. cov_type : str, optional (default='unadjusted')     Type of covariance matrix estimator to use.</p>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator--attributes","title":"Attributes","text":"<p>model : PanelOLS     The fitted model.</p>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator--methods","title":"Methods","text":"<p>fit(X, y, sample_weight=None):     Fits the model using the provided data. predict(X):     Predicts the response variable using the provided data. summary():     Returns a summary of the regression results. rsquared():     Returns various R^2 measures for the model. estimated_effects():     Returns the estimated entity and time effects.</p> Source code in <code>igf_toolbox/estimators/regressors.py</code> <pre><code>class PanelLeastSquaresEstimator(BaseEstimator):\n    \"\"\"\n    An estimator for panel data using Ordinary Least Squares (OLS).\n\n    Parameters\n    ----------\n    entity_effects : bool\n        Whether to include entity (fixed) effects in the model.\n    time_effects : bool\n        Whether to include time effects in the model.\n    drop_absorbed : bool, optional (default=False)\n        If true, drops variables that are fully absorbed by the entity or time effects.\n    cov_type : str, optional (default='unadjusted')\n        Type of covariance matrix estimator to use.\n\n    Attributes\n    ----------\n    model : PanelOLS\n        The fitted model.\n\n    Methods\n    -------\n    fit(X, y, sample_weight=None):\n        Fits the model using the provided data.\n    predict(X):\n        Predicts the response variable using the provided data.\n    summary():\n        Returns a summary of the regression results.\n    rsquared():\n        Returns various R^2 measures for the model.\n    estimated_effects():\n        Returns the estimated entity and time effects.\n    \"\"\"\n\n    def __init__(\n        self,\n        entity_effects: bool,\n        time_effects: bool,\n        drop_absorbed: Optional[bool] = False,\n        check_rank: Optional[bool] = True,\n        cov_type: Optional[str] = \"unadjusted\",\n    ) -&gt; None:\n        # Initialisation des valeurs des bool\u00e9ens indiquant si les effets fixes individuels et temporels doivent \u00eatre trait\u00e9s\n        self.entity_effects = entity_effects\n        self.time_effects = time_effects\n        self.drop_absorbed = drop_absorbed\n        # V\u00e9rification du rang\n        self.check_rank = check_rank\n        # Type d'estimation de la convariance\n        self.cov_type = cov_type\n\n        # return self\n\n    def fit(self, X, y, sample_weight: Optional[Union[pd.Series, None]] = None) -&gt; None:\n        \"\"\"\n        Fit the model using the provided data.\n\n        Parameters\n        ----------\n        X : DataFrame\n            Feature matrix.\n        y : Series\n            Response variable.\n        sample_weight : Series, optional\n            Weights for each observation.\n\n        Returns\n        -------\n        self : PanelLeastSquaresEstimator\n            The instance itself.\n        \"\"\"\n        # Par convention les Entity X Year index sont les deux premi\u00e8res colonnes si X est un ndarray\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X).set_index([0, 1])\n\n        # Entrainement du mod\u00e8le\n        self.model = PanelOLS(\n            dependent=y,\n            exog=X,\n            weights=sample_weight,\n            entity_effects=self.entity_effects,\n            time_effects=self.time_effects,\n            drop_absorbed=self.drop_absorbed,\n        ).fit(cov_type=self.cov_type)\n        return self\n\n    def predict(self, X):\n        \"\"\"\n        Predict the response variable using the provided data.\n\n        Parameters\n        ----------\n        X : DataFrame\n            Feature matrix to predict response for.\n\n        Returns\n        -------\n        Series\n            Predicted values.\n        \"\"\"\n        # Par convention les Entity X Year index sont les deux premi\u00e8res colonnes si X est un ndarray\n        if isinstance(X, np.ndarray):\n            X = pd.DataFrame(X).set_index([0, 1])\n\n        return self.model.predict(X)\n\n    def summary(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns a summary of the regression results.\n\n        Returns\n        -------\n        DataFrame\n            A DataFrame with coefficients, p-values, and 95% confidence intervals.\n        \"\"\"\n        # Description des r\u00e9sultats du mod\u00e8le\n        coefs = self.model.params.to_frame().rename(\n            {\"parameter\": \"Coefficients\"}, axis=1\n        )\n        p_values = self.model.pvalues.to_frame().rename({\"pvalue\": \"P-valeurs\"}, axis=1)\n        ic_95 = self.model.conf_int(level=0.95).rename(\n            {\"lower\": \"ICg\", \"upper\": \"ICd\"}, axis=1\n        )\n\n        # Concat\u00e9nation des r\u00e9sultats\n        data_res = pd.concat([coefs, p_values, ic_95], axis=1, join=\"outer\")\n\n        return data_res\n\n    def rsquared(self) -&gt; pd.DataFrame:\n        \"\"\"\n        Returns various R^2 measures for the model.\n\n        Returns\n        -------\n        DataFrame\n            A DataFrame containing various R^2 measures and the number of observations.\n        \"\"\"\n        return pd.DataFrame(\n            data=[\n                [\n                    self.model.nobs,\n                    self.model.rsquared,\n                    self.model.rsquared_inclusive,\n                    self.model.rsquared_overall,\n                    self.model.rsquared_between,\n                    self.model.rsquared_within,\n                ]\n            ],\n            index=[0],\n            columns=[\n                \"n_observations\",\n                \"R2\",\n                \"R2 - Effets fixes inclus\",\n                \"R2 - Effets fixes exclus\",\n                \"R2 - Purg\u00e9 des effets temporels\",\n                \"R2 - Purg\u00e9 des effets individuels\",\n            ],\n        )\n\n    def estimated_effects(self):\n        \"\"\"\n        Returns the estimated entity and time effects.\n\n        Returns\n        -------\n        DataFrame\n            A DataFrame containing the estimated entity and time effects.\n        \"\"\"\n        return self.model.estimated_effects\n</code></pre>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator.estimated_effects","title":"<code>estimated_effects()</code>","text":"<p>Returns the estimated entity and time effects.</p>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator.estimated_effects--returns","title":"Returns","text":"<p>DataFrame     A DataFrame containing the estimated entity and time effects.</p> Source code in <code>igf_toolbox/estimators/regressors.py</code> <pre><code>def estimated_effects(self):\n    \"\"\"\n    Returns the estimated entity and time effects.\n\n    Returns\n    -------\n    DataFrame\n        A DataFrame containing the estimated entity and time effects.\n    \"\"\"\n    return self.model.estimated_effects\n</code></pre>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator.fit","title":"<code>fit(X, y, sample_weight=None)</code>","text":"<p>Fit the model using the provided data.</p>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator.fit--parameters","title":"Parameters","text":"<p>X : DataFrame     Feature matrix. y : Series     Response variable. sample_weight : Series, optional     Weights for each observation.</p>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator.fit--returns","title":"Returns","text":"<p>self : PanelLeastSquaresEstimator     The instance itself.</p> Source code in <code>igf_toolbox/estimators/regressors.py</code> <pre><code>def fit(self, X, y, sample_weight: Optional[Union[pd.Series, None]] = None) -&gt; None:\n    \"\"\"\n    Fit the model using the provided data.\n\n    Parameters\n    ----------\n    X : DataFrame\n        Feature matrix.\n    y : Series\n        Response variable.\n    sample_weight : Series, optional\n        Weights for each observation.\n\n    Returns\n    -------\n    self : PanelLeastSquaresEstimator\n        The instance itself.\n    \"\"\"\n    # Par convention les Entity X Year index sont les deux premi\u00e8res colonnes si X est un ndarray\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X).set_index([0, 1])\n\n    # Entrainement du mod\u00e8le\n    self.model = PanelOLS(\n        dependent=y,\n        exog=X,\n        weights=sample_weight,\n        entity_effects=self.entity_effects,\n        time_effects=self.time_effects,\n        drop_absorbed=self.drop_absorbed,\n    ).fit(cov_type=self.cov_type)\n    return self\n</code></pre>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator.predict","title":"<code>predict(X)</code>","text":"<p>Predict the response variable using the provided data.</p>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator.predict--parameters","title":"Parameters","text":"<p>X : DataFrame     Feature matrix to predict response for.</p>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator.predict--returns","title":"Returns","text":"<p>Series     Predicted values.</p> Source code in <code>igf_toolbox/estimators/regressors.py</code> <pre><code>def predict(self, X):\n    \"\"\"\n    Predict the response variable using the provided data.\n\n    Parameters\n    ----------\n    X : DataFrame\n        Feature matrix to predict response for.\n\n    Returns\n    -------\n    Series\n        Predicted values.\n    \"\"\"\n    # Par convention les Entity X Year index sont les deux premi\u00e8res colonnes si X est un ndarray\n    if isinstance(X, np.ndarray):\n        X = pd.DataFrame(X).set_index([0, 1])\n\n    return self.model.predict(X)\n</code></pre>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator.rsquared","title":"<code>rsquared()</code>","text":"<p>Returns various R^2 measures for the model.</p>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator.rsquared--returns","title":"Returns","text":"<p>DataFrame     A DataFrame containing various R^2 measures and the number of observations.</p> Source code in <code>igf_toolbox/estimators/regressors.py</code> <pre><code>def rsquared(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns various R^2 measures for the model.\n\n    Returns\n    -------\n    DataFrame\n        A DataFrame containing various R^2 measures and the number of observations.\n    \"\"\"\n    return pd.DataFrame(\n        data=[\n            [\n                self.model.nobs,\n                self.model.rsquared,\n                self.model.rsquared_inclusive,\n                self.model.rsquared_overall,\n                self.model.rsquared_between,\n                self.model.rsquared_within,\n            ]\n        ],\n        index=[0],\n        columns=[\n            \"n_observations\",\n            \"R2\",\n            \"R2 - Effets fixes inclus\",\n            \"R2 - Effets fixes exclus\",\n            \"R2 - Purg\u00e9 des effets temporels\",\n            \"R2 - Purg\u00e9 des effets individuels\",\n        ],\n    )\n</code></pre>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator.summary","title":"<code>summary()</code>","text":"<p>Returns a summary of the regression results.</p>"},{"location":"api/PanelLeastSquaresEstimator/#igf_toolbox.estimators.regressors.PanelLeastSquaresEstimator.summary--returns","title":"Returns","text":"<p>DataFrame     A DataFrame with coefficients, p-values, and 95% confidence intervals.</p> Source code in <code>igf_toolbox/estimators/regressors.py</code> <pre><code>def summary(self) -&gt; pd.DataFrame:\n    \"\"\"\n    Returns a summary of the regression results.\n\n    Returns\n    -------\n    DataFrame\n        A DataFrame with coefficients, p-values, and 95% confidence intervals.\n    \"\"\"\n    # Description des r\u00e9sultats du mod\u00e8le\n    coefs = self.model.params.to_frame().rename(\n        {\"parameter\": \"Coefficients\"}, axis=1\n    )\n    p_values = self.model.pvalues.to_frame().rename({\"pvalue\": \"P-valeurs\"}, axis=1)\n    ic_95 = self.model.conf_int(level=0.95).rename(\n        {\"lower\": \"ICg\", \"upper\": \"ICd\"}, axis=1\n    )\n\n    # Concat\u00e9nation des r\u00e9sultats\n    data_res = pd.concat([coefs, p_values, ic_95], axis=1, join=\"outer\")\n\n    return data_res\n</code></pre>"},{"location":"api/PrimarySecretStatController/","title":"PrimarySecretStatController","text":"<p>               Bases: <code>object</code></p> <p>Control primary statistical secrets in a dataset.</p> <p>This class provides methods to ensure that specific statistical criteria (secret statistics) are maintained in a dataset. The criteria are based on predefined thresholds.</p> <p>Attributes: - var_individu (str or None): The variable representing individuals. - var_entreprise (str or None): The variable representing companies. - threshold_secret_stat_effectif_individu (int or None): The threshold for primary statistical secrecy control (individuals). - threshold_secret_stat_effectif_entreprise (int or None): The threshold for primary statistical secrecy control (companies). - threshold_secret_stat_contrib_individu (float or None): The threshold for contribution statistical secrecy control (individuals). - threshold_secret_stat_contrib_entreprise (float or None): The threshold for contribution statistical secrecy control (companies).</p> <ul> <li>_control_secret_stat_effectif(data, var_of_interest, threshold): Check if the number of unique values of a variable in the dataset meet   a minimum threshold. Adds a boolean column indicating whether the data in each row respects the secret   statistic criteria or not.</li> <li>_control_secret_stat_contrib(data, var_of_interest, threshold): Check if the values of a variable in the dataset   exceed a maximum threshold. Adds a boolean column indicating the status.</li> <li>control_primary_statistic_secret(data, iterable_operations, list_var_of_interest_max_sum_effectif): Control primary statistical secrecy for the given dataset.   This function controls primary and contribution statistical secrecy for a dataset by comparing the results with   predefined thresholds. It modifies the dataset as needed to maintain the secrecy of statistical information.</li> </ul> Source code in <code>igf_toolbox/stats_des/control_secret_stat.py</code> <pre><code>class PrimarySecretStatController(object):\n    \"\"\"\n    Control primary statistical secrets in a dataset.\n\n    This class provides methods to ensure that specific statistical criteria (secret statistics) are maintained\n    in a dataset. The criteria are based on predefined thresholds.\n\n    Attributes:\n    - var_individu (str or None): The variable representing individuals.\n    - var_entreprise (str or None): The variable representing companies.\n    - threshold_secret_stat_effectif_individu (int or None): The threshold for primary statistical secrecy control (individuals).\n    - threshold_secret_stat_effectif_entreprise (int or None): The threshold for primary statistical secrecy control (companies).\n    - threshold_secret_stat_contrib_individu (float or None): The threshold for contribution statistical secrecy control (individuals).\n    - threshold_secret_stat_contrib_entreprise (float or None): The threshold for contribution statistical secrecy control (companies).\n\n    Methods:\n    - _control_secret_stat_effectif(data, var_of_interest, threshold): Check if the number of unique values of a variable in the dataset meet\n      a minimum threshold. Adds a boolean column indicating whether the data in each row respects the secret\n      statistic criteria or not.\n    - _control_secret_stat_contrib(data, var_of_interest, threshold): Check if the values of a variable in the dataset\n      exceed a maximum threshold. Adds a boolean column indicating the status.\n    - control_primary_statistic_secret(data, iterable_operations, list_var_of_interest_max_sum_effectif): Control primary statistical secrecy for the given dataset.\n      This function controls primary and contribution statistical secrecy for a dataset by comparing the results with\n      predefined thresholds. It modifies the dataset as needed to maintain the secrecy of statistical information.\n    \"\"\"\n\n    def __init__(\n        self,\n        var_individu: Union[str, None],\n        var_entreprise: Union[str, None],\n        threshold_secret_stat_effectif_individu: Union[int, None],\n        threshold_secret_stat_effectif_entreprise: Union[int, None],\n        threshold_secret_stat_contrib_individu: Union[float, None],\n        threshold_secret_stat_contrib_entreprise: Union[float, None],\n    ) -&gt; None:\n        \"\"\"\n        Initialize the PrimarySecretStatController class.\n\n        Parameters:\n        - var_individu (str or None): Variable representing individuals.\n        - var_entreprise (str or None): Variable representing companies.\n        - threshold_secret_stat_effectif_individu (int or None): The threshold for primary statistical secrecy control (individuals).\n        - threshold_secret_stat_effectif_entreprise (int or None): The threshold for primary statistical secrecy control (companies).\n        - threshold_secret_stat_contrib_individu (float or None): The threshold for contribution statistical secrecy control (individuals).\n        - threshold_secret_stat_contrib_entreprise (float or None): The threshold for contribution statistical secrecy control (companies).\n        \"\"\"\n        # Initialisation des param\u00e8tres\n        # Variables de contr\u00f4le du secret statistique\n        self.var_individu = var_individu\n        self.var_entreprise = var_entreprise\n        # Seuils de contr\u00f4le du secret statistique\n        self.threshold_secret_stat_effectif_individu = (\n            threshold_secret_stat_effectif_individu\n        )\n        self.threshold_secret_stat_effectif_entreprise = (\n            threshold_secret_stat_effectif_entreprise\n        )\n        self.threshold_secret_stat_contrib_individu = (\n            threshold_secret_stat_contrib_individu\n        )\n        self.threshold_secret_stat_contrib_entreprise = (\n            threshold_secret_stat_contrib_entreprise\n        )\n\n    def _control_secret_stat_effectif(\n        self, data: pd.DataFrame, var_of_interest: str, threshold: int\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Control the primary secret statistic on the number of observations for a variable of interest based on a threshold.\n\n        Adds a column indicating whether the data in each row respects the secret statistic criteria or not.\n\n        Parameters:\n        - data (pd.DataFrame) : Dataset with the variable to be controlled\n        - var_of_interest (str): Column name of the variable to be controlled.\n        - threshold (int): The threshold for primary statistical secrecy control.\n\n        Returns:\n        - pd.DataFrame: Dataset with an added column indicating the status of the secret statistic.\n        \"\"\"\n\n        # Ajout d'une colonne secret stat\n        data[var_of_interest + \"_secret_stat\"] = True\n        data.loc[\n            data[var_of_interest] &lt; threshold, var_of_interest + \"_secret_stat\"\n        ] = False\n\n        return data\n\n    def _control_secret_stat_contrib(\n        self, data: pd.DataFrame, var_of_interest: str, threshold: int\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Control the secondary secret statistic for a variable of interest based on a threshold.\n\n        Adds a column indicating whether the data in each row respects the secret statistic criteria or not.\n\n        Parameters:\n        - data (pd.DataFrame) : Dataset with the variable to be controlled\n        - var_of_interest (str): Column name of the variable to be controlled.\n        - threshold (int): The threshold for contribution statistical secrecy control.\n\n        Returns:\n        - pd.DataFrame: Dataset with an added column indicating the status of the secret statistic.\n        \"\"\"\n        # Ajout d'une colonne secret stat\n        data[var_of_interest + \"_secret_stat_second\"] = True\n        data.loc[\n            data[var_of_interest] &gt; threshold, var_of_interest + \"_secret_stat_second\"\n        ] = False\n\n        return data\n\n    # Fonction liminaire de contr\u00f4le du secret statistique primaire sur les effectifs et les contributions\n    def control_primary_statistic_secret(\n        self,\n        data: pd.DataFrame,\n        iterable_operations: Union[list, dict],\n        list_var_of_interest_max_sum_effectif: List[str],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Control primary statistical secrecy for the given dataset.\n\n        This function controls primary and contribution statistical secrecy for a dataset by comparing the results with\n        predefined thresholds. It modifies the dataset as needed to maintain the secrecy of statistical information.\n\n        Parameters:\n            data (pd.DataFrame): The dataset containing descriptive statistics to control the statistical secret on.\n            iterable_operations (dict or list): The list of statistical operations to be applied.\n            list_var_of_interest_max_sum_effectif (list): List of variables related to contribution control operations.\n\n        Returns:\n            DataFrame: The modified dataset after applying statistical secrecy controls.\n\n        Note:\n            - Primary statistical secrecy control compares the number of unique values in specific variables to predefined thresholds.\n            - Contribution statistical secrecy control may include further checks based on specific variables.\n            - The function updates the dataset to ensure that statistical secrecy is maintained based on the provided thresholds.\n        \"\"\"\n        # Contr\u00f4le du secret statistique sur les effectifs\n        if (self.var_individu is not None) &amp; (self.var_entreprise is not None):\n            data = self._control_secret_stat_effectif(\n                data=data,\n                var_of_interest=self.var_individu + \"_nunique\",\n                threshold=self.threshold_secret_stat_effectif_individu,\n            )\n            data = self._control_secret_stat_effectif(\n                data=data,\n                var_of_interest=self.var_entreprise + \"_nunique\",\n                threshold=self.threshold_secret_stat_effectif_entreprise,\n            )\n        elif (self.var_individu is not None) &amp; (self.var_entreprise is None):\n            data = self._control_secret_stat_effectif(\n                data=data,\n                var_of_interest=self.var_individu + \"_nunique\",\n                threshold=self.threshold_secret_stat_effectif_individu,\n            )\n        elif (self.var_individu is None) &amp; (self.var_entreprise is not None):\n            data = self._control_secret_stat_effectif(\n                data=data,\n                var_of_interest=self.var_entreprise + \"_nunique\",\n                threshold=self.threshold_secret_stat_effectif_entreprise,\n            )\n\n        # D\u00e9finition de la n\u00e9cessit\u00e9 de contr\u00f4ler le secret statistique secondaire\n        if isinstance(iterable_operations, dict):\n            if any(x in iterable_operations.keys() for x in [\"sum\", \"mean\"]):\n                need_secret_stat_second = True\n            else:\n                need_secret_stat_second = False\n        elif isinstance(iterable_operations, list):\n            if any(x in iterable_operations for x in [\"sum\", \"mean\"]):\n                need_secret_stat_second = True\n            else:\n                need_secret_stat_second = False\n\n        # Contr\u00f4le du secret statistique secondaire\n        if (\n            need_secret_stat_second\n            &amp; (self.var_individu is not None)\n            &amp; (self.var_entreprise is not None)\n        ):\n            for var_of_interest in list_var_of_interest_max_sum_effectif:\n                data = self._control_secret_stat_contrib(\n                    data=data,\n                    var_of_interest=var_of_interest + f\"_{self.var_individu}_max/sum\",\n                    threshold=self.threshold_secret_stat_contrib_individu,\n                )\n                data = self._control_secret_stat_contrib(\n                    data=data,\n                    var_of_interest=var_of_interest + f\"_{self.var_entreprise}_max/sum\",\n                    threshold=self.threshold_secret_stat_contrib_entreprise,\n                )\n        elif (\n            need_secret_stat_second\n            &amp; (self.var_individu is not None)\n            &amp; (self.var_entreprise is None)\n        ):\n            for var_of_interest in list_var_of_interest_max_sum_effectif:\n                data = self._control_secret_stat_contrib(\n                    data=data,\n                    var_of_interest=var_of_interest + f\"_{self.var_individu}_max/sum\",\n                    threshold=self.threshold_secret_stat_contrib_individu,\n                )\n        elif (\n            need_secret_stat_second\n            &amp; (self.var_individu is None)\n            &amp; (self.var_entreprise is not None)\n        ):\n            for var_of_interest in list_var_of_interest_max_sum_effectif:\n                data = self._control_secret_stat_contrib(\n                    data=data,\n                    var_of_interest=var_of_interest + f\"_{self.var_entreprise}_max/sum\",\n                    threshold=self.threshold_secret_stat_contrib_entreprise,\n                )\n\n        return data\n</code></pre>"},{"location":"api/PrimarySecretStatController/#igf_toolbox.stats_des.control_secret_stat.PrimarySecretStatController.__init__","title":"<code>__init__(var_individu, var_entreprise, threshold_secret_stat_effectif_individu, threshold_secret_stat_effectif_entreprise, threshold_secret_stat_contrib_individu, threshold_secret_stat_contrib_entreprise)</code>","text":"<p>Initialize the PrimarySecretStatController class.</p> <p>Parameters: - var_individu (str or None): Variable representing individuals. - var_entreprise (str or None): Variable representing companies. - threshold_secret_stat_effectif_individu (int or None): The threshold for primary statistical secrecy control (individuals). - threshold_secret_stat_effectif_entreprise (int or None): The threshold for primary statistical secrecy control (companies). - threshold_secret_stat_contrib_individu (float or None): The threshold for contribution statistical secrecy control (individuals). - threshold_secret_stat_contrib_entreprise (float or None): The threshold for contribution statistical secrecy control (companies).</p> Source code in <code>igf_toolbox/stats_des/control_secret_stat.py</code> <pre><code>def __init__(\n    self,\n    var_individu: Union[str, None],\n    var_entreprise: Union[str, None],\n    threshold_secret_stat_effectif_individu: Union[int, None],\n    threshold_secret_stat_effectif_entreprise: Union[int, None],\n    threshold_secret_stat_contrib_individu: Union[float, None],\n    threshold_secret_stat_contrib_entreprise: Union[float, None],\n) -&gt; None:\n    \"\"\"\n    Initialize the PrimarySecretStatController class.\n\n    Parameters:\n    - var_individu (str or None): Variable representing individuals.\n    - var_entreprise (str or None): Variable representing companies.\n    - threshold_secret_stat_effectif_individu (int or None): The threshold for primary statistical secrecy control (individuals).\n    - threshold_secret_stat_effectif_entreprise (int or None): The threshold for primary statistical secrecy control (companies).\n    - threshold_secret_stat_contrib_individu (float or None): The threshold for contribution statistical secrecy control (individuals).\n    - threshold_secret_stat_contrib_entreprise (float or None): The threshold for contribution statistical secrecy control (companies).\n    \"\"\"\n    # Initialisation des param\u00e8tres\n    # Variables de contr\u00f4le du secret statistique\n    self.var_individu = var_individu\n    self.var_entreprise = var_entreprise\n    # Seuils de contr\u00f4le du secret statistique\n    self.threshold_secret_stat_effectif_individu = (\n        threshold_secret_stat_effectif_individu\n    )\n    self.threshold_secret_stat_effectif_entreprise = (\n        threshold_secret_stat_effectif_entreprise\n    )\n    self.threshold_secret_stat_contrib_individu = (\n        threshold_secret_stat_contrib_individu\n    )\n    self.threshold_secret_stat_contrib_entreprise = (\n        threshold_secret_stat_contrib_entreprise\n    )\n</code></pre>"},{"location":"api/PrimarySecretStatController/#igf_toolbox.stats_des.control_secret_stat.PrimarySecretStatController.control_primary_statistic_secret","title":"<code>control_primary_statistic_secret(data, iterable_operations, list_var_of_interest_max_sum_effectif)</code>","text":"<p>Control primary statistical secrecy for the given dataset.</p> <p>This function controls primary and contribution statistical secrecy for a dataset by comparing the results with predefined thresholds. It modifies the dataset as needed to maintain the secrecy of statistical information.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset containing descriptive statistics to control the statistical secret on.</p> required <code>iterable_operations</code> <code>dict or list</code> <p>The list of statistical operations to be applied.</p> required <code>list_var_of_interest_max_sum_effectif</code> <code>list</code> <p>List of variables related to contribution control operations.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The modified dataset after applying statistical secrecy controls.</p> Note <ul> <li>Primary statistical secrecy control compares the number of unique values in specific variables to predefined thresholds.</li> <li>Contribution statistical secrecy control may include further checks based on specific variables.</li> <li>The function updates the dataset to ensure that statistical secrecy is maintained based on the provided thresholds.</li> </ul> Source code in <code>igf_toolbox/stats_des/control_secret_stat.py</code> <pre><code>def control_primary_statistic_secret(\n    self,\n    data: pd.DataFrame,\n    iterable_operations: Union[list, dict],\n    list_var_of_interest_max_sum_effectif: List[str],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Control primary statistical secrecy for the given dataset.\n\n    This function controls primary and contribution statistical secrecy for a dataset by comparing the results with\n    predefined thresholds. It modifies the dataset as needed to maintain the secrecy of statistical information.\n\n    Parameters:\n        data (pd.DataFrame): The dataset containing descriptive statistics to control the statistical secret on.\n        iterable_operations (dict or list): The list of statistical operations to be applied.\n        list_var_of_interest_max_sum_effectif (list): List of variables related to contribution control operations.\n\n    Returns:\n        DataFrame: The modified dataset after applying statistical secrecy controls.\n\n    Note:\n        - Primary statistical secrecy control compares the number of unique values in specific variables to predefined thresholds.\n        - Contribution statistical secrecy control may include further checks based on specific variables.\n        - The function updates the dataset to ensure that statistical secrecy is maintained based on the provided thresholds.\n    \"\"\"\n    # Contr\u00f4le du secret statistique sur les effectifs\n    if (self.var_individu is not None) &amp; (self.var_entreprise is not None):\n        data = self._control_secret_stat_effectif(\n            data=data,\n            var_of_interest=self.var_individu + \"_nunique\",\n            threshold=self.threshold_secret_stat_effectif_individu,\n        )\n        data = self._control_secret_stat_effectif(\n            data=data,\n            var_of_interest=self.var_entreprise + \"_nunique\",\n            threshold=self.threshold_secret_stat_effectif_entreprise,\n        )\n    elif (self.var_individu is not None) &amp; (self.var_entreprise is None):\n        data = self._control_secret_stat_effectif(\n            data=data,\n            var_of_interest=self.var_individu + \"_nunique\",\n            threshold=self.threshold_secret_stat_effectif_individu,\n        )\n    elif (self.var_individu is None) &amp; (self.var_entreprise is not None):\n        data = self._control_secret_stat_effectif(\n            data=data,\n            var_of_interest=self.var_entreprise + \"_nunique\",\n            threshold=self.threshold_secret_stat_effectif_entreprise,\n        )\n\n    # D\u00e9finition de la n\u00e9cessit\u00e9 de contr\u00f4ler le secret statistique secondaire\n    if isinstance(iterable_operations, dict):\n        if any(x in iterable_operations.keys() for x in [\"sum\", \"mean\"]):\n            need_secret_stat_second = True\n        else:\n            need_secret_stat_second = False\n    elif isinstance(iterable_operations, list):\n        if any(x in iterable_operations for x in [\"sum\", \"mean\"]):\n            need_secret_stat_second = True\n        else:\n            need_secret_stat_second = False\n\n    # Contr\u00f4le du secret statistique secondaire\n    if (\n        need_secret_stat_second\n        &amp; (self.var_individu is not None)\n        &amp; (self.var_entreprise is not None)\n    ):\n        for var_of_interest in list_var_of_interest_max_sum_effectif:\n            data = self._control_secret_stat_contrib(\n                data=data,\n                var_of_interest=var_of_interest + f\"_{self.var_individu}_max/sum\",\n                threshold=self.threshold_secret_stat_contrib_individu,\n            )\n            data = self._control_secret_stat_contrib(\n                data=data,\n                var_of_interest=var_of_interest + f\"_{self.var_entreprise}_max/sum\",\n                threshold=self.threshold_secret_stat_contrib_entreprise,\n            )\n    elif (\n        need_secret_stat_second\n        &amp; (self.var_individu is not None)\n        &amp; (self.var_entreprise is None)\n    ):\n        for var_of_interest in list_var_of_interest_max_sum_effectif:\n            data = self._control_secret_stat_contrib(\n                data=data,\n                var_of_interest=var_of_interest + f\"_{self.var_individu}_max/sum\",\n                threshold=self.threshold_secret_stat_contrib_individu,\n            )\n    elif (\n        need_secret_stat_second\n        &amp; (self.var_individu is None)\n        &amp; (self.var_entreprise is not None)\n    ):\n        for var_of_interest in list_var_of_interest_max_sum_effectif:\n            data = self._control_secret_stat_contrib(\n                data=data,\n                var_of_interest=var_of_interest + f\"_{self.var_entreprise}_max/sum\",\n                threshold=self.threshold_secret_stat_contrib_entreprise,\n            )\n\n    return data\n</code></pre>"},{"location":"api/ProbitClassifier/","title":"ProbitClassifier","text":"<p>               Bases: <code>BaseEstimator</code></p> <p>Probit model wrapped to a sklearn class Parameters</p> <p>proba_threshold : float, default=0.5     Threshold to distinguish forecast corresponding to the zero class and to the one class</p> string, default='roc' <p>Score to calculate via the score method</p> <p>regularized : bool, default=True     Whether to add a regularisation constrain to select variables in the minimization process and ensure convergence</p> string, default='l1' <p>Type of regularization to apply</p> float, default=0.01 <p>Regularization parameter</p>"},{"location":"api/ProbitClassifier/#igf_toolbox.estimators.classifiers.ProbitClassifier--attributes","title":"Attributes","text":"<p>probit : Probit     Probit fitted regressor</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>class ProbitClassifier(BaseEstimator):\n    \"\"\"\n    Probit model wrapped to a sklearn class\n    Parameters\n    ----------\n    proba_threshold : float, default=0.5\n        Threshold to distinguish forecast corresponding to the zero class and to the one class\n\n    scoring : string, default='roc'\n        Score to calculate via the score method\n    regularized : bool, default=True\n        Whether to add a regularisation constrain to select variables in the minimization process and ensure convergence\n\n    method : string, default='l1'\n        Type of regularization to apply\n\n    alpha : float, default=0.01\n        Regularization parameter\n\n    Attributes\n    ----------\n    probit : Probit\n        Probit fitted regressor\n    \"\"\"\n\n    def __init__(\n        self,\n        proba_threshold: Optional[float] = 0.5,\n        scoring: Optional[str] = \"roc\",\n        regularized: Optional[bool] = False,\n        method: Optional[str] = \"l1\",\n        alpha: Optional[float] = 0.01,\n    ) -&gt; None:\n        # Initialisation des param\u00e8tres du mod\u00e8le\n        self.scoring = scoring\n        self.regularized = regularized\n        self.method = method\n        self.alpha = alpha\n        self.trim_mode = \"size\"\n        self.estimator_type = \"classifier\"\n        self._estimator_type = \"classifier\"\n        self.proba_threshold = proba_threshold\n\n    def set_params(self, **parameters) -&gt; None:\n        \"\"\"\n        Change the parameters of the model\n        Returns\n        -------\n        self : returns an instance of self\n        \"\"\"\n        # Ajout des param\u00e8tres\n        for parameter, value in parameters.items():\n            setattr(self, parameter, value)\n        return self\n\n    def get_params(self, deep=True) -&gt; dict:\n        \"\"\"\n        Return a dictionnary containing the current parameters of the model\n        Parameters\n        ----------\n\n        deep : bool, default=True\n            Whether to return an independent copy of the parameters\n        Returns\n        -------\n\n        DictParams : dictionnary\n            Dictionnary containing the name of the parameter and its corresponding value\n        \"\"\"\n        # Retourne les param\u00e8tres\n        return {\n            \"alpha\": self.alpha,\n            \"scoring\": self.scoring,\n            \"regularized\": self.regularized,\n            \"method\": self.method,\n            \"trim_mode\": self.trim_mode,\n            \"proba_threshold\": self.proba_threshold,\n        }\n\n    def fit(self, X, y) -&gt; None:\n        \"\"\"\n        Fit the probit model.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            Training data\n\n        y : array-like of shape (n_samples,) or (n_samples, n_targets)\n            Target values. Will be cast to X's dtype if necessary\n        Returns\n        -------\n        self : returns an instance of self\n        \"\"\"\n        # Entrainement du mod\u00e8le\n        if self.regularized:\n            self.probit = Probit(y, X).fit_regularized(\n                method=self.method,\n                alpha=self.alpha,\n                trim_mode=self.trim_mode,\n                maxiter=100000,\n            )\n        else:\n            self.probit = Probit(y, X).fit()\n        # Extraction des coefficients et des classes\n        self._coef = self.probit.params\n        self.classes_ = np.unique(y)\n\n        return self\n\n    def predict(self, X) -&gt; np.ndarray:\n        \"\"\"Predict class for X.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n        Returns\n        -------\n        Prediction : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            The predicted classes.\n        \"\"\"\n        # Pr\u00e9diction\n        if self.proba_threshold:\n            return (self.probit.predict(X) &gt;= self.proba_threshold) * 1\n        return self.probit.predict(X)\n\n    def predict_proba(self, X) -&gt; np.ndarray:\n        \"\"\"Predict class associated with the class labelized as one for X.\n        Parameters\n        ----------\n        X : array-like of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n        Returns\n        -------\n        Prediction : array-like of shape (n_samples,) or (n_samples, n_outputs)\n            The predicted probabilities.\n        \"\"\"\n        # Pr\u00e9diction de la probabilit\u00e9 d'appartenance \u00e0 chaque classe\n        proba = []\n        XArray = np.array(X)\n        for i in range(len(XArray)):\n            proba.append(\n                [\n                    1 - self.probit.predict(XArray[i, :])[0],\n                    self.probit.predict(XArray[i, :])[0],\n                ]\n            )\n\n        return np.array(proba).reshape(-1, 2)\n\n    def score(self, X_test, y_test):\n        \"\"\"Calculate the score for the label predicted by the model for X_test et les vrais labels y_test.\n        Parameters\n        ----------\n        X_test : array-like of shape (n_samples, n_features)\n            The input samples. Internally, it will be converted to\n            ``dtype=np.float32`` and if a sparse matrix is provided\n            to a sparse ``csr_matrix``.\n        y_test : array-like of shape (n_samples, 1).\n            The test samples\n        Returns\n        -------\n        score : float or array-like\n            The computed score.\n        \"\"\"\n        # Calcul du ROC-AUC ou de l'Accuracy\n        self.y_pred = ((self.predict(X_test) &gt; 0.25) * 1) * True\n        self.y_true = y_test\n        if self.scoring == \"accuracy\":\n            score = np.count_nonzero(\n                np.add(\n                    self.y_pred.tolist(),\n                    np.multiply(self.y_true.iloc[:, 0].tolist(), -1),\n                )\n                == 0\n            ) / len(self.y_pred)\n        if self.scoring == \"roc\":\n            from sklearn.metrics import roc_auc_score\n\n            score = roc_auc_score(self.y_true, self.predict(X_test))\n        return score\n\n    def summary(self):\n        \"\"\"\n        Give information about the estimation of the probit model\n        Returns\n        -------\n        Summary : Information about the coefficients estimated by the model\n        \"\"\"\n        # R\u00e9sum\u00e9 des r\u00e9sultats de l'estimation\n        return self.probit.summary()\n\n    def cov_matrix(self):\n        \"\"\"\n        Give the covariance matrix of the model\n        Returns\n        -------\n        Cov : Covariance matrix of the model\n        \"\"\"\n        # Matrice de variance-covariance\n        return self.probit.cov_params()\n</code></pre>"},{"location":"api/ProbitClassifier/#igf_toolbox.estimators.classifiers.ProbitClassifier.cov_matrix","title":"<code>cov_matrix()</code>","text":"<p>Give the covariance matrix of the model Returns</p> <p>Cov : Covariance matrix of the model</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def cov_matrix(self):\n    \"\"\"\n    Give the covariance matrix of the model\n    Returns\n    -------\n    Cov : Covariance matrix of the model\n    \"\"\"\n    # Matrice de variance-covariance\n    return self.probit.cov_params()\n</code></pre>"},{"location":"api/ProbitClassifier/#igf_toolbox.estimators.classifiers.ProbitClassifier.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit the probit model. Parameters</p> <p>X : array-like of shape (n_samples, n_features)     Training data</p> array-like of shape (n_samples,) or (n_samples, n_targets) <p>Target values. Will be cast to X's dtype if necessary</p>"},{"location":"api/ProbitClassifier/#igf_toolbox.estimators.classifiers.ProbitClassifier.fit--returns","title":"Returns","text":"<p>self : returns an instance of self</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def fit(self, X, y) -&gt; None:\n    \"\"\"\n    Fit the probit model.\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        Training data\n\n    y : array-like of shape (n_samples,) or (n_samples, n_targets)\n        Target values. Will be cast to X's dtype if necessary\n    Returns\n    -------\n    self : returns an instance of self\n    \"\"\"\n    # Entrainement du mod\u00e8le\n    if self.regularized:\n        self.probit = Probit(y, X).fit_regularized(\n            method=self.method,\n            alpha=self.alpha,\n            trim_mode=self.trim_mode,\n            maxiter=100000,\n        )\n    else:\n        self.probit = Probit(y, X).fit()\n    # Extraction des coefficients et des classes\n    self._coef = self.probit.params\n    self.classes_ = np.unique(y)\n\n    return self\n</code></pre>"},{"location":"api/ProbitClassifier/#igf_toolbox.estimators.classifiers.ProbitClassifier.get_params","title":"<code>get_params(deep=True)</code>","text":"<p>Return a dictionnary containing the current parameters of the model Parameters</p> bool, default=True <p>Whether to return an independent copy of the parameters</p>"},{"location":"api/ProbitClassifier/#igf_toolbox.estimators.classifiers.ProbitClassifier.get_params--returns","title":"Returns","text":"dictionnary <p>Dictionnary containing the name of the parameter and its corresponding value</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def get_params(self, deep=True) -&gt; dict:\n    \"\"\"\n    Return a dictionnary containing the current parameters of the model\n    Parameters\n    ----------\n\n    deep : bool, default=True\n        Whether to return an independent copy of the parameters\n    Returns\n    -------\n\n    DictParams : dictionnary\n        Dictionnary containing the name of the parameter and its corresponding value\n    \"\"\"\n    # Retourne les param\u00e8tres\n    return {\n        \"alpha\": self.alpha,\n        \"scoring\": self.scoring,\n        \"regularized\": self.regularized,\n        \"method\": self.method,\n        \"trim_mode\": self.trim_mode,\n        \"proba_threshold\": self.proba_threshold,\n    }\n</code></pre>"},{"location":"api/ProbitClassifier/#igf_toolbox.estimators.classifiers.ProbitClassifier.predict","title":"<code>predict(X)</code>","text":"<p>Predict class for X. Parameters</p> <p>X : array-like of shape (n_samples, n_features)     The input samples. Internally, it will be converted to     <code>dtype=np.float32</code> and if a sparse matrix is provided     to a sparse <code>csr_matrix</code>. Returns</p> <p>Prediction : array-like of shape (n_samples,) or (n_samples, n_outputs)     The predicted classes.</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def predict(self, X) -&gt; np.ndarray:\n    \"\"\"Predict class for X.\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples. Internally, it will be converted to\n        ``dtype=np.float32`` and if a sparse matrix is provided\n        to a sparse ``csr_matrix``.\n    Returns\n    -------\n    Prediction : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        The predicted classes.\n    \"\"\"\n    # Pr\u00e9diction\n    if self.proba_threshold:\n        return (self.probit.predict(X) &gt;= self.proba_threshold) * 1\n    return self.probit.predict(X)\n</code></pre>"},{"location":"api/ProbitClassifier/#igf_toolbox.estimators.classifiers.ProbitClassifier.predict_proba","title":"<code>predict_proba(X)</code>","text":"<p>Predict class associated with the class labelized as one for X. Parameters</p> <p>X : array-like of shape (n_samples, n_features)     The input samples. Internally, it will be converted to     <code>dtype=np.float32</code> and if a sparse matrix is provided     to a sparse <code>csr_matrix</code>. Returns</p> <p>Prediction : array-like of shape (n_samples,) or (n_samples, n_outputs)     The predicted probabilities.</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def predict_proba(self, X) -&gt; np.ndarray:\n    \"\"\"Predict class associated with the class labelized as one for X.\n    Parameters\n    ----------\n    X : array-like of shape (n_samples, n_features)\n        The input samples. Internally, it will be converted to\n        ``dtype=np.float32`` and if a sparse matrix is provided\n        to a sparse ``csr_matrix``.\n    Returns\n    -------\n    Prediction : array-like of shape (n_samples,) or (n_samples, n_outputs)\n        The predicted probabilities.\n    \"\"\"\n    # Pr\u00e9diction de la probabilit\u00e9 d'appartenance \u00e0 chaque classe\n    proba = []\n    XArray = np.array(X)\n    for i in range(len(XArray)):\n        proba.append(\n            [\n                1 - self.probit.predict(XArray[i, :])[0],\n                self.probit.predict(XArray[i, :])[0],\n            ]\n        )\n\n    return np.array(proba).reshape(-1, 2)\n</code></pre>"},{"location":"api/ProbitClassifier/#igf_toolbox.estimators.classifiers.ProbitClassifier.score","title":"<code>score(X_test, y_test)</code>","text":"<p>Calculate the score for the label predicted by the model for X_test et les vrais labels y_test. Parameters</p> <p>X_test : array-like of shape (n_samples, n_features)     The input samples. Internally, it will be converted to     <code>dtype=np.float32</code> and if a sparse matrix is provided     to a sparse <code>csr_matrix</code>. y_test : array-like of shape (n_samples, 1).     The test samples Returns</p> <p>score : float or array-like     The computed score.</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def score(self, X_test, y_test):\n    \"\"\"Calculate the score for the label predicted by the model for X_test et les vrais labels y_test.\n    Parameters\n    ----------\n    X_test : array-like of shape (n_samples, n_features)\n        The input samples. Internally, it will be converted to\n        ``dtype=np.float32`` and if a sparse matrix is provided\n        to a sparse ``csr_matrix``.\n    y_test : array-like of shape (n_samples, 1).\n        The test samples\n    Returns\n    -------\n    score : float or array-like\n        The computed score.\n    \"\"\"\n    # Calcul du ROC-AUC ou de l'Accuracy\n    self.y_pred = ((self.predict(X_test) &gt; 0.25) * 1) * True\n    self.y_true = y_test\n    if self.scoring == \"accuracy\":\n        score = np.count_nonzero(\n            np.add(\n                self.y_pred.tolist(),\n                np.multiply(self.y_true.iloc[:, 0].tolist(), -1),\n            )\n            == 0\n        ) / len(self.y_pred)\n    if self.scoring == \"roc\":\n        from sklearn.metrics import roc_auc_score\n\n        score = roc_auc_score(self.y_true, self.predict(X_test))\n    return score\n</code></pre>"},{"location":"api/ProbitClassifier/#igf_toolbox.estimators.classifiers.ProbitClassifier.set_params","title":"<code>set_params(**parameters)</code>","text":"<p>Change the parameters of the model Returns</p> <p>self : returns an instance of self</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def set_params(self, **parameters) -&gt; None:\n    \"\"\"\n    Change the parameters of the model\n    Returns\n    -------\n    self : returns an instance of self\n    \"\"\"\n    # Ajout des param\u00e8tres\n    for parameter, value in parameters.items():\n        setattr(self, parameter, value)\n    return self\n</code></pre>"},{"location":"api/ProbitClassifier/#igf_toolbox.estimators.classifiers.ProbitClassifier.summary","title":"<code>summary()</code>","text":"<p>Give information about the estimation of the probit model Returns</p> <p>Summary : Information about the coefficients estimated by the model</p> Source code in <code>igf_toolbox/estimators/classifiers.py</code> <pre><code>def summary(self):\n    \"\"\"\n    Give information about the estimation of the probit model\n    Returns\n    -------\n    Summary : Information about the coefficients estimated by the model\n    \"\"\"\n    # R\u00e9sum\u00e9 des r\u00e9sultats de l'estimation\n    return self.probit.summary()\n</code></pre>"},{"location":"api/QuantileExcluder/","title":"QuantileExcluder","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>Exclude observations from a DataFrame based on quantile criteria.</p> <p>This transformer filters rows of a DataFrame based on specified column quantiles. Multiple criteria can be applied simultaneously, and rows which don't satisfy all the conditions will be excluded from the result.</p> <ul> <li>list_dict_params (List[Dict]): A list of dictionaries specifying the exclusion criteria.     Each dictionary should contain:<ul> <li>'variable': The column on which to apply the criterion.</li> <li>'operator': The comparison operator, which can be one of the following: 'left', 'right', 'both'.</li> <li>'threshold': The quantile value to compare against.</li> </ul> </li> <li>drop (bool) : A boolean indicating whether to drop or fill with np.nan excluded observations</li> </ul> <p>Methods: - fit(X, y=None): Returns self. - transform(X, y=None): Exclude observations based on the criteria.</p> <p>Example:</p> <p>excluder = QuantileExcluder([list_dict_params={'variable': 'A', 'operator': 'left', 'threshold': 0.25},                              {'variable': 'B', 'operator': 'right', 'threshold': 0.1}], drop=True) df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [5, 6, 7, 8]}) df_transformed = excluder.transform(df)</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>class QuantileExcluder(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Exclude observations from a DataFrame based on quantile criteria.\n\n    This transformer filters rows of a DataFrame based on specified column quantiles.\n    Multiple criteria can be applied simultaneously, and rows which don't satisfy all the conditions\n    will be excluded from the result.\n\n    Attributes:\n    - list_dict_params (List[Dict]): A list of dictionaries specifying the exclusion criteria.\n        Each dictionary should contain:\n        - 'variable': The column on which to apply the criterion.\n        - 'operator': The comparison operator, which can be one of the following: 'left', 'right', 'both'.\n        - 'threshold': The quantile value to compare against.\n    - drop (bool) : A boolean indicating whether to drop or fill with np.nan excluded observations\n\n    Methods:\n    - fit(X, y=None): Returns self.\n    - transform(X, y=None): Exclude observations based on the criteria.\n\n    Example:\n    &gt;&gt;&gt; excluder = QuantileExcluder([list_dict_params={'variable': 'A', 'operator': 'left', 'threshold': 0.25},\n    &gt;&gt;&gt;                              {'variable': 'B', 'operator': 'right', 'threshold': 0.1}], drop=True)\n    &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [5, 6, 7, 8]})\n    &gt;&gt;&gt; df_transformed = excluder.transform(df)\n    \"\"\"\n\n    def __init__(self, list_dict_params: List[Dict], drop: bool = True) -&gt; None:\n        \"\"\"\n        Initialize the QuantileExcluder.\n\n        Parameters:\n        - list_dict_params (List[Dict]): A list of dictionaries specifying the exclusion criteria.\n        \"\"\"\n        # Initialisation de la liste du dictionnaire de param\u00e8tres\n        self.list_dict_params = list_dict_params\n        # Initialisation du bool\u00e9en indiquant s'il faut supprimer les colonnes ou remplacer par un Nan les observations exclues\n        self.drop = drop\n\n    def fit(self, X, y=None) -&gt; None:\n        \"\"\"\n        Return self.\n\n        The fit method is implemented for compatibility with sklearn's TransformerMixin,\n        but doesn't perform any actual computation.\n\n        Parameters:\n        - X (pd.DataFrame): The input data. Not used, only needed for compatibility.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - self: The instance itself.\n        \"\"\"\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Exclude observations based on the specified quantile criteria.\n\n        Parameters:\n        - X (pd.DataFrame): The input data to transform.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - pd.DataFrame: The transformed data with observations not meeting the criteria excluded.\n        \"\"\"\n        # Disjonction suivant la suppression\n        if self.drop:\n            # Initialisation de la s\u00e9rie de bool\u00e9ens\n            list_data_boolean = []\n\n            # Parcours des dictionnaires de param\u00e8tres\n            for dict_params in self.list_dict_params:\n                if dict_params[\"operator\"] == \"left\":\n                    q = X[dict_params[\"variable\"]].quantile(q=dict_params[\"threshold\"])\n                    list_data_boolean.append(\n                        (X[dict_params[\"variable\"]] &gt; q).to_frame()\n                    )\n                elif dict_params[\"operator\"] == \"right\":\n                    q = X[dict_params[\"variable\"]].quantile(\n                        q=1 - dict_params[\"threshold\"]\n                    )\n                    list_data_boolean.append(\n                        (X[dict_params[\"variable\"]] &lt; q).to_frame()\n                    )\n                elif dict_params[\"operator\"] == \"both\":\n                    q = X[dict_params[\"variable\"]].quantile(\n                        q=[dict_params[\"threshold\"], 1 - dict_params[\"threshold\"]]\n                    )\n                    list_data_boolean.append(\n                        (\n                            (\n                                X[dict_params[\"variable\"]]\n                                &gt; q.loc[dict_params[\"threshold\"]]\n                            )\n                            &amp; (\n                                X[dict_params[\"variable\"]]\n                                &lt; q.loc[1 - dict_params[\"threshold\"]]\n                            )\n                        ).to_frame()\n                    )\n\n            # Concat\u00e9nation des s\u00e9ries bool\u00e9ennes\n            data_boolean = pd.concat(list_data_boolean, axis=1, join=\"outer\")\n\n            # Restriction aux observations respectant tous les seuils\n            X_transformed = X.loc[data_boolean.all(axis=1)].copy()\n        else:\n            # Copie ind\u00e9pendante du jeu de donn\u00e9es\n            X_transformed = X.copy()\n            # Parcours des dictionnaires de param\u00e8tres\n            for dict_params in self.list_dict_params:\n                if dict_params[\"operator\"] == \"left\":\n                    q = X[dict_params[\"variable\"]].quantile(q=dict_params[\"threshold\"])\n                    X_transformed.loc[\n                        (X_transformed[dict_params[\"variable\"]] &lt; q),\n                        dict_params[\"variable\"],\n                    ] = np.nan\n                elif dict_params[\"operator\"] == \"right\":\n                    q = X[dict_params[\"variable\"]].quantile(\n                        q=1 - dict_params[\"threshold\"]\n                    )\n                    X_transformed.loc[\n                        (X_transformed[dict_params[\"variable\"]] &gt; q),\n                        dict_params[\"variable\"],\n                    ] = np.nan\n                elif dict_params[\"operator\"] == \"both\":\n                    q = X[dict_params[\"variable\"]].quantile(\n                        q=[dict_params[\"threshold\"], 1 - dict_params[\"threshold\"]]\n                    )\n                    X_transformed.loc[\n                        (\n                            (\n                                X_transformed[dict_params[\"variable\"]]\n                                &lt; q.loc[dict_params[\"threshold\"]]\n                            )\n                            | (\n                                X_transformed[dict_params[\"variable\"]]\n                                &gt; q.loc[1 - dict_params[\"threshold\"]]\n                            )\n                        ),\n                        dict_params[\"variable\"],\n                    ] = np.nan\n\n        return X_transformed\n</code></pre>"},{"location":"api/QuantileExcluder/#igf_toolbox.preprocessing.excluders.QuantileExcluder.__init__","title":"<code>__init__(list_dict_params, drop=True)</code>","text":"<p>Initialize the QuantileExcluder.</p> <p>Parameters: - list_dict_params (List[Dict]): A list of dictionaries specifying the exclusion criteria.</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>def __init__(self, list_dict_params: List[Dict], drop: bool = True) -&gt; None:\n    \"\"\"\n    Initialize the QuantileExcluder.\n\n    Parameters:\n    - list_dict_params (List[Dict]): A list of dictionaries specifying the exclusion criteria.\n    \"\"\"\n    # Initialisation de la liste du dictionnaire de param\u00e8tres\n    self.list_dict_params = list_dict_params\n    # Initialisation du bool\u00e9en indiquant s'il faut supprimer les colonnes ou remplacer par un Nan les observations exclues\n    self.drop = drop\n</code></pre>"},{"location":"api/QuantileExcluder/#igf_toolbox.preprocessing.excluders.QuantileExcluder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Return self.</p> <p>The fit method is implemented for compatibility with sklearn's TransformerMixin, but doesn't perform any actual computation.</p> <p>Parameters: - X (pd.DataFrame): The input data. Not used, only needed for compatibility. - y (ignored): This parameter is ignored.</p> <p>Returns: - self: The instance itself.</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>def fit(self, X, y=None) -&gt; None:\n    \"\"\"\n    Return self.\n\n    The fit method is implemented for compatibility with sklearn's TransformerMixin,\n    but doesn't perform any actual computation.\n\n    Parameters:\n    - X (pd.DataFrame): The input data. Not used, only needed for compatibility.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - self: The instance itself.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/QuantileExcluder/#igf_toolbox.preprocessing.excluders.QuantileExcluder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Exclude observations based on the specified quantile criteria.</p> <p>Parameters: - X (pd.DataFrame): The input data to transform. - y (ignored): This parameter is ignored.</p> <p>Returns: - pd.DataFrame: The transformed data with observations not meeting the criteria excluded.</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Exclude observations based on the specified quantile criteria.\n\n    Parameters:\n    - X (pd.DataFrame): The input data to transform.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - pd.DataFrame: The transformed data with observations not meeting the criteria excluded.\n    \"\"\"\n    # Disjonction suivant la suppression\n    if self.drop:\n        # Initialisation de la s\u00e9rie de bool\u00e9ens\n        list_data_boolean = []\n\n        # Parcours des dictionnaires de param\u00e8tres\n        for dict_params in self.list_dict_params:\n            if dict_params[\"operator\"] == \"left\":\n                q = X[dict_params[\"variable\"]].quantile(q=dict_params[\"threshold\"])\n                list_data_boolean.append(\n                    (X[dict_params[\"variable\"]] &gt; q).to_frame()\n                )\n            elif dict_params[\"operator\"] == \"right\":\n                q = X[dict_params[\"variable\"]].quantile(\n                    q=1 - dict_params[\"threshold\"]\n                )\n                list_data_boolean.append(\n                    (X[dict_params[\"variable\"]] &lt; q).to_frame()\n                )\n            elif dict_params[\"operator\"] == \"both\":\n                q = X[dict_params[\"variable\"]].quantile(\n                    q=[dict_params[\"threshold\"], 1 - dict_params[\"threshold\"]]\n                )\n                list_data_boolean.append(\n                    (\n                        (\n                            X[dict_params[\"variable\"]]\n                            &gt; q.loc[dict_params[\"threshold\"]]\n                        )\n                        &amp; (\n                            X[dict_params[\"variable\"]]\n                            &lt; q.loc[1 - dict_params[\"threshold\"]]\n                        )\n                    ).to_frame()\n                )\n\n        # Concat\u00e9nation des s\u00e9ries bool\u00e9ennes\n        data_boolean = pd.concat(list_data_boolean, axis=1, join=\"outer\")\n\n        # Restriction aux observations respectant tous les seuils\n        X_transformed = X.loc[data_boolean.all(axis=1)].copy()\n    else:\n        # Copie ind\u00e9pendante du jeu de donn\u00e9es\n        X_transformed = X.copy()\n        # Parcours des dictionnaires de param\u00e8tres\n        for dict_params in self.list_dict_params:\n            if dict_params[\"operator\"] == \"left\":\n                q = X[dict_params[\"variable\"]].quantile(q=dict_params[\"threshold\"])\n                X_transformed.loc[\n                    (X_transformed[dict_params[\"variable\"]] &lt; q),\n                    dict_params[\"variable\"],\n                ] = np.nan\n            elif dict_params[\"operator\"] == \"right\":\n                q = X[dict_params[\"variable\"]].quantile(\n                    q=1 - dict_params[\"threshold\"]\n                )\n                X_transformed.loc[\n                    (X_transformed[dict_params[\"variable\"]] &gt; q),\n                    dict_params[\"variable\"],\n                ] = np.nan\n            elif dict_params[\"operator\"] == \"both\":\n                q = X[dict_params[\"variable\"]].quantile(\n                    q=[dict_params[\"threshold\"], 1 - dict_params[\"threshold\"]]\n                )\n                X_transformed.loc[\n                    (\n                        (\n                            X_transformed[dict_params[\"variable\"]]\n                            &lt; q.loc[dict_params[\"threshold\"]]\n                        )\n                        | (\n                            X_transformed[dict_params[\"variable\"]]\n                            &gt; q.loc[1 - dict_params[\"threshold\"]]\n                        )\n                    ),\n                    dict_params[\"variable\"],\n                ] = np.nan\n\n    return X_transformed\n</code></pre>"},{"location":"api/S3Loader/","title":"S3Loader","text":"<p>               Bases: <code>_S3Connection</code></p> <p>A class for loading data from an Amazon S3 bucket using 'boto3' or 's3fs' as the underlying package.</p> <p>This class extends the <code>_S3Connection</code> parent class and provides methods for establishing a connection to the S3 bucket and loading data from a specified S3 object.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>The package to use for connecting to S3 ('s3fs' or 'boto3').</p> <code>'boto3'</code> <p>Methods:</p> Name Description <code>connect</code> <p>Establishes a connection to the S3 bucket.</p> <code>load</code> <p>Loads data from a specified S3 object based on its file extension and returns it as a Pandas DataFrame, JSON object, Pickle object, or GeoDataFrame, depending on the file extension.</p> <p>Example :</p> <p>s3_loader = S3Loader(package='boto3') s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key') data = s3_loader.load(bucket='your_bucket', key='your_file.csv')</p> Source code in <code>igf_toolbox/s3/loaders.py</code> <pre><code>class S3Loader(_S3Connection):\n    \"\"\"\n    A class for loading data from an Amazon S3 bucket using 'boto3' or 's3fs' as the underlying package.\n\n    This class extends the `_S3Connection` parent class and provides methods for establishing a connection to the S3 bucket\n    and loading data from a specified S3 object.\n\n    Args:\n        package (str): The package to use for connecting to S3 ('s3fs' or 'boto3').\n\n    Methods:\n        connect(**kwargs):\n            Establishes a connection to the S3 bucket.\n\n        load(bucket, key, **kwargs):\n            Loads data from a specified S3 object based on its file extension and returns it as a Pandas DataFrame, JSON object,\n            Pickle object, or GeoDataFrame, depending on the file extension.\n\n    Example :\n    &gt;&gt;&gt; s3_loader = S3Loader(package='boto3')\n    &gt;&gt;&gt; s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n    &gt;&gt;&gt; data = s3_loader.load(bucket='your_bucket', key='your_file.csv')\n    \"\"\"\n\n    def __init__(self, package: Optional[str] = \"boto3\") -&gt; None:\n        \"\"\"\n        Initialize the S3Loader class with the specified package.\n\n        Args:\n            package (str): The package to use for connecting to S3 ('s3fs' or 'boto3').\n        \"\"\"\n        # Initialisation du parent\n        super().__init__(package=package)\n\n    def connect(self, **kwargs) -&gt; None:\n        \"\"\"\n        Establish a connection to the S3 bucket.\n\n        Args:\n            **kwargs: Additional keyword arguments for establishing the connection.\n\n        Returns:\n            obj: The established S3 connection.\n\n        Example :\n        &gt;&gt;&gt; s3_loader = S3Loader(package='boto3')\n        &gt;&gt;&gt; s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n        \"\"\"\n        # Etablissement d'une connection\n        return self._connect(**kwargs)\n\n    def load(self, bucket: str, key: str, **kwargs) -&gt; None:\n        \"\"\"\n        Load data from a specified S3 object based on its file extension.\n\n        Args:\n            bucket (str): The name of the S3 bucket.\n            key (str): The key of the S3 object to load.\n            **kwargs: Additional keyword arguments for reading the data.\n\n        Returns:\n            obj: The loaded data (Pandas DataFrame, JSON object, Pickle object, or GeoDataFrame).\n\n        Example :\n        &gt;&gt;&gt; s3_loader = S3Loader(package='boto3')\n        &gt;&gt;&gt; s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n        &gt;&gt;&gt; data = s3_loader.load(bucket='your_bucket', key='your_file.csv')\n        \"\"\"\n        # Etablissement d'une connexion s'il n'en existe pas une nouvelle\n        if not hasattr(self, \"s3\"):\n            self.connect()\n\n        # Extraction de l'extension du fichier \u00e0 charger\n        extension = key.split(\".\")[-1]\n\n        # Chargement des donn\u00e9es\n        if self.package == \"boto3\":\n            # Ouverture du fichier\n            s3_file = self.s3.get_object(Bucket=bucket, Key=key)[\"Body\"]\n            # Test suivant l'extension du fichier \u00e0 charger et lecture de ce-dernier\n            if extension == \"xlsx\":\n                data = pd.read_excel(s3_file.read(), engine=\"openpyxl\", **kwargs)\n            elif extension == \"xls\":\n                data = pd.read_excel(s3_file.read(), engine=\"xlrd\", **kwargs)\n            elif extension == \"parquet\":\n                data = pd.read_parquet(BytesIO(s3_file.read()), **kwargs)\n            else:\n                data = self._read_data(s3_file=s3_file, extension=extension, **kwargs)\n        elif self.package == \"s3fs\":\n            with self.s3.open(f\"{bucket}/{key}\", \"rb\") as s3_file:\n                # Test suivant l'extension du fichier \u00e0 charger et lecture de ce-dernier\n                if extension == \"xlsx\":\n                    data = pd.read_excel(s3_file, engine=\"openpyxl\", **kwargs)\n                elif extension == \"xls\":\n                    data = pd.read_excel(s3_file, engine=\"xlrd\", **kwargs)\n                else:\n                    data = self._read_data(\n                        s3_file=s3_file, extension=extension, **kwargs\n                    )\n\n        return data\n\n    # Fonction auxiliaire de lecture des donn\u00e9es\n    def _read_data(self, s3_file, extension: str, **kwargs):\n        \"\"\"\n        Read data from an S3 file based on its extension.\n\n        This function reads data from an S3 file based on its file extension and returns the data as a Pandas DataFrame,\n        JSON object, Pickle object, or GeoDataFrame, depending on the extension.\n\n        Args:\n            s3_file (obj): The S3 file object to read from.\n            extension (str): The file extension indicating the file format ('csv', 'json', 'pkl', 'geojson', 'parquet').\n            **kwargs: Additional keyword arguments specific to the file format's reading method.\n\n        Returns:\n            obj: The read data (Pandas DataFrame, JSON object, Pickle object, or GeoDataFrame).\n\n        Raises:\n            ValueError: If the 'extension' argument is not one of ['csv', 'json', 'pkl', 'geojson', 'parquet'].\n\n        Example :\n        &gt;&gt;&gt; s3_connection = _S3Connection(package='boto3')\n        &gt;&gt;&gt; s3_client = s3_connection._connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n        &gt;&gt;&gt; s3_file = s3_client.get_object(Bucket='your_bucket', Key='your_file.csv')\n        &gt;&gt;&gt; data = _read_data(s3_file, extension='csv')\n        \"\"\"\n        # Test sur l'extension et lecture du fichier\n        if extension == \"csv\":\n            data = pd.read_csv(s3_file, **kwargs)\n        elif extension == \"json\":\n            data = json.load(s3_file, **kwargs)\n        elif extension == \"pkl\":\n            data = pd.read_pickle(s3_file, **kwargs)\n        elif extension == \"geojson\":\n            data = read_file(s3_file, **kwargs)\n        elif extension == \"parquet\":\n            data = pd.read_parquet(s3_file, **kwargs)\n        else:\n            raise ValueError(\n                \"Invalid extension : should be in ['xlsx', 'xls', 'json', 'pkl', 'geojson', 'csv', 'parquet'].\"\n            )\n        return data\n</code></pre>"},{"location":"api/S3Loader/#igf_toolbox.s3.loaders.S3Loader.__init__","title":"<code>__init__(package='boto3')</code>","text":"<p>Initialize the S3Loader class with the specified package.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>The package to use for connecting to S3 ('s3fs' or 'boto3').</p> <code>'boto3'</code> Source code in <code>igf_toolbox/s3/loaders.py</code> <pre><code>def __init__(self, package: Optional[str] = \"boto3\") -&gt; None:\n    \"\"\"\n    Initialize the S3Loader class with the specified package.\n\n    Args:\n        package (str): The package to use for connecting to S3 ('s3fs' or 'boto3').\n    \"\"\"\n    # Initialisation du parent\n    super().__init__(package=package)\n</code></pre>"},{"location":"api/S3Loader/#igf_toolbox.s3.loaders.S3Loader.connect","title":"<code>connect(**kwargs)</code>","text":"<p>Establish a connection to the S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments for establishing the connection.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>obj</code> <code>None</code> <p>The established S3 connection.</p> <p>Example :</p> <p>s3_loader = S3Loader(package='boto3') s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')</p> Source code in <code>igf_toolbox/s3/loaders.py</code> <pre><code>def connect(self, **kwargs) -&gt; None:\n    \"\"\"\n    Establish a connection to the S3 bucket.\n\n    Args:\n        **kwargs: Additional keyword arguments for establishing the connection.\n\n    Returns:\n        obj: The established S3 connection.\n\n    Example :\n    &gt;&gt;&gt; s3_loader = S3Loader(package='boto3')\n    &gt;&gt;&gt; s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n    \"\"\"\n    # Etablissement d'une connection\n    return self._connect(**kwargs)\n</code></pre>"},{"location":"api/S3Loader/#igf_toolbox.s3.loaders.S3Loader.load","title":"<code>load(bucket, key, **kwargs)</code>","text":"<p>Load data from a specified S3 object based on its file extension.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>key</code> <code>str</code> <p>The key of the S3 object to load.</p> required <code>**kwargs</code> <p>Additional keyword arguments for reading the data.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>obj</code> <code>None</code> <p>The loaded data (Pandas DataFrame, JSON object, Pickle object, or GeoDataFrame).</p> <p>Example :</p> <p>s3_loader = S3Loader(package='boto3') s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key') data = s3_loader.load(bucket='your_bucket', key='your_file.csv')</p> Source code in <code>igf_toolbox/s3/loaders.py</code> <pre><code>def load(self, bucket: str, key: str, **kwargs) -&gt; None:\n    \"\"\"\n    Load data from a specified S3 object based on its file extension.\n\n    Args:\n        bucket (str): The name of the S3 bucket.\n        key (str): The key of the S3 object to load.\n        **kwargs: Additional keyword arguments for reading the data.\n\n    Returns:\n        obj: The loaded data (Pandas DataFrame, JSON object, Pickle object, or GeoDataFrame).\n\n    Example :\n    &gt;&gt;&gt; s3_loader = S3Loader(package='boto3')\n    &gt;&gt;&gt; s3_connection = s3_loader.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n    &gt;&gt;&gt; data = s3_loader.load(bucket='your_bucket', key='your_file.csv')\n    \"\"\"\n    # Etablissement d'une connexion s'il n'en existe pas une nouvelle\n    if not hasattr(self, \"s3\"):\n        self.connect()\n\n    # Extraction de l'extension du fichier \u00e0 charger\n    extension = key.split(\".\")[-1]\n\n    # Chargement des donn\u00e9es\n    if self.package == \"boto3\":\n        # Ouverture du fichier\n        s3_file = self.s3.get_object(Bucket=bucket, Key=key)[\"Body\"]\n        # Test suivant l'extension du fichier \u00e0 charger et lecture de ce-dernier\n        if extension == \"xlsx\":\n            data = pd.read_excel(s3_file.read(), engine=\"openpyxl\", **kwargs)\n        elif extension == \"xls\":\n            data = pd.read_excel(s3_file.read(), engine=\"xlrd\", **kwargs)\n        elif extension == \"parquet\":\n            data = pd.read_parquet(BytesIO(s3_file.read()), **kwargs)\n        else:\n            data = self._read_data(s3_file=s3_file, extension=extension, **kwargs)\n    elif self.package == \"s3fs\":\n        with self.s3.open(f\"{bucket}/{key}\", \"rb\") as s3_file:\n            # Test suivant l'extension du fichier \u00e0 charger et lecture de ce-dernier\n            if extension == \"xlsx\":\n                data = pd.read_excel(s3_file, engine=\"openpyxl\", **kwargs)\n            elif extension == \"xls\":\n                data = pd.read_excel(s3_file, engine=\"xlrd\", **kwargs)\n            else:\n                data = self._read_data(\n                    s3_file=s3_file, extension=extension, **kwargs\n                )\n\n    return data\n</code></pre>"},{"location":"api/S3Saver/","title":"S3Saver","text":"<p>               Bases: <code>_S3Connection</code></p> <p>A class for saving data to an Amazon S3 bucket using 'boto3' or 's3fs' as the underlying package.</p> <p>This class extends the <code>_S3Connection</code> parent class and provides methods for establishing a connection to the S3 bucket and saving data to a specified S3 object.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>The package to use for connecting to S3 ('s3fs' or 'boto3').</p> <code>'boto3'</code> <p>Methods:</p> Name Description <code>connect</code> <p>Establishes a connection to the S3 bucket.</p> <code>save</code> <p>Saves an object to a specified S3 object based on its file extension and object type.</p> <p>Example :</p> <p>s3_saver = S3Saver(package='boto3') s3_connection = s3_saver.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key') s3_saver.save(bucket='your_bucket', key='your_file.csv', obj=dataframe)</p> Source code in <code>igf_toolbox/s3/savers.py</code> <pre><code>class S3Saver(_S3Connection):\n    \"\"\"\n    A class for saving data to an Amazon S3 bucket using 'boto3' or 's3fs' as the underlying package.\n\n    This class extends the `_S3Connection` parent class and provides methods for establishing a connection to the S3 bucket\n    and saving data to a specified S3 object.\n\n    Args:\n        package (str): The package to use for connecting to S3 ('s3fs' or 'boto3').\n\n    Methods:\n        connect(**kwargs):\n            Establishes a connection to the S3 bucket.\n\n        save(bucket, key, obj=None, **kwargs):\n            Saves an object to a specified S3 object based on its file extension and object type.\n\n    Example :\n    &gt;&gt;&gt; s3_saver = S3Saver(package='boto3')\n    &gt;&gt;&gt; s3_connection = s3_saver.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n    &gt;&gt;&gt; s3_saver.save(bucket='your_bucket', key='your_file.csv', obj=dataframe)\n    \"\"\"\n\n    def __init__(self, package: Optional[str] = \"boto3\") -&gt; None:\n        \"\"\"\n        Initialize the S3Saver class with the specified package.\n\n        Args:\n            package (str): The package to use for connecting to S3 ('s3fs' or 'boto3').\n        \"\"\"\n        # Initialisation du parent\n        super().__init__(package=package)\n\n    def connect(self, **kwargs) -&gt; None:\n        \"\"\"\n        Establish a connection to the S3 bucket.\n\n        Args:\n            **kwargs: Additional keyword arguments for establishing the connection.\n\n        Returns:\n            obj: The established S3 connection.\n\n        Example usage:\n        &gt;&gt;&gt; s3_saver = S3Saver(package='boto3')\n        &gt;&gt;&gt; s3_connection = s3_saver.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n        \"\"\"\n        # Etablissement d'une connection\n        return self._connect(**kwargs)\n\n    def save(\n        self, bucket: str, key: str, obj: Optional[Union[object, None]] = None, **kwargs\n    ) -&gt; None:\n        \"\"\"\n        Save an object to a specified S3 object based on its file extension and object type.\n\n        Args:\n            bucket (str): The name of the S3 bucket.\n            key (str): The key of the S3 object to save.\n            obj (obj): The object to save (Pandas DataFrame, dictionary, Pickle object, Matplotlib figure, etc.).\n            **kwargs: Additional keyword arguments for saving the object.\n\n        Raises:\n            ValueError: If the 'extension' argument is not one of ['csv', 'xlsx', 'xls', 'json', 'pkl', 'png', 'parquet].\n\n        Example :\n        &gt;&gt;&gt; s3_saver = S3Saver(package='boto3')\n        &gt;&gt;&gt; s3_connection = s3_saver.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n        &gt;&gt;&gt; s3_saver.save(bucket='your_bucket', key='your_file.csv', obj=dataframe)\n        \"\"\"\n        # Etablissement d'une connexion s'il n'en existe pas une nouvelle\n        if not hasattr(self, \"s3\"):\n            self.connect()\n\n        # Extraction de l'extension du fichier \u00e0 charger\n        extension = key.split(\".\")[-1]\n\n        # Exportation de l'objet\n        if self.package == \"boto3\":\n            if extension == \"csv\":\n                self.s3.put_object(Bucket=bucket, Key=key, Body=obj.to_csv(**kwargs))\n            elif extension in [\"xlsx\", \"xls\"]:\n                # Si l'objet est un dictionnaire de DataFrame, un jeu de donn\u00e9es est export\u00e9 par feuille\n                if isinstance(obj, dict):\n                    # Construction de l'objet \u00e0 exporter\n                    with BytesIO() as output:\n                        with pd.ExcelWriter(output, engine=\"xlsxwriter\") as writer:\n                            for key_obj, value_obj in obj.items():\n                                # La longueur d'une sheet_name est major\u00e9 \u00e0 31 caract\u00e8res\n                                export_key = (\n                                    key_obj if len(key_obj) &lt;= 31 else key_obj[:31]\n                                )\n                                value_obj.to_excel(\n                                    writer, sheet_name=export_key, **kwargs\n                                )\n                        output_data = output.getvalue()\n                elif isinstance(obj, pd.DataFrame):\n                    with BytesIO() as output:\n                        with pd.ExcelWriter(output, engine=\"xlsxwriter\") as writer:\n                            obj.to_excel(writer, **kwargs)\n                        output_data = output.getvalue()\n                # Exportation de l'objet\n                self.s3.put_object(Bucket=bucket, Key=key, Body=output_data)\n            elif extension == \"json\":\n                if isinstance(obj, pd.DataFrame):\n                    self.s3.put_object(\n                        Bucket=bucket, Key=key, Body=obj.to_json(**kwargs)\n                    )\n                else:\n                    self.s3.put_object(Bucket=bucket, Key=key, Body=dumps(obj))\n            elif extension == \"pkl\":\n                with BytesIO() as output:\n                    dump(obj, output)\n                    output_data = output.getvalue()\n                self.s3.put_object(Bucket=bucket, Key=key, Body=output_data)\n            elif extension == \"png\":\n                # Construction de l'objet \u00e0 exporter\n                with BytesIO() as output:\n                    savefig(output, format=\"png\", **kwargs)\n                    output_data = output.getvalue()\n                # Exportation de l'objet\n                self.s3.put_object(Bucket=bucket, Key=key, Body=output_data)\n                # Fermeture des figures\n                close(\"all\")\n            elif extension == \"parquet\":\n                # Construction de l'objet \u00e0 exporter\n                with BytesIO() as output:\n                    obj.to_parquet(output, **kwargs)\n            elif extension == \"geojson\":\n                self.s3.put_object(\n                    Bucket=bucket, Key=key, Body=obj.to_json().encode(\"utf-8\")\n                )\n            else:\n                raise ValueError(\n                    \"File type should either be csv, xlsx, xls, json, pkl, geojson or png.\"\n                )\n\n        elif self.package == \"s3fs\":\n            # Distinction suivant le format du fichier et export\n            if extension in [\"xlsx\", \"xls\"]:\n                with self.s3.open(f\"{bucket}/{key}\", \"wb\") as s3_file:\n                    # Si l'objet est un dictionnaire de DataFrame, un jeu de donn\u00e9es est export\u00e9 par feuille\n                    if isinstance(obj, dict):\n                        # Construction de l'objet \u00e0 exporter\n                        with BytesIO() as output:\n                            with pd.ExcelWriter(output, engine=\"xlsxwriter\") as writer:\n                                for key_obj, value_obj in obj.items():\n                                    # La longueur d'une sheet_name est major\u00e9e \u00e0 31 caract\u00e8res\n                                    export_key = (\n                                        key_obj if len(key_obj) &lt;= 31 else key_obj[:31]\n                                    )\n                                    value_obj.to_excel(\n                                        writer, sheet_name=export_key, **kwargs\n                                    )\n                            output_data = output.getvalue()\n                        # Exportation de l'objet\n                        s3_file.write(output_data)\n                    elif isinstance(obj, pd.DataFrame):\n                        # Exportation de l'objet\n                        with pd.ExcelWriter(s3_file, engine=\"xlsxwriter\") as writer:\n                            obj.to_excel(writer, **kwargs)\n            elif extension == \"parquet\":\n                with self.s3.open(f\"{bucket}/{key}\", \"wb\") as s3_file:\n                    obj.to_parquet(s3_file)\n            elif extension == \"png\":\n                with self.s3.open(f\"{bucket}/{key}\", \"wb\") as s3_file:\n                    # Construction de l'objet \u00e0 exporter\n                    with io.BytesIO() as output:\n                        plt.savefig(output, format=\"png\", **kwargs)\n                        output_data = output.getvalue()\n                    # Exportation de l'objet\n                    s3_file.write(output_data)\n                    # Fermeture des figures\n                    plt.close(\"all\")\n            else:\n                with self.s3.open(f\"{bucket}/{key}\", \"w\") as s3_file:\n                    # Distinction suivant le format du fichier et export\n                    if extension == \"csv\":\n                        obj.to_csv(s3_file, **kwargs)\n                    elif extension == \"json\":\n                        if isinstance(obj, pd.DataFrame):\n                            s3_file.write(obj.to_json(**kwargs))\n                        else:\n                            s3_file.write(dumps(obj))\n                    elif extension == \"pkl\":\n                        obj.to_pickle(s3_file, **kwargs)\n                    elif extension == \"geojson\":\n                        obj.to_file(s3_file, **kwargs)\n                    else:\n                        raise ValueError(\n                            \"File type should either be csv, xlsx, xls, json, pkl, parquet, geojson or png.\"\n                        )\n</code></pre>"},{"location":"api/S3Saver/#igf_toolbox.s3.savers.S3Saver.__init__","title":"<code>__init__(package='boto3')</code>","text":"<p>Initialize the S3Saver class with the specified package.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>The package to use for connecting to S3 ('s3fs' or 'boto3').</p> <code>'boto3'</code> Source code in <code>igf_toolbox/s3/savers.py</code> <pre><code>def __init__(self, package: Optional[str] = \"boto3\") -&gt; None:\n    \"\"\"\n    Initialize the S3Saver class with the specified package.\n\n    Args:\n        package (str): The package to use for connecting to S3 ('s3fs' or 'boto3').\n    \"\"\"\n    # Initialisation du parent\n    super().__init__(package=package)\n</code></pre>"},{"location":"api/S3Saver/#igf_toolbox.s3.savers.S3Saver.connect","title":"<code>connect(**kwargs)</code>","text":"<p>Establish a connection to the S3 bucket.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <p>Additional keyword arguments for establishing the connection.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>obj</code> <code>None</code> <p>The established S3 connection.</p> <p>Example usage:</p> <p>s3_saver = S3Saver(package='boto3') s3_connection = s3_saver.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')</p> Source code in <code>igf_toolbox/s3/savers.py</code> <pre><code>def connect(self, **kwargs) -&gt; None:\n    \"\"\"\n    Establish a connection to the S3 bucket.\n\n    Args:\n        **kwargs: Additional keyword arguments for establishing the connection.\n\n    Returns:\n        obj: The established S3 connection.\n\n    Example usage:\n    &gt;&gt;&gt; s3_saver = S3Saver(package='boto3')\n    &gt;&gt;&gt; s3_connection = s3_saver.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n    \"\"\"\n    # Etablissement d'une connection\n    return self._connect(**kwargs)\n</code></pre>"},{"location":"api/S3Saver/#igf_toolbox.s3.savers.S3Saver.save","title":"<code>save(bucket, key, obj=None, **kwargs)</code>","text":"<p>Save an object to a specified S3 object based on its file extension and object type.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The name of the S3 bucket.</p> required <code>key</code> <code>str</code> <p>The key of the S3 object to save.</p> required <code>obj</code> <code>obj</code> <p>The object to save (Pandas DataFrame, dictionary, Pickle object, Matplotlib figure, etc.).</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for saving the object.</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the 'extension' argument is not one of ['csv', 'xlsx', 'xls', 'json', 'pkl', 'png', 'parquet].</p> <p>Example :</p> <p>s3_saver = S3Saver(package='boto3') s3_connection = s3_saver.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key') s3_saver.save(bucket='your_bucket', key='your_file.csv', obj=dataframe)</p> Source code in <code>igf_toolbox/s3/savers.py</code> <pre><code>def save(\n    self, bucket: str, key: str, obj: Optional[Union[object, None]] = None, **kwargs\n) -&gt; None:\n    \"\"\"\n    Save an object to a specified S3 object based on its file extension and object type.\n\n    Args:\n        bucket (str): The name of the S3 bucket.\n        key (str): The key of the S3 object to save.\n        obj (obj): The object to save (Pandas DataFrame, dictionary, Pickle object, Matplotlib figure, etc.).\n        **kwargs: Additional keyword arguments for saving the object.\n\n    Raises:\n        ValueError: If the 'extension' argument is not one of ['csv', 'xlsx', 'xls', 'json', 'pkl', 'png', 'parquet].\n\n    Example :\n    &gt;&gt;&gt; s3_saver = S3Saver(package='boto3')\n    &gt;&gt;&gt; s3_connection = s3_saver.connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n    &gt;&gt;&gt; s3_saver.save(bucket='your_bucket', key='your_file.csv', obj=dataframe)\n    \"\"\"\n    # Etablissement d'une connexion s'il n'en existe pas une nouvelle\n    if not hasattr(self, \"s3\"):\n        self.connect()\n\n    # Extraction de l'extension du fichier \u00e0 charger\n    extension = key.split(\".\")[-1]\n\n    # Exportation de l'objet\n    if self.package == \"boto3\":\n        if extension == \"csv\":\n            self.s3.put_object(Bucket=bucket, Key=key, Body=obj.to_csv(**kwargs))\n        elif extension in [\"xlsx\", \"xls\"]:\n            # Si l'objet est un dictionnaire de DataFrame, un jeu de donn\u00e9es est export\u00e9 par feuille\n            if isinstance(obj, dict):\n                # Construction de l'objet \u00e0 exporter\n                with BytesIO() as output:\n                    with pd.ExcelWriter(output, engine=\"xlsxwriter\") as writer:\n                        for key_obj, value_obj in obj.items():\n                            # La longueur d'une sheet_name est major\u00e9 \u00e0 31 caract\u00e8res\n                            export_key = (\n                                key_obj if len(key_obj) &lt;= 31 else key_obj[:31]\n                            )\n                            value_obj.to_excel(\n                                writer, sheet_name=export_key, **kwargs\n                            )\n                    output_data = output.getvalue()\n            elif isinstance(obj, pd.DataFrame):\n                with BytesIO() as output:\n                    with pd.ExcelWriter(output, engine=\"xlsxwriter\") as writer:\n                        obj.to_excel(writer, **kwargs)\n                    output_data = output.getvalue()\n            # Exportation de l'objet\n            self.s3.put_object(Bucket=bucket, Key=key, Body=output_data)\n        elif extension == \"json\":\n            if isinstance(obj, pd.DataFrame):\n                self.s3.put_object(\n                    Bucket=bucket, Key=key, Body=obj.to_json(**kwargs)\n                )\n            else:\n                self.s3.put_object(Bucket=bucket, Key=key, Body=dumps(obj))\n        elif extension == \"pkl\":\n            with BytesIO() as output:\n                dump(obj, output)\n                output_data = output.getvalue()\n            self.s3.put_object(Bucket=bucket, Key=key, Body=output_data)\n        elif extension == \"png\":\n            # Construction de l'objet \u00e0 exporter\n            with BytesIO() as output:\n                savefig(output, format=\"png\", **kwargs)\n                output_data = output.getvalue()\n            # Exportation de l'objet\n            self.s3.put_object(Bucket=bucket, Key=key, Body=output_data)\n            # Fermeture des figures\n            close(\"all\")\n        elif extension == \"parquet\":\n            # Construction de l'objet \u00e0 exporter\n            with BytesIO() as output:\n                obj.to_parquet(output, **kwargs)\n        elif extension == \"geojson\":\n            self.s3.put_object(\n                Bucket=bucket, Key=key, Body=obj.to_json().encode(\"utf-8\")\n            )\n        else:\n            raise ValueError(\n                \"File type should either be csv, xlsx, xls, json, pkl, geojson or png.\"\n            )\n\n    elif self.package == \"s3fs\":\n        # Distinction suivant le format du fichier et export\n        if extension in [\"xlsx\", \"xls\"]:\n            with self.s3.open(f\"{bucket}/{key}\", \"wb\") as s3_file:\n                # Si l'objet est un dictionnaire de DataFrame, un jeu de donn\u00e9es est export\u00e9 par feuille\n                if isinstance(obj, dict):\n                    # Construction de l'objet \u00e0 exporter\n                    with BytesIO() as output:\n                        with pd.ExcelWriter(output, engine=\"xlsxwriter\") as writer:\n                            for key_obj, value_obj in obj.items():\n                                # La longueur d'une sheet_name est major\u00e9e \u00e0 31 caract\u00e8res\n                                export_key = (\n                                    key_obj if len(key_obj) &lt;= 31 else key_obj[:31]\n                                )\n                                value_obj.to_excel(\n                                    writer, sheet_name=export_key, **kwargs\n                                )\n                        output_data = output.getvalue()\n                    # Exportation de l'objet\n                    s3_file.write(output_data)\n                elif isinstance(obj, pd.DataFrame):\n                    # Exportation de l'objet\n                    with pd.ExcelWriter(s3_file, engine=\"xlsxwriter\") as writer:\n                        obj.to_excel(writer, **kwargs)\n        elif extension == \"parquet\":\n            with self.s3.open(f\"{bucket}/{key}\", \"wb\") as s3_file:\n                obj.to_parquet(s3_file)\n        elif extension == \"png\":\n            with self.s3.open(f\"{bucket}/{key}\", \"wb\") as s3_file:\n                # Construction de l'objet \u00e0 exporter\n                with io.BytesIO() as output:\n                    plt.savefig(output, format=\"png\", **kwargs)\n                    output_data = output.getvalue()\n                # Exportation de l'objet\n                s3_file.write(output_data)\n                # Fermeture des figures\n                plt.close(\"all\")\n        else:\n            with self.s3.open(f\"{bucket}/{key}\", \"w\") as s3_file:\n                # Distinction suivant le format du fichier et export\n                if extension == \"csv\":\n                    obj.to_csv(s3_file, **kwargs)\n                elif extension == \"json\":\n                    if isinstance(obj, pd.DataFrame):\n                        s3_file.write(obj.to_json(**kwargs))\n                    else:\n                        s3_file.write(dumps(obj))\n                elif extension == \"pkl\":\n                    obj.to_pickle(s3_file, **kwargs)\n                elif extension == \"geojson\":\n                    obj.to_file(s3_file, **kwargs)\n                else:\n                    raise ValueError(\n                        \"File type should either be csv, xlsx, xls, json, pkl, parquet, geojson or png.\"\n                    )\n</code></pre>"},{"location":"api/SecondarySecretStatController/","title":"SecondarySecretStatController","text":"<p>               Bases: <code>object</code></p> <p>A class for controlling secondary statistical secrecy in a dataset.</p> <p>This class provides methods to control secondary statistical secrecy in a dataset based on specified grouping variables and a secrecy strategy. It allows for the identification and flagging of non-secret observations based on the strategy.</p> <p>Attributes:</p> Name Type Description <code>list_var_groupby</code> <code>List[str]</code> <p>A list of variables used for grouping.</p> <code>var_individu</code> <code>Union[str, None]</code> <p>The variable representing individual-level data.</p> <code>var_entreprise</code> <code>Union[str, None]</code> <p>The variable representing enterprise-level data.</p> <code>strategy</code> <code>Optional[str]</code> <p>The strategy for controlling secrecy. Either 'min' or 'total'.</p> <p>Methods:</p> Name Description <code>var_effectif</code> <p>A property that returns the variable representing the number of observations or entities.</p> <code>_control_local_ss2</code> <p>A method for controlling secondary statistical secrecy at the local level using the specified strategy.</p> <code>_iterate_local_ss2</code> <p>A method for iteratively controlling the secondary statistical secrecy at the local level for different subgroups.</p> <code>control_ss2</code> <p>A method for controlling secondary statistical secrecy for a given dataset.</p> Source code in <code>igf_toolbox/stats_des/control_secret_stat.py</code> <pre><code>class SecondarySecretStatController(object):\n    \"\"\"\n    A class for controlling secondary statistical secrecy in a dataset.\n\n    This class provides methods to control secondary statistical secrecy in a dataset based on specified grouping variables\n    and a secrecy strategy. It allows for the identification and flagging of non-secret observations based on the strategy.\n\n    Attributes:\n        list_var_groupby (List[str]): A list of variables used for grouping.\n        var_individu (Union[str, None]): The variable representing individual-level data.\n        var_entreprise (Union[str, None]): The variable representing enterprise-level data.\n        strategy (Optional[str]): The strategy for controlling secrecy. Either 'min' or 'total'.\n\n    Methods:\n        var_effectif: A property that returns the variable representing the number of observations or entities.\n        _control_local_ss2: A method for controlling secondary statistical secrecy at the local level using the specified strategy.\n        _iterate_local_ss2: A method for iteratively controlling the secondary statistical secrecy at the local level for different subgroups.\n        control_ss2: A method for controlling secondary statistical secrecy for a given dataset.\n    \"\"\"\n\n    # Initialisation\n    def __init__(\n        self,\n        list_var_groupby: List[str],\n        var_individu: Union[str, None],\n        var_entreprise: Union[str, None],\n        strategy: Optional[str] = \"total\",\n    ) -&gt; None:\n        \"\"\"\n        Initialize the SecondarySecretStatController class.\n\n        Parameters:\n            list_var_groupby (List[str]): A list of variables used for grouping.\n            var_individu (Union[str, None]): The variable representing individual-level data.\n            var_entreprise (Union[str, None]): The variable representing enterprise-level data.\n            strategy (Optional[str]): The strategy for controlling secrecy. Either 'min' or 'total'. Defaults to 'total'.\n        \"\"\"\n        # Initialisation des param\u00e8tres\n        self.list_var_groupby = list_var_groupby\n        self.var_individu = var_individu\n        self.var_entreprise = var_entreprise\n        self.strategy = strategy\n\n    # Initialisation de la variable d'effectifs\n    @property\n    def var_effectif(self) -&gt; Union[str, None]:\n        \"\"\"\n        Get the variable representing the number of observations or entities.\n\n        Returns:\n            Union[str, None]: The variable representing the number of observations or entities, or None if not defined.\n        \"\"\"\n        if self.var_individu is not None:\n            var_effectif = f\"{self.var_individu}_nunique\"\n        elif self.var_entreprise is not None:\n            var_effectif = f\"{self.var_entreprise}_nunique\"\n        else:\n            var_effectif = None\n        return var_effectif\n\n    # Fonction liminaire de contr\u00f4le du secret statistique secondaire sur un sous-ensemble des variables\n    def _control_local_ss2(\n        self,\n        data: pd.DataFrame,\n        list_var_subgroupby: List[str],\n        var_secret_primary: str,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Control for secondary statistical secrecy at the local level using the specified strategy.\n\n        This function performs secondary statistical secrecy control at the local level using the specified strategy, where only specific\n        data points are flagged as non-secret based on a given strategy.\n\n        Parameters:\n            data (DataFrame): The dataset containing descriptive statistics.\n            list_var_subgroupby (List[str]): The list of variables used in the sub-groupby.\n            var_secret_primary (str): The primary secret variable to control.\n\n        Returns:\n            DataFrame: A DataFrame with a boolean column indicating whether each observation is considered non-secret (True) or secret (False).\n\n        Raises:\n            ValueError: If the 'var_effectif' parameter is not provided when the strategy is 'min'.\n        \"\"\"\n        # Extraction du level d'int\u00e9r\u00eat\n        level_of_interest = np.setdiff1d(data.index.names, list_var_subgroupby)[0]\n        # Initialisation de la s\u00e9rie r\u00e9sultat\n        serie_res = pd.Series(\n            True, index=data.index.get_level_values(level_of_interest)\n        )\n        # Si le nombre de False vaut le nombre d'observations -1, il faut controler le secret\n        if len(data) - data[var_secret_primary].sum() == 1:\n            # Diff\u00e9renciation suivant la strat\u00e9gie\n            if self.strategy == \"min\":\n                if self.var_effectif is not None:\n                    # Extraction de l'indice d'int\u00e9r\u00eat\n                    idx_of_interest = (\n                        data.loc[data[var_secret_primary], self.var_effectif]\n                        .droplevel(list_var_subgroupby)\n                        .idxmin()\n                    )\n                    serie_res.loc[idx_of_interest] = False\n                else:\n                    raise ValueError(\"'self.var_effectif' is undefined\")\n            elif self.strategy == \"total\":\n                serie_res.loc[\"Total\"] = False\n\n        return serie_res.to_frame()\n\n    # Fonction liminaire d'it\u00e9ration du contr\u00f4le local du secret statistique secondaire\n    def _iterate_local_ss2(\n        self,\n        data: pd.DataFrame,\n        subgroupby_iterable: Iterable,\n        var_secret_primary: str,\n        var_secret_secondary: str,\n    ) -&gt; Tuple[pd.DataFrame, List[str]]:\n        \"\"\"\n        Iterate the secondary statistical secrecy control at the local level.\n\n        This function iteratively controls the secondary statistical secrecy at the local level for different subgroups\n        defined by combinations of grouping variables. It generates secondary secrecy flags for these subgroups based on\n        the specified strategy.\n\n        Parameters:\n            data (DataFrame): The dataset to perform secrecy control on.\n            subgroupby_iterable (Iterable): An iterable containing combinations of grouping variables.\n            var_secret_primary (str): The primary secret variable to control.\n            var_secret_secondary (str): The secondary secret variable to control.\n\n        Returns:\n            Tuple[DataFrame, List[str]]: A tuple containing the modified dataset with secondary secrecy flags added for subgroups\n                and a list of combinations of grouping variables that need further secrecy control.\n        \"\"\"\n        # Copie ind\u00e9pendante du jeu de donn\u00e9es\n        data_work = data.copy()\n        # Initialisation de la liste des combinaisons \u00e0 reparcourir\n        list_reiterate = []\n        for e in subgroupby_iterable:\n            # Initialisation de la liste du groupby\n            list_var_subgroupby = list(e)\n            # Nom de la colonne\n            name_col_ss2 = \"_\".join(list_var_subgroupby) + \"_ss2\"\n            # Construction de la colonne de secret stat secondaire\n            ss2_series = data_work.groupby(list_var_subgroupby).apply(\n                func=lambda x: self._control_local_ss2(\n                    data=x,\n                    list_var_subgroupby=list_var_subgroupby,\n                    var_secret_primary=var_secret_primary,\n                )\n            )\n            data_work[name_col_ss2] = ss2_series\n            # Combinaisons \u00e0 reparcourir\n            if data_work[name_col_ss2].sum() &lt; len(data_work):\n                # Construction de la liste des variables \u00e0 ajouter\n                if len(list_var_subgroupby) == 1:\n                    list_combinations_add = np.setdiff1d(\n                        self.list_var_groupby, list_var_subgroupby\n                    ).tolist()\n                elif len(list_var_subgroupby) &gt;= 2:\n                    # Extraction du degr\u00e9 de libert\u00e9\n                    list_var_compl = np.setdiff1d(\n                        self.list_var_groupby, list_var_subgroupby\n                    ).tolist()\n                    list_combinations_add = [\n                        list(e) + list_var_compl\n                        for e in combinations(\n                            list_var_subgroupby, len(self.list_var_groupby) - 2\n                        )\n                    ]\n                # Ajout \u00e0 la liste r\u00e9sultat\n                list_reiterate.append(list_combinations_add)\n\n        # Synth\u00e8se du secret statistique primaire et des diff\u00e9rents secrets statistiques secondaires avant une \u00e9ventuelle nouovelle it\u00e9ration sur les nouveaux Nan ajout\u00e9s\n        if self.var_effectif is not None:\n            list_col_ss2 = np.setdiff1d(\n                data_work.columns.tolist(),\n                [var_secret_primary, var_secret_secondary, self.var_effectif],\n            ).tolist()\n        else:\n            list_col_ss2 = np.setdiff1d(\n                data_work.columns.tolist(), [var_secret_primary, var_secret_secondary]\n            ).tolist()\n        data_work[var_secret_secondary] &amp;= data_work[list_col_ss2].all(axis=1)\n        data_work[var_secret_primary] = data_work[\n            [var_secret_primary, var_secret_secondary]\n        ].all(axis=1)\n\n        return data_work.drop(list_col_ss2, axis=1), list_reiterate\n\n    # Fonction liminaire de contr\u00f4le du secret statistique\n    def control_ss2(\n        self, data: pd.DataFrame, var_ss_primary: Optional[str] = \"secret_stat_primary\"\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Control secondary statistical secrecy for a given dataset.\n\n        This function controls the secondary statistical secrecy of a dataset based on a specified primary secrecy variable\n        ('var_ss_primary'). It checks for secrecy violations in different subgroups defined by grouping variables\n        ('var_individu' and 'var_entreprise'). The control process iterates over subgroups to determine the secondary secrecy\n        status.\n\n        Parameters:\n            data (DataFrame): The dataset containing primary secrecy information.\n            var_ss_primary (Optional[str]): The primary secrecy variable to control. Defaults to 'secret_stat_primary'.\n\n        Returns:\n            DataFrame: The dataset with secondary statistical secrecy controlled.\n        \"\"\"\n        # Dans le cas o\u00f9 l'on ne groupby que sur un seul \u00e9l\u00e9ment, on v\u00e9rifie juste qu'il y a z\u00e9ro ou strictement plus de une case ne respectant pas le secret statistique\n        if len(self.list_var_groupby) == 1:\n            # Ajout de la colonne relative au secret statistique secondaire\n            data[\"secret_stat_secondary\"] = self._control_local_ss2(\n                data=data, list_var_subgroupby=[], var_secret_primary=var_ss_primary\n            )\n            # Ajout de la colonne relative au secret statistique\n            data[\"secret_stat\"] = data[[var_ss_primary, \"secret_stat_secondary\"]].all(\n                axis=1\n            )\n        # Dans le cas o\u00f9 l'on groupby sur plus de variables, on v\u00e9rifie qu'il y a z\u00e9ro ou strictement plus de une case\n        else:\n            # Initialisation de la colonne de secret statistique\n            data[\"secret_stat\"] = data[var_ss_primary].copy()\n            # Initialisation des combinaisons \u00e0 parcourir\n            subgroupby_iterable = list(\n                combinations(self.list_var_groupby, len(self.list_var_groupby) - 1)\n            )\n            # Initialisation de la colonne d'int\u00e9r\u00eat\n            var_secret_primary = \"secret_stat\"\n            var_secret_secondary = \"secret_stat_secondary\"\n            # Initialisation du jeu de donn\u00e9es de travail\n            if self.var_effectif is not None:\n                data_source = data[[var_secret_primary, self.var_effectif]].copy()\n            else:\n                data_source = data[[var_secret_primary]].copy()\n            data_source[var_secret_secondary] = True\n\n            # It\u00e9ration\n            while len(subgroupby_iterable) &gt; 0:\n                data_source, subgroupby_iterable = self._iterate_local_ss2(\n                    data=data_source,\n                    subgroupby_iterable=subgroupby_iterable,\n                    var_secret_primary=var_secret_primary,\n                    var_secret_secondary=var_secret_secondary,\n                )\n\n            # Mise \u00e0 jour du secret statistique\n            data[[\"secret_stat_secondary\", \"secret_stat\"]] = data_source[\n                [\"secret_stat_secondary\", \"secret_stat\"]\n            ].copy()\n\n        return data\n</code></pre>"},{"location":"api/SecondarySecretStatController/#igf_toolbox.stats_des.control_secret_stat.SecondarySecretStatController.var_effectif","title":"<code>var_effectif: Union[str, None]</code>  <code>property</code>","text":"<p>Get the variable representing the number of observations or entities.</p> <p>Returns:</p> Type Description <code>Union[str, None]</code> <p>Union[str, None]: The variable representing the number of observations or entities, or None if not defined.</p>"},{"location":"api/SecondarySecretStatController/#igf_toolbox.stats_des.control_secret_stat.SecondarySecretStatController.__init__","title":"<code>__init__(list_var_groupby, var_individu, var_entreprise, strategy='total')</code>","text":"<p>Initialize the SecondarySecretStatController class.</p> <p>Parameters:</p> Name Type Description Default <code>list_var_groupby</code> <code>List[str]</code> <p>A list of variables used for grouping.</p> required <code>var_individu</code> <code>Union[str, None]</code> <p>The variable representing individual-level data.</p> required <code>var_entreprise</code> <code>Union[str, None]</code> <p>The variable representing enterprise-level data.</p> required <code>strategy</code> <code>Optional[str]</code> <p>The strategy for controlling secrecy. Either 'min' or 'total'. Defaults to 'total'.</p> <code>'total'</code> Source code in <code>igf_toolbox/stats_des/control_secret_stat.py</code> <pre><code>def __init__(\n    self,\n    list_var_groupby: List[str],\n    var_individu: Union[str, None],\n    var_entreprise: Union[str, None],\n    strategy: Optional[str] = \"total\",\n) -&gt; None:\n    \"\"\"\n    Initialize the SecondarySecretStatController class.\n\n    Parameters:\n        list_var_groupby (List[str]): A list of variables used for grouping.\n        var_individu (Union[str, None]): The variable representing individual-level data.\n        var_entreprise (Union[str, None]): The variable representing enterprise-level data.\n        strategy (Optional[str]): The strategy for controlling secrecy. Either 'min' or 'total'. Defaults to 'total'.\n    \"\"\"\n    # Initialisation des param\u00e8tres\n    self.list_var_groupby = list_var_groupby\n    self.var_individu = var_individu\n    self.var_entreprise = var_entreprise\n    self.strategy = strategy\n</code></pre>"},{"location":"api/SecondarySecretStatController/#igf_toolbox.stats_des.control_secret_stat.SecondarySecretStatController.control_ss2","title":"<code>control_ss2(data, var_ss_primary='secret_stat_primary')</code>","text":"<p>Control secondary statistical secrecy for a given dataset.</p> <p>This function controls the secondary statistical secrecy of a dataset based on a specified primary secrecy variable ('var_ss_primary'). It checks for secrecy violations in different subgroups defined by grouping variables ('var_individu' and 'var_entreprise'). The control process iterates over subgroups to determine the secondary secrecy status.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The dataset containing primary secrecy information.</p> required <code>var_ss_primary</code> <code>Optional[str]</code> <p>The primary secrecy variable to control. Defaults to 'secret_stat_primary'.</p> <code>'secret_stat_primary'</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>The dataset with secondary statistical secrecy controlled.</p> Source code in <code>igf_toolbox/stats_des/control_secret_stat.py</code> <pre><code>def control_ss2(\n    self, data: pd.DataFrame, var_ss_primary: Optional[str] = \"secret_stat_primary\"\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Control secondary statistical secrecy for a given dataset.\n\n    This function controls the secondary statistical secrecy of a dataset based on a specified primary secrecy variable\n    ('var_ss_primary'). It checks for secrecy violations in different subgroups defined by grouping variables\n    ('var_individu' and 'var_entreprise'). The control process iterates over subgroups to determine the secondary secrecy\n    status.\n\n    Parameters:\n        data (DataFrame): The dataset containing primary secrecy information.\n        var_ss_primary (Optional[str]): The primary secrecy variable to control. Defaults to 'secret_stat_primary'.\n\n    Returns:\n        DataFrame: The dataset with secondary statistical secrecy controlled.\n    \"\"\"\n    # Dans le cas o\u00f9 l'on ne groupby que sur un seul \u00e9l\u00e9ment, on v\u00e9rifie juste qu'il y a z\u00e9ro ou strictement plus de une case ne respectant pas le secret statistique\n    if len(self.list_var_groupby) == 1:\n        # Ajout de la colonne relative au secret statistique secondaire\n        data[\"secret_stat_secondary\"] = self._control_local_ss2(\n            data=data, list_var_subgroupby=[], var_secret_primary=var_ss_primary\n        )\n        # Ajout de la colonne relative au secret statistique\n        data[\"secret_stat\"] = data[[var_ss_primary, \"secret_stat_secondary\"]].all(\n            axis=1\n        )\n    # Dans le cas o\u00f9 l'on groupby sur plus de variables, on v\u00e9rifie qu'il y a z\u00e9ro ou strictement plus de une case\n    else:\n        # Initialisation de la colonne de secret statistique\n        data[\"secret_stat\"] = data[var_ss_primary].copy()\n        # Initialisation des combinaisons \u00e0 parcourir\n        subgroupby_iterable = list(\n            combinations(self.list_var_groupby, len(self.list_var_groupby) - 1)\n        )\n        # Initialisation de la colonne d'int\u00e9r\u00eat\n        var_secret_primary = \"secret_stat\"\n        var_secret_secondary = \"secret_stat_secondary\"\n        # Initialisation du jeu de donn\u00e9es de travail\n        if self.var_effectif is not None:\n            data_source = data[[var_secret_primary, self.var_effectif]].copy()\n        else:\n            data_source = data[[var_secret_primary]].copy()\n        data_source[var_secret_secondary] = True\n\n        # It\u00e9ration\n        while len(subgroupby_iterable) &gt; 0:\n            data_source, subgroupby_iterable = self._iterate_local_ss2(\n                data=data_source,\n                subgroupby_iterable=subgroupby_iterable,\n                var_secret_primary=var_secret_primary,\n                var_secret_secondary=var_secret_secondary,\n            )\n\n        # Mise \u00e0 jour du secret statistique\n        data[[\"secret_stat_secondary\", \"secret_stat\"]] = data_source[\n            [\"secret_stat_secondary\", \"secret_stat\"]\n        ].copy()\n\n    return data\n</code></pre>"},{"location":"api/SecretStatEstimator/","title":"SecretStatEstimator","text":"<p>               Bases: <code>StatDesGroupBy</code>, <code>PrimarySecretStatController</code>, <code>SecondarySecretStatController</code></p> <p>A class for estimating and controlling statistical secrecy in a dataset.</p> <p>This class inherits from three other classes: StatDesGroupBy, PrimarySecretStatController, and SecondarySecretStatController. It provides methods to estimate and control statistical secrecy in a dataset based on specified parameters.</p> <p>Attributes:</p> Name Type Description <code>data_source</code> <code>DataFrame</code> <p>The dataset to estimate and control secrecy.</p> <code>list_var_groupby</code> <code>List[str]</code> <p>The list of variables to group the data by.</p> <code>list_var_of_interest</code> <code>List[str]</code> <p>The list of variables of interest for statistics.</p> <code>var_individu</code> <code>Optional[Union[str, None]]</code> <p>The variable representing individual-level data.</p> <code>var_entreprise</code> <code>Optional[Union[str, None]]</code> <p>The variable representing enterprise-level data.</p> <code>var_weights</code> <code>Optional[Union[str, None]]</code> <p>The variable for weighting.</p> <code>threshold_secret_stat_effectif_individu</code> <code>Optional[Union[int, None]]</code> <p>The threshold for individual secrecy control.</p> <code>threshold_secret_stat_effectif_entreprise</code> <code>Optional[Union[int, None]]</code> <p>The threshold for enterprise secrecy control.</p> <code>threshold_secret_stat_contrib_individu</code> <code>Optional[Union[float, None]]</code> <p>The threshold for secondary individual secrecy control.</p> <code>threshold_secret_stat_contrib_entreprise</code> <code>Optional[Union[float, None]]</code> <p>The threshold for secondary enterprise secrecy control.</p> <code>strategy</code> <code>Optional[str]</code> <p>The control strategy ('total' or 'min').</p> <p>Methods:</p> Name Description <code>_clean_operations</code> <p>Clean and update the list of operations and variables of interest based on specific criteria.</p> <code>_get_drop_add_columns</code> <p>Determine columns to be added and dropped in a descriptive statistics dataset.</p> <code>estimate_secret_stat</code> <p>Estimate and control secondary statistical secrecy for a given dataset.</p> Source code in <code>igf_toolbox/stats_des/control_secret_stat.py</code> <pre><code>class SecretStatEstimator(\n    StatDesGroupBy, PrimarySecretStatController, SecondarySecretStatController\n):\n    \"\"\"\n    A class for estimating and controlling statistical secrecy in a dataset.\n\n    This class inherits from three other classes: StatDesGroupBy, PrimarySecretStatController, and SecondarySecretStatController.\n    It provides methods to estimate and control statistical secrecy in a dataset based on specified parameters.\n\n    Attributes:\n        data_source (DataFrame): The dataset to estimate and control secrecy.\n        list_var_groupby (List[str]): The list of variables to group the data by.\n        list_var_of_interest (List[str]): The list of variables of interest for statistics.\n        var_individu (Optional[Union[str, None]]): The variable representing individual-level data.\n        var_entreprise (Optional[Union[str, None]]): The variable representing enterprise-level data.\n        var_weights (Optional[Union[str, None]]): The variable for weighting.\n        threshold_secret_stat_effectif_individu (Optional[Union[int, None]]): The threshold for individual secrecy control.\n        threshold_secret_stat_effectif_entreprise (Optional[Union[int, None]]): The threshold for enterprise secrecy control.\n        threshold_secret_stat_contrib_individu (Optional[Union[float, None]]): The threshold for secondary individual secrecy control.\n        threshold_secret_stat_contrib_entreprise (Optional[Union[float, None]]): The threshold for secondary enterprise secrecy control.\n        strategy (Optional[str]): The control strategy ('total' or 'min').\n\n    Methods:\n        _clean_operations: Clean and update the list of operations and variables of interest based on specific criteria.\n        _get_drop_add_columns: Determine columns to be added and dropped in a descriptive statistics dataset.\n        estimate_secret_stat: Estimate and control secondary statistical secrecy for a given dataset.\n    \"\"\"\n\n    # Initialisation\n    def __init__(\n        self,\n        data_source: pd.DataFrame,\n        list_var_groupby: List[str],\n        list_var_of_interest: List[str],\n        var_individu: Optional[Union[str, None]] = None,\n        var_entreprise: Optional[Union[str, None]] = None,\n        var_weights: Optional[Union[str, None]] = None,\n        threshold_secret_stat_effectif_individu: Optional[Union[int, None]] = None,\n        threshold_secret_stat_effectif_entreprise: Optional[Union[int, None]] = None,\n        threshold_secret_stat_contrib_individu: Optional[Union[float, None]] = None,\n        threshold_secret_stat_contrib_entreprise: Optional[Union[float, None]] = None,\n        strategy: Optional[str] = \"total\",\n    ) -&gt; None:\n        \"\"\"\n        Initialize the SecretStatEstimator class.\n\n        Parameters:\n            data_source (DataFrame): The dataset to estimate and control secrecy.\n            list_var_groupby (List[str]): The list of variables to group the data by.\n            list_var_of_interest (List[str]): The list of variables of interest for statistics.\n            var_individu (Optional[Union[str, None]]): The variable representing individual-level data.\n            var_entreprise (Optional[Union[str, None]]): The variable representing enterprise-level data.\n            var_weights (Optional[Union[str, None]]): The variable for weighting.\n            threshold_secret_stat_effectif_individu (Optional[Union[int, None]]): The threshold for individual secrecy control.\n            threshold_secret_stat_effectif_entreprise (Optional[Union[int, None]]): The threshold for enterprise secrecy control.\n            threshold_secret_stat_contrib_individu (Optional[Union[float, None]]): The threshold for secondary individual secrecy control.\n            threshold_secret_stat_contrib_entreprise (Optional[Union[float, None]]): The threshold for secondary enterprise secrecy control.\n            strategy (Optional[str]): The control strategy ('total' or 'min').\n        \"\"\"\n        # Initialisation de la classe de statistiques descriptives\n        StatDesGroupBy.__init__(\n            self,\n            data_source=data_source,\n            list_var_groupby=list_var_groupby,\n            list_var_of_interest=list_var_of_interest,\n            var_individu=var_individu,\n            var_entreprise=var_entreprise,\n            var_weights=var_weights,\n            dropna=True,\n        )\n        # Initialisation des classes de contr\u00f4le du secret statistique\n        PrimarySecretStatController.__init__(\n            self,\n            var_individu=var_individu,\n            var_entreprise=var_entreprise,\n            threshold_secret_stat_effectif_individu=threshold_secret_stat_effectif_individu,\n            threshold_secret_stat_effectif_entreprise=threshold_secret_stat_effectif_entreprise,\n            threshold_secret_stat_contrib_individu=threshold_secret_stat_contrib_individu,\n            threshold_secret_stat_contrib_entreprise=threshold_secret_stat_contrib_entreprise,\n        )\n        SecondarySecretStatController.__init__(\n            self,\n            list_var_groupby=list_var_groupby,\n            var_individu=var_individu,\n            var_entreprise=var_entreprise,\n            strategy=strategy,\n        )\n\n    # Fonction liminaire de nettoyage des op\u00e9rations \u00e0 effectuer sur les donn\u00e9es\n    def _clean_operations(\n        self, iterable_operations: Union[dict, list]\n    ) -&gt; Tuple[Union[dict, list], List[str], List[str]]:\n        \"\"\"\n        Clean and update the list of operations and variables of interest based on specific criteria.\n\n        This function modifies the list of operations and variables of interest, ensuring it includes certain control operations\n        for statistical secrecy and makes necessary adjustments.\n\n        Parameters:\n            iterable_operations (dict or list): The initial list of operations.\n\n        Returns:\n            Tuple[Union[dict, list], List[str], List[str]]: A tuple containing the following:\n                - iterable_operations_work (list or dict): The cleaned and updated list of operations.\n                - list_var_of_interest_work (list): The cleaned and updated list of variables of interest.\n                - list_var_of_interest_max_sum_effectif (list): A list of variables related to secondary control operations.\n        \"\"\"\n        # Copie ind\u00e9pendante des op\u00e9rations\n        iterable_operations_work = deepcopy(iterable_operations)\n        # Copie ind\u00e9pendante des variables d'int\u00e9r\u00eat\n        list_var_of_interest_work = deepcopy(self.list_var_of_interest)\n        # Initialisation de la liste des max/sum\n        list_var_of_interest_max_sum_effectif = []\n        # Ajout des op\u00e9rations de contr\u00f4le du secret statistique si elles ne sont pas pr\u00e9vues dans la liste d'op\u00e9rations\n        if isinstance(iterable_operations, dict):\n            # Test du besoin d'ajout d'une op\u00e9ration de contr\u00f4le du secret statistique primaire\n            if \"count_effectif\" not in iterable_operations.keys():\n                iterable_operations_work[\"count_effectif\"] = []\n            # Suppression des \u00e9ventuelles op\u00e9rations dupliqu\u00e9es\n            if \"nunique\" in iterable_operations.keys():\n                # Comptage des individus et des entreprises\n                if (self.var_individu in iterable_operations[\"nunique\"]) &amp; (\n                    self.var_entreprise in iterable_operations[\"nunique\"]\n                ):\n                    if len(iterable_operations[\"nunique\"]) &gt; 2:\n                        iterable_operations_work[\"nunique\"] = [\n                            e\n                            for e in iterable_operations[\"nunique\"]\n                            if e not in [self.var_individu, self.var_entreprise]\n                        ]\n                    else:\n                        del iterable_operations_work[\"nunique\"]\n                # Comptage des individus\n                elif self.var_individu in iterable_operations[\"nunique\"]:\n                    if len(iterable_operations[\"nunique\"]) &gt; 1:\n                        iterable_operations_work[\"nunique\"] = [\n                            e\n                            for e in iterable_operations[\"nunique\"]\n                            if e != self.var_individu\n                        ]\n                    else:\n                        del iterable_operations_work[\"nunique\"]\n                # Comptage des entreprises\n                elif self.var_entreprise in iterable_operations[\"nunique\"]:\n                    if len(iterable_operations[\"nunique\"]) &gt; 1:\n                        iterable_operations_work[\"nunique\"] = [\n                            e\n                            for e in iterable_operations[\"nunique\"]\n                            if e != self.var_entreprise\n                        ]\n                    else:\n                        del iterable_operations_work[\"nunique\"]\n\n            # Test du besoin d'ajout d'une op\u00e9ration de contr\u00f4le du secret statistique secondaire\n            if any(x in iterable_operations.keys() for x in [\"sum\", \"mean\"]):\n                # Extraction des op\u00e9rations concern\u00e9es\n                list_operation_max_sum_effectif = np.intersect1d(\n                    [\"sum\", \"mean\"],\n                    [\n                        e[0] if isinstance(e, tuple) else e\n                        for e in iterable_operations.keys()\n                    ],\n                ).tolist()\n                # Extraction des variables concern\u00e9es par ces op\u00e9rations\n                list_var_of_interest_max_sum_effectif = np.unique(\n                    np.concatenate(\n                        [\n                            iterable_operations[operation]\n                            for operation in list_operation_max_sum_effectif\n                        ]\n                    )\n                ).tolist()\n                # Suppression des variables \u00e9ventuellement d\u00e9j\u00e0 comprises dans l'op\u00e9rateur\n                if \"max_sum_effectif\" in iterable_operations.keys():\n                    iterable_operations_work[\"max_sum_effectif\"] = np.unique(\n                        iterable_operations[\"max_sum_effectif\"]\n                        + list_var_of_interest_max_sum_effectif\n                    ).tolist()\n                else:\n                    iterable_operations_work[\"max_sum_effectif\"] = (\n                        list_var_of_interest_max_sum_effectif\n                    )\n\n        elif isinstance(iterable_operations, list):\n            # Test du besoin d'ajout d'une op\u00e9ration de contr\u00f4le du secret statistique primaire\n            if (\n                (\"nunique\" in iterable_operations)\n                | (self.var_individu in self.list_var_of_interest)\n                | (self.var_entreprise in self.list_var_of_interest)\n            ):\n                # Actualisation des valeurs des variables\n                if (self.var_individu is not None) &amp; (self.var_entreprise is not None):\n                    list_var_of_interest_work = np.unique(\n                        self.list_var_of_interest\n                        + [self.var_individu, self.var_entreprise]\n                    ).tolist()\n                elif self.var_individu is not None:\n                    list_var_of_interest_work = np.unique(\n                        self.list_var_of_interest + [self.var_individu]\n                    ).tolist()\n                elif self.var_entreprise is not None:\n                    list_var_of_interest_work = np.unique(\n                        self.list_var_of_interest + [self.var_entreprise]\n                    ).tolist()\n                iterable_operations_work = np.unique(\n                    iterable_operations_work + [\"nunique\"]\n                ).tolist()\n            elif \"count_effectif\" not in iterable_operations:\n                iterable_operations_work.append(\"count_effectif\")\n\n            # Test du besoin d'ajout d'une op\u00e9ration de contr\u00f4le du secret statistique secondaire\n            list_var_of_interest_max_sum_effectif = deepcopy(self.list_var_of_interest)\n            # Pour les effectifs\n            if any(x in iterable_operations for x in [\"sum\", \"mean\"]) &amp; (\n                \"max_sum_effectif\" not in iterable_operations\n            ):\n                iterable_operations_work.append(\"max_sum_effectif\")\n        else:\n            raise ValueError(\"Unsupported type for iterable_operations\")\n\n        return (\n            iterable_operations_work,\n            list_var_of_interest_work,\n            list_var_of_interest_max_sum_effectif,\n        )\n\n    # Fonction liminaire des colonnes \u00e0 ajouter et supprimer au jeu de donn\u00e9es de statsitiques descriptives r\u00e9sultat\n    def _get_drop_add_columns(\n        self, data: pd.DataFrame, iterable_operations: Union[dict, list]\n    ) -&gt; Tuple[List[str], List[str]]:\n        \"\"\"\n        Determine columns to be added and dropped in a descriptive statistics dataset.\n\n        This function determines which columns should be added or dropped from a descriptive statistics dataset based on\n        the provided parameters. The goal is to maintain statistical secrecy and ensure data integrity.\n\n        Parameters:\n            data (DataFrame): The dataset containing descriptive statistics.\n            iterable_operations (dict or list): The list of statistical operations to be applied.\n\n        Returns:\n            Tuple[List[str], List[str]]: A tuple containing the following:\n                - list_var_drop (list): A list of columns to be dropped from the dataset.\n                - list_var_data_secret_stat_add (list): A list of columns to be added to the dataset.\n        \"\"\"\n        if isinstance(iterable_operations, dict):\n            if \"nunique\" not in iterable_operations.keys():\n                if \"count_effectif\" in iterable_operations.keys():\n                    list_var_drop = [\n                        col\n                        for col in data.columns\n                        if (\"_secret_stat\" in col) | (\"_max/sum\" in col)\n                    ]\n                    list_var_data_secret_stat_add = []\n                    if self.var_individu is not None:\n                        list_var_data_secret_stat_add.append(\n                            self.var_individu + \"_nunique\"\n                        )\n                    if self.var_entreprise is not None:\n                        list_var_data_secret_stat_add.append(\n                            self.var_entreprise + \"_nunique\"\n                        )\n                else:\n                    list_var_drop = [\n                        col\n                        for col in data.columns\n                        if (\"_secret_stat\" in col) | (\"_max/sum\" in col)\n                    ]\n                    if self.var_individu is not None:\n                        list_var_drop.append(self.var_individu + \"_nunique\")\n                    if self.var_entreprise is not None:\n                        list_var_drop.append(self.var_entreprise + \"_nunique\")\n                    list_var_data_secret_stat_add = []\n            elif (self.var_individu in iterable_operations[\"nunique\"]) | (\n                self.var_entreprise in iterable_operations[\"nunique\"]\n            ):\n                list_var_drop = [\n                    col\n                    for col in data.columns\n                    if (\"_secret_stat\" in col) | (\"_max/sum\" in col)\n                ]\n                if self.var_individu not in iterable_operations[\"nunique\"]:\n                    if self.var_individu is not None:\n                        list_var_drop.append(self.var_individu + \"_nunique\")\n                    list_var_data_secret_stat_add = [self.var_entreprise + \"_nunique\"]\n                elif self.var_entreprise not in iterable_operations[\"nunique\"]:\n                    if self.var_entreprise is not None:\n                        list_var_drop.append(self.var_entreprise + \"_nunique\")\n                    list_var_data_secret_stat_add = [self.var_individu + \"_nunique\"]\n                else:\n                    list_var_data_secret_stat_add = [\n                        self.var_individu + \"_nunique\",\n                        self.var_entreprise + \"_nunique\",\n                    ]\n            else:\n                if (self.var_individu is not None) &amp; (self.var_entreprise is not None):\n                    list_var_drop = [\n                        col\n                        for col in data.columns\n                        if (\"_secret_stat\" in col)\n                        | (\"_max/sum\" in col)\n                        | (self.var_individu + \"_nunique\" in col)\n                        | (self.var_entreprise + \"_nunique\" in col)\n                    ]\n                elif self.var_individu is not None:\n                    list_var_drop = [\n                        col\n                        for col in data.columns\n                        if (\"_secret_stat\" in col)\n                        | (\"_max/sum\" in col)\n                        | (self.var_individu + \"_nunique\" in col)\n                    ]\n                elif self.var_entreprise is not None:\n                    list_var_drop = [\n                        col\n                        for col in data.columns\n                        if (\"_secret_stat\" in col)\n                        | (\"_max/sum\" in col)\n                        | (self.var_entreprise + \"_nunique\" in col)\n                    ]\n                else:\n                    list_var_drop = [\n                        col\n                        for col in data.columns\n                        if (\"_secret_stat\" in col) | (\"_max/sum\" in col)\n                    ]\n                list_var_data_secret_stat_add = []\n\n        elif isinstance(iterable_operations, list):\n            if (\"count_effectif\" in iterable_operations) | (\n                (\"nunique\" in iterable_operations)\n                &amp; (\n                    (self.var_individu in self.list_var_of_interest)\n                    | (self.var_entreprise in self.list_var_of_interest)\n                )\n            ):\n                if (\n                    (\"nunique\" in iterable_operations)\n                    &amp; (self.var_individu in self.list_var_of_interest)\n                    &amp; (self.var_entreprise in self.list_var_of_interest)\n                ):\n                    list_var_drop = [\n                        col\n                        for col in data.columns\n                        if (\"_secret_stat\" in col) | (\"_max/sum\" in col)\n                    ]\n                    list_var_data_secret_stat_add = [\n                        self.var_individu + \"_nunique\",\n                        self.var_entreprise + \"_nunique\",\n                    ]\n                elif (\"nunique\" in iterable_operations) &amp; (\n                    self.var_individu in self.list_var_of_interest\n                ):\n                    if self.var_entreprise is not None:\n                        list_var_drop = [\n                            col\n                            for col in data.columns\n                            if (\"_secret_stat\" in col)\n                            | (\"_max/sum\" in col)\n                            | (self.var_entreprise + \"_nunique\" in col)\n                        ]\n                    else:\n                        list_var_drop = [\n                            col\n                            for col in data.columns\n                            if (\"_secret_stat\" in col) | (\"_max/sum\" in col)\n                        ]\n                    list_var_data_secret_stat_add = [self.var_individu + \"_nunique\"]\n                elif (\"nunique\" in iterable_operations) &amp; (\n                    self.var_entreprise in self.list_var_of_interest\n                ):\n                    if self.var_individu is not None:\n                        list_var_drop = [\n                            col\n                            for col in data.columns\n                            if (\"_secret_stat\" in col)\n                            | (\"_max/sum\" in col)\n                            | (self.var_individu + \"_nunique\" in col)\n                        ]\n                    else:\n                        list_var_drop = [\n                            col\n                            for col in data.columns\n                            if (\"_secret_stat\" in col) | (\"_max/sum\" in col)\n                        ]\n                    list_var_data_secret_stat_add = [self.var_entreprise + \"_nunique\"]\n                # On est dans le cas o\u00f9 l'op\u00e9ration est count_effectif\n                else:\n                    list_var_drop = [\n                        col\n                        for col in data.columns\n                        if (\"_secret_stat\" in col) | (\"_max/sum\" in col)\n                    ]\n                    if (self.var_individu is not None) &amp; (\n                        self.var_entreprise is not None\n                    ):\n                        list_var_data_secret_stat_add = [\n                            self.var_individu + \"_nunique\",\n                            self.var_entreprise + \"_nunique\",\n                        ]\n                    elif self.var_individu is not None:\n                        list_var_data_secret_stat_add = [self.var_individu + \"_nunique\"]\n                    elif self.var_entreprise is not None:\n                        list_var_data_secret_stat_add = [\n                            self.var_entreprise + \"_nunique\"\n                        ]\n                    else:\n                        list_var_data_secret_stat_add = []\n            else:\n                list_var_data_secret_stat_add = []\n                if (self.var_individu is not None) &amp; (self.var_entreprise is not None):\n                    list_var_drop = [\n                        col\n                        for col in data.columns\n                        if (\"_secret_stat\" in col)\n                        | (\"_max/sum\" in col)\n                        | (self.var_individu + \"_nunique\" in col)\n                        | (self.var_entreprise + \"_nunique\" in col)\n                    ]\n                elif self.var_individu is not None:\n                    list_var_drop = [\n                        col\n                        for col in data.columns\n                        if (\"_secret_stat\" in col)\n                        | (\"_max/sum\" in col)\n                        | (self.var_individu + \"_nunique\" in col)\n                    ]\n                elif self.var_entreprise is not None:\n                    list_var_drop = [\n                        col\n                        for col in data.columns\n                        if (\"_secret_stat\" in col)\n                        | (\"_max/sum\" in col)\n                        | (self.var_entreprise + \"_nunique\" in col)\n                    ]\n                else:\n                    list_var_drop = [\n                        col\n                        for col in data.columns\n                        if (\"_secret_stat\" in col) | (\"_max/sum\" in col)\n                    ]\n\n        return list_var_drop, list_var_data_secret_stat_add\n\n    # Fonction d'estimation et de conttrole du secret statistique\n    def estimate_secret_stat(\n        self,\n        iterable_operations: Union[dict, list],\n        include_total: Optional[bool] = True,\n        drop: Optional[bool] = True,\n        fill_value: Optional[Union[int, float, str]] = np.nan,\n        nest: Optional[bool] = False,\n    ) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n        \"\"\"\n        Estimate and control secondary statistical secrecy for a given dataset.\n\n        This function estimates secondary statistical secrecy for a dataset and controls it based on specified parameters.\n        It calculates descriptive statistics, checks for primary and secondary secrecy violations, and separates variables\n        to be added or removed in the resulting datasets.\n\n        Parameters:\n            iterable_operations (dict or list): The operations to perform on the variables of interest.\n            include_total (bool, optional): Include total statistics in the result. Defaults to True.\n            drop (bool, optional): Drop non-compliant rows from the result. Defaults to True.\n            fill_value (float or np.nan, optional): The value to fill non-compliant rows when 'drop' is False. Defaults to np.nan.\n            nest (bool, optional): Nest the index in the result. Defaults to False.\n\n        Returns:\n            Tuple[DataFrame, DataFrame]: A tuple containing the following:\n                - data_stat_des (DataFrame): The dataset with secondary statistical secrecy estimated and controlled.\n                - data_secret_stat (DataFrame): The dataset containing the secondary statistical secrecy information.\n        \"\"\"\n        # Modification des op\u00e9rations en fonction des secrets \u00e0 estimer\n        (\n            iterable_operations_work,\n            list_var_of_interest_work,\n            list_var_of_interest_max_sum_effectif,\n        ) = self._clean_operations(iterable_operations)\n\n        # Statistiques descriptives\n        # It\u00e9rations\n        if include_total:\n            data_stat_des = self.iterate_with_total(\n                iterable_operations=iterable_operations_work\n            )  # .reset_index()\n        else:\n            data_stat_des = self.iterate_without_total(\n                iterable_operations=iterable_operations_work\n            )  # .reset_index()\n\n        # Contr\u00f4le du secret statistique primaire\n        data_stat_des = self.control_primary_statistic_secret(\n            data=data_stat_des,\n            iterable_operations=iterable_operations,\n            list_var_of_interest_max_sum_effectif=list_var_of_interest_max_sum_effectif,\n        )\n\n        # Enum\u00e9ration des variables contenant du secret statistique\n        list_var_secret_stat = [\n            col for col in data_stat_des.columns if \"_secret_stat\" in col\n        ]\n\n        # S\u00e9paration des variables \u00e0 supprimer et ajouter aux jeux de donn\u00e9es r\u00e9sultats\n        list_var_drop, list_var_data_secret_stat_add = self._get_drop_add_columns(\n            data=data_stat_des, iterable_operations=iterable_operations\n        )\n\n        # Cr\u00e9ation d'une colonne secret stat\n        data_stat_des[\"secret_stat_primary\"] = data_stat_des[list_var_secret_stat].all(\n            axis=1\n        )\n\n        # V\u00e9rification du secret statistique secondaire\n        if include_total &amp; (len(self.list_var_groupby) &gt;= 1):\n            data_stat_des = self.control_ss2(\n                data=data_stat_des, var_ss_primary=\"secret_stat_primary\"\n            )\n        else:\n            data_stat_des[\"secret_stat_secondary\"] = True\n            data_stat_des[\"secret_stat\"] = data_stat_des[\"secret_stat_primary\"].copy()\n\n        # Initialisation du jeu de donn\u00e9es de secret stat\n        data_secret_stat = data_stat_des[\n            list_var_data_secret_stat_add\n            + list_var_drop\n            + [\"secret_stat_primary\", \"secret_stat_secondary\", \"secret_stat\"]\n        ].copy()\n\n        # Restriction dans les deux jeux de donn\u00e9es aux lignes respectant le secret statistique\n        if drop is True:\n            data_stat_des = data_stat_des.loc[data_stat_des[\"secret_stat\"]]\n            data_secret_stat = data_secret_stat.loc[data_secret_stat[\"secret_stat\"]]\n        else:\n            data_stat_des.loc[~data_stat_des[\"secret_stat\"]] = fill_value\n\n        # Suppression des colonnes de secret stat du jeu de donn\u00e9es principal\n        data_stat_des.drop(\n            list_var_drop\n            + [\"secret_stat_primary\", \"secret_stat_secondary\", \"secret_stat\"],\n            axis=1,\n            inplace=True,\n        )\n\n        # Nesting des index\n        if nest:\n            data_stat_des = nest_groupby(\n                data_stat_des=data_stat_des.reset_index().rename(\n                    {\"index\": self.list_var_groupby[0]}, axis=1\n                ),\n                list_var_groupby=self.list_var_groupby,\n                modality=\"Total\",\n            ).set_index(self.list_var_groupby)\n            data_secret_stat = nest_groupby(\n                data_stat_des=data_secret_stat.reset_index().rename(\n                    {\"index\": self.list_var_groupby[0]}, axis=1\n                ),\n                list_var_groupby=self.list_var_groupby,\n                modality=\"Total\",\n            ).set_index(self.list_var_groupby)\n\n        return data_stat_des, data_secret_stat\n</code></pre>"},{"location":"api/SecretStatEstimator/#igf_toolbox.stats_des.control_secret_stat.SecretStatEstimator.__init__","title":"<code>__init__(data_source, list_var_groupby, list_var_of_interest, var_individu=None, var_entreprise=None, var_weights=None, threshold_secret_stat_effectif_individu=None, threshold_secret_stat_effectif_entreprise=None, threshold_secret_stat_contrib_individu=None, threshold_secret_stat_contrib_entreprise=None, strategy='total')</code>","text":"<p>Initialize the SecretStatEstimator class.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>DataFrame</code> <p>The dataset to estimate and control secrecy.</p> required <code>list_var_groupby</code> <code>List[str]</code> <p>The list of variables to group the data by.</p> required <code>list_var_of_interest</code> <code>List[str]</code> <p>The list of variables of interest for statistics.</p> required <code>var_individu</code> <code>Optional[Union[str, None]]</code> <p>The variable representing individual-level data.</p> <code>None</code> <code>var_entreprise</code> <code>Optional[Union[str, None]]</code> <p>The variable representing enterprise-level data.</p> <code>None</code> <code>var_weights</code> <code>Optional[Union[str, None]]</code> <p>The variable for weighting.</p> <code>None</code> <code>threshold_secret_stat_effectif_individu</code> <code>Optional[Union[int, None]]</code> <p>The threshold for individual secrecy control.</p> <code>None</code> <code>threshold_secret_stat_effectif_entreprise</code> <code>Optional[Union[int, None]]</code> <p>The threshold for enterprise secrecy control.</p> <code>None</code> <code>threshold_secret_stat_contrib_individu</code> <code>Optional[Union[float, None]]</code> <p>The threshold for secondary individual secrecy control.</p> <code>None</code> <code>threshold_secret_stat_contrib_entreprise</code> <code>Optional[Union[float, None]]</code> <p>The threshold for secondary enterprise secrecy control.</p> <code>None</code> <code>strategy</code> <code>Optional[str]</code> <p>The control strategy ('total' or 'min').</p> <code>'total'</code> Source code in <code>igf_toolbox/stats_des/control_secret_stat.py</code> <pre><code>def __init__(\n    self,\n    data_source: pd.DataFrame,\n    list_var_groupby: List[str],\n    list_var_of_interest: List[str],\n    var_individu: Optional[Union[str, None]] = None,\n    var_entreprise: Optional[Union[str, None]] = None,\n    var_weights: Optional[Union[str, None]] = None,\n    threshold_secret_stat_effectif_individu: Optional[Union[int, None]] = None,\n    threshold_secret_stat_effectif_entreprise: Optional[Union[int, None]] = None,\n    threshold_secret_stat_contrib_individu: Optional[Union[float, None]] = None,\n    threshold_secret_stat_contrib_entreprise: Optional[Union[float, None]] = None,\n    strategy: Optional[str] = \"total\",\n) -&gt; None:\n    \"\"\"\n    Initialize the SecretStatEstimator class.\n\n    Parameters:\n        data_source (DataFrame): The dataset to estimate and control secrecy.\n        list_var_groupby (List[str]): The list of variables to group the data by.\n        list_var_of_interest (List[str]): The list of variables of interest for statistics.\n        var_individu (Optional[Union[str, None]]): The variable representing individual-level data.\n        var_entreprise (Optional[Union[str, None]]): The variable representing enterprise-level data.\n        var_weights (Optional[Union[str, None]]): The variable for weighting.\n        threshold_secret_stat_effectif_individu (Optional[Union[int, None]]): The threshold for individual secrecy control.\n        threshold_secret_stat_effectif_entreprise (Optional[Union[int, None]]): The threshold for enterprise secrecy control.\n        threshold_secret_stat_contrib_individu (Optional[Union[float, None]]): The threshold for secondary individual secrecy control.\n        threshold_secret_stat_contrib_entreprise (Optional[Union[float, None]]): The threshold for secondary enterprise secrecy control.\n        strategy (Optional[str]): The control strategy ('total' or 'min').\n    \"\"\"\n    # Initialisation de la classe de statistiques descriptives\n    StatDesGroupBy.__init__(\n        self,\n        data_source=data_source,\n        list_var_groupby=list_var_groupby,\n        list_var_of_interest=list_var_of_interest,\n        var_individu=var_individu,\n        var_entreprise=var_entreprise,\n        var_weights=var_weights,\n        dropna=True,\n    )\n    # Initialisation des classes de contr\u00f4le du secret statistique\n    PrimarySecretStatController.__init__(\n        self,\n        var_individu=var_individu,\n        var_entreprise=var_entreprise,\n        threshold_secret_stat_effectif_individu=threshold_secret_stat_effectif_individu,\n        threshold_secret_stat_effectif_entreprise=threshold_secret_stat_effectif_entreprise,\n        threshold_secret_stat_contrib_individu=threshold_secret_stat_contrib_individu,\n        threshold_secret_stat_contrib_entreprise=threshold_secret_stat_contrib_entreprise,\n    )\n    SecondarySecretStatController.__init__(\n        self,\n        list_var_groupby=list_var_groupby,\n        var_individu=var_individu,\n        var_entreprise=var_entreprise,\n        strategy=strategy,\n    )\n</code></pre>"},{"location":"api/SecretStatEstimator/#igf_toolbox.stats_des.control_secret_stat.SecretStatEstimator.estimate_secret_stat","title":"<code>estimate_secret_stat(iterable_operations, include_total=True, drop=True, fill_value=np.nan, nest=False)</code>","text":"<p>Estimate and control secondary statistical secrecy for a given dataset.</p> <p>This function estimates secondary statistical secrecy for a dataset and controls it based on specified parameters. It calculates descriptive statistics, checks for primary and secondary secrecy violations, and separates variables to be added or removed in the resulting datasets.</p> <p>Parameters:</p> Name Type Description Default <code>iterable_operations</code> <code>dict or list</code> <p>The operations to perform on the variables of interest.</p> required <code>include_total</code> <code>bool</code> <p>Include total statistics in the result. Defaults to True.</p> <code>True</code> <code>drop</code> <code>bool</code> <p>Drop non-compliant rows from the result. Defaults to True.</p> <code>True</code> <code>fill_value</code> <code>float or nan</code> <p>The value to fill non-compliant rows when 'drop' is False. Defaults to np.nan.</p> <code>nan</code> <code>nest</code> <code>bool</code> <p>Nest the index in the result. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tuple[DataFrame, DataFrame]</code> <p>Tuple[DataFrame, DataFrame]: A tuple containing the following: - data_stat_des (DataFrame): The dataset with secondary statistical secrecy estimated and controlled. - data_secret_stat (DataFrame): The dataset containing the secondary statistical secrecy information.</p> Source code in <code>igf_toolbox/stats_des/control_secret_stat.py</code> <pre><code>def estimate_secret_stat(\n    self,\n    iterable_operations: Union[dict, list],\n    include_total: Optional[bool] = True,\n    drop: Optional[bool] = True,\n    fill_value: Optional[Union[int, float, str]] = np.nan,\n    nest: Optional[bool] = False,\n) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"\n    Estimate and control secondary statistical secrecy for a given dataset.\n\n    This function estimates secondary statistical secrecy for a dataset and controls it based on specified parameters.\n    It calculates descriptive statistics, checks for primary and secondary secrecy violations, and separates variables\n    to be added or removed in the resulting datasets.\n\n    Parameters:\n        iterable_operations (dict or list): The operations to perform on the variables of interest.\n        include_total (bool, optional): Include total statistics in the result. Defaults to True.\n        drop (bool, optional): Drop non-compliant rows from the result. Defaults to True.\n        fill_value (float or np.nan, optional): The value to fill non-compliant rows when 'drop' is False. Defaults to np.nan.\n        nest (bool, optional): Nest the index in the result. Defaults to False.\n\n    Returns:\n        Tuple[DataFrame, DataFrame]: A tuple containing the following:\n            - data_stat_des (DataFrame): The dataset with secondary statistical secrecy estimated and controlled.\n            - data_secret_stat (DataFrame): The dataset containing the secondary statistical secrecy information.\n    \"\"\"\n    # Modification des op\u00e9rations en fonction des secrets \u00e0 estimer\n    (\n        iterable_operations_work,\n        list_var_of_interest_work,\n        list_var_of_interest_max_sum_effectif,\n    ) = self._clean_operations(iterable_operations)\n\n    # Statistiques descriptives\n    # It\u00e9rations\n    if include_total:\n        data_stat_des = self.iterate_with_total(\n            iterable_operations=iterable_operations_work\n        )  # .reset_index()\n    else:\n        data_stat_des = self.iterate_without_total(\n            iterable_operations=iterable_operations_work\n        )  # .reset_index()\n\n    # Contr\u00f4le du secret statistique primaire\n    data_stat_des = self.control_primary_statistic_secret(\n        data=data_stat_des,\n        iterable_operations=iterable_operations,\n        list_var_of_interest_max_sum_effectif=list_var_of_interest_max_sum_effectif,\n    )\n\n    # Enum\u00e9ration des variables contenant du secret statistique\n    list_var_secret_stat = [\n        col for col in data_stat_des.columns if \"_secret_stat\" in col\n    ]\n\n    # S\u00e9paration des variables \u00e0 supprimer et ajouter aux jeux de donn\u00e9es r\u00e9sultats\n    list_var_drop, list_var_data_secret_stat_add = self._get_drop_add_columns(\n        data=data_stat_des, iterable_operations=iterable_operations\n    )\n\n    # Cr\u00e9ation d'une colonne secret stat\n    data_stat_des[\"secret_stat_primary\"] = data_stat_des[list_var_secret_stat].all(\n        axis=1\n    )\n\n    # V\u00e9rification du secret statistique secondaire\n    if include_total &amp; (len(self.list_var_groupby) &gt;= 1):\n        data_stat_des = self.control_ss2(\n            data=data_stat_des, var_ss_primary=\"secret_stat_primary\"\n        )\n    else:\n        data_stat_des[\"secret_stat_secondary\"] = True\n        data_stat_des[\"secret_stat\"] = data_stat_des[\"secret_stat_primary\"].copy()\n\n    # Initialisation du jeu de donn\u00e9es de secret stat\n    data_secret_stat = data_stat_des[\n        list_var_data_secret_stat_add\n        + list_var_drop\n        + [\"secret_stat_primary\", \"secret_stat_secondary\", \"secret_stat\"]\n    ].copy()\n\n    # Restriction dans les deux jeux de donn\u00e9es aux lignes respectant le secret statistique\n    if drop is True:\n        data_stat_des = data_stat_des.loc[data_stat_des[\"secret_stat\"]]\n        data_secret_stat = data_secret_stat.loc[data_secret_stat[\"secret_stat\"]]\n    else:\n        data_stat_des.loc[~data_stat_des[\"secret_stat\"]] = fill_value\n\n    # Suppression des colonnes de secret stat du jeu de donn\u00e9es principal\n    data_stat_des.drop(\n        list_var_drop\n        + [\"secret_stat_primary\", \"secret_stat_secondary\", \"secret_stat\"],\n        axis=1,\n        inplace=True,\n    )\n\n    # Nesting des index\n    if nest:\n        data_stat_des = nest_groupby(\n            data_stat_des=data_stat_des.reset_index().rename(\n                {\"index\": self.list_var_groupby[0]}, axis=1\n            ),\n            list_var_groupby=self.list_var_groupby,\n            modality=\"Total\",\n        ).set_index(self.list_var_groupby)\n        data_secret_stat = nest_groupby(\n            data_stat_des=data_secret_stat.reset_index().rename(\n                {\"index\": self.list_var_groupby[0]}, axis=1\n            ),\n            list_var_groupby=self.list_var_groupby,\n            modality=\"Total\",\n        ).set_index(self.list_var_groupby)\n\n    return data_stat_des, data_secret_stat\n</code></pre>"},{"location":"api/StandardScalerTransformer/","title":"StandardScalerTransformer","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>DataFrame Wrapper for the StandardScaler from sklearn.preprocessing.</p> <p>This class provides a simple wrapper around the StandardScaler from scikit-learn to work with pandas DataFrames. It standardizes features by removing the mean and scaling to unit variance, while retaining DataFrame structure, column names, and index.</p> <p>Attributes: - StandardScaler (StandardScaler): Instance of scikit-learn's StandardScaler.</p> <p>Methods: - fit(X, y=None): Compute the mean and standard deviation to be used for later scaling. - transform(X, y=None): Perform standard scaling (mean centering and variance normalization) on the data. - inverse_transform(X, y=None): Scale the data back to its original state.</p> <p>Example:</p> <p>df_scaler = StandardScalerTransformer() df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [10, 20, 30, 40, 50]}) df_scaled = df_scaler.fit_transform(df) df_original = df_scaler.inverse_transform(df_scaled)</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>class StandardScalerTransformer(TransformerMixin, BaseEstimator):\n    \"\"\"\n    DataFrame Wrapper for the StandardScaler from sklearn.preprocessing.\n\n    This class provides a simple wrapper around the StandardScaler from scikit-learn to work with\n    pandas DataFrames. It standardizes features by removing the mean and scaling to unit variance,\n    while retaining DataFrame structure, column names, and index.\n\n    Attributes:\n    - StandardScaler (StandardScaler): Instance of scikit-learn's StandardScaler.\n\n    Methods:\n    - fit(X, y=None): Compute the mean and standard deviation to be used for later scaling.\n    - transform(X, y=None): Perform standard scaling (mean centering and variance normalization) on the data.\n    - inverse_transform(X, y=None): Scale the data back to its original state.\n\n    Example:\n    &gt;&gt;&gt; df_scaler = StandardScalerTransformer()\n    &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 2, 3, 4, 5], 'B': [10, 20, 30, 40, 50]})\n    &gt;&gt;&gt; df_scaled = df_scaler.fit_transform(df)\n    &gt;&gt;&gt; df_original = df_scaler.inverse_transform(df_scaled)\n    \"\"\"\n\n    def __init__(self) -&gt; None:\n        \"\"\"\n        Initialize the StandardScalerTransformer.\n\n        This method creates an instance of the StandardScaler from scikit-learn.\n        \"\"\"\n        # Initialisation du StandardScaler\n        self.StandardScaler = StandardScaler()\n\n    def fit(self, X, y=None) -&gt; None:\n        \"\"\"\n        Compute the mean and standard deviation for standard scaling.\n\n        This method fits the internal StandardScaler to the provided data.\n\n        Parameters:\n        - X (pd.DataFrame): The input data to fit the scaler.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - self: The fitted transformer instance.\n        \"\"\"\n        # Entrainement du StandardScaler\n        self.StandardScaler.fit(X=X, y=y)\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Perform standard scaling on the provided data.\n\n        This method uses the internal StandardScaler to perform the scaling and then\n        returns the result as a pandas DataFrame with the same column names and index.\n\n        Parameters:\n        - X (pd.DataFrame): The input data to scale.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - pd.DataFrame: The scaled data.\n        \"\"\"\n        # Transformation du jeu de donn\u00e9es en concervant les index\n        X_transformed = pd.DataFrame(\n            data=self.StandardScaler.transform(X=X), index=X.index, columns=X.columns\n        )\n        return X_transformed\n\n    def inverse_transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Scale the data back to its original state.\n\n        This method uses the internal StandardScaler to perform the inverse scaling and\n        then returns the result as a pandas DataFrame with the same column names and index.\n\n        Parameters:\n        - X (pd.DataFrame): The scaled input data to inverse scale.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - pd.DataFrame: The original unscaled data.\n        \"\"\"\n        # Inversion de la transformation en conservant les index\n        X_inverse_transformed = pd.DataFrame(\n            data=self.StandardScaler.inverse_transform(X=X),\n            index=X.index,\n            columns=X.columns,\n        )\n        return X_inverse_transformed\n</code></pre>"},{"location":"api/StandardScalerTransformer/#igf_toolbox.preprocessing.transformers.StandardScalerTransformer.__init__","title":"<code>__init__()</code>","text":"<p>Initialize the StandardScalerTransformer.</p> <p>This method creates an instance of the StandardScaler from scikit-learn.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"\n    Initialize the StandardScalerTransformer.\n\n    This method creates an instance of the StandardScaler from scikit-learn.\n    \"\"\"\n    # Initialisation du StandardScaler\n    self.StandardScaler = StandardScaler()\n</code></pre>"},{"location":"api/StandardScalerTransformer/#igf_toolbox.preprocessing.transformers.StandardScalerTransformer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Compute the mean and standard deviation for standard scaling.</p> <p>This method fits the internal StandardScaler to the provided data.</p> <p>Parameters: - X (pd.DataFrame): The input data to fit the scaler. - y (ignored): This parameter is ignored.</p> <p>Returns: - self: The fitted transformer instance.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def fit(self, X, y=None) -&gt; None:\n    \"\"\"\n    Compute the mean and standard deviation for standard scaling.\n\n    This method fits the internal StandardScaler to the provided data.\n\n    Parameters:\n    - X (pd.DataFrame): The input data to fit the scaler.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - self: The fitted transformer instance.\n    \"\"\"\n    # Entrainement du StandardScaler\n    self.StandardScaler.fit(X=X, y=y)\n    return self\n</code></pre>"},{"location":"api/StandardScalerTransformer/#igf_toolbox.preprocessing.transformers.StandardScalerTransformer.inverse_transform","title":"<code>inverse_transform(X, y=None)</code>","text":"<p>Scale the data back to its original state.</p> <p>This method uses the internal StandardScaler to perform the inverse scaling and then returns the result as a pandas DataFrame with the same column names and index.</p> <p>Parameters: - X (pd.DataFrame): The scaled input data to inverse scale. - y (ignored): This parameter is ignored.</p> <p>Returns: - pd.DataFrame: The original unscaled data.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def inverse_transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Scale the data back to its original state.\n\n    This method uses the internal StandardScaler to perform the inverse scaling and\n    then returns the result as a pandas DataFrame with the same column names and index.\n\n    Parameters:\n    - X (pd.DataFrame): The scaled input data to inverse scale.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - pd.DataFrame: The original unscaled data.\n    \"\"\"\n    # Inversion de la transformation en conservant les index\n    X_inverse_transformed = pd.DataFrame(\n        data=self.StandardScaler.inverse_transform(X=X),\n        index=X.index,\n        columns=X.columns,\n    )\n    return X_inverse_transformed\n</code></pre>"},{"location":"api/StandardScalerTransformer/#igf_toolbox.preprocessing.transformers.StandardScalerTransformer.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Perform standard scaling on the provided data.</p> <p>This method uses the internal StandardScaler to perform the scaling and then returns the result as a pandas DataFrame with the same column names and index.</p> <p>Parameters: - X (pd.DataFrame): The input data to scale. - y (ignored): This parameter is ignored.</p> <p>Returns: - pd.DataFrame: The scaled data.</p> Source code in <code>igf_toolbox/preprocessing/transformers.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform standard scaling on the provided data.\n\n    This method uses the internal StandardScaler to perform the scaling and then\n    returns the result as a pandas DataFrame with the same column names and index.\n\n    Parameters:\n    - X (pd.DataFrame): The input data to scale.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - pd.DataFrame: The scaled data.\n    \"\"\"\n    # Transformation du jeu de donn\u00e9es en concervant les index\n    X_transformed = pd.DataFrame(\n        data=self.StandardScaler.transform(X=X), index=X.index, columns=X.columns\n    )\n    return X_transformed\n</code></pre>"},{"location":"api/StatDesGroupBy/","title":"StatDesGroupBy","text":"<p>               Bases: <code>object</code></p> <p>A class to compute descriptive statistics grouped by specified variables. Compared to the pandas.DataFrame.groupby method it handles weighted operations, and can add totals.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy--attributes","title":"Attributes:","text":"<p>list_var_groupby : list     List of variables to group data by. list_var_of_interest : list     List of variables for which descriptive statistics will be computed. var_individu : str, optional     Variable representing individual data. var_entreprise : str, optional     Variable representing enterprise data. var_weights : str, optional     Variable representing the weights for each data entry. data_source : DataFrame     Data source after initialization and cleaning.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy--methods","title":"Methods:","text":"<p>iterate_with_total(iterable_operations)     Computes descriptive statistics and returns results with subtotals and a grand total. iterate_without_total(iterable_operations)     Computes descriptive statistics without subtotals or a grand total and returns results. add_under_total(iterable_operations)     Generates and returns subtotals for combinations of grouping variables.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>class StatDesGroupBy(object):\n    \"\"\"\n    A class to compute descriptive statistics grouped by specified variables.\n    Compared to the pandas.DataFrame.groupby method it handles weighted operations, and can add totals.\n\n    Attributes:\n    -----------\n    list_var_groupby : list\n        List of variables to group data by.\n    list_var_of_interest : list\n        List of variables for which descriptive statistics will be computed.\n    var_individu : str, optional\n        Variable representing individual data.\n    var_entreprise : str, optional\n        Variable representing enterprise data.\n    var_weights : str, optional\n        Variable representing the weights for each data entry.\n    data_source : DataFrame\n        Data source after initialization and cleaning.\n\n    Methods:\n    --------\n    iterate_with_total(iterable_operations)\n        Computes descriptive statistics and returns results with subtotals and a grand total.\n    iterate_without_total(iterable_operations)\n        Computes descriptive statistics without subtotals or a grand total and returns results.\n    add_under_total(iterable_operations)\n        Generates and returns subtotals for combinations of grouping variables.\n    \"\"\"\n\n    def __init__(\n        self,\n        data_source: pd.DataFrame,\n        list_var_groupby: List[str],\n        list_var_of_interest: List[str],\n        var_individu: Optional[Union[str, None]] = None,\n        var_entreprise: Optional[Union[str, None]] = None,\n        var_weights: Optional[Union[str, None]] = None,\n        dropna: Optional[bool] = False,\n    ) -&gt; None:\n        \"\"\"\n        Initialize the StatDesGroupBy class with data and parameters.\n\n        Parameters:\n        -----------\n        data_source : DataFrame\n            Source data for computing statistics.\n        list_var_groupby : list\n            List of variables to group data by.\n        list_var_of_interest : list\n            List of variables for which descriptive statistics will be computed.\n        var_individu : str, optional\n            Variable representing individual data.\n        var_entreprise : str, optional\n            Variable representing enterprise data.\n        var_weights : str, optional\n            Variable representing the weights for each data entry.\n        \"\"\"\n        # Initialisation des param\u00e8tres\n        self.list_var_groupby = list_var_groupby\n        self.list_var_of_interest = list_var_of_interest\n        self.var_individu = var_individu\n        self.var_entreprise = var_entreprise\n        self.var_weights = var_weights\n\n        # Initialisation de la liste des variables d'int\u00e9r\u00eat\n        if var_weights is not None:\n            list_var_of_interest_weights = list_var_of_interest + [var_weights]\n        else:\n            list_var_of_interest_weights = list_var_of_interest\n\n        # Copie ind\u00e9pendante du jeu de donn\u00e9es\n        if (\n            (var_individu not in list_var_groupby)\n            &amp; (var_individu is not None)\n            &amp; (var_entreprise not in list_var_groupby)\n            &amp; (var_entreprise is not None)\n        ):\n            list_var_keep = np.unique(\n                list_var_groupby\n                + list_var_of_interest_weights\n                + [var_individu, var_entreprise]\n            ).tolist()\n        elif (\n            (var_individu not in list_var_groupby)\n            &amp; (var_individu is not None)\n            &amp; ((var_entreprise in list_var_groupby) | (var_entreprise is None))\n        ):\n            list_var_keep = np.unique(\n                list_var_groupby + list_var_of_interest_weights + [var_individu]\n            ).tolist()\n        elif (\n            ((var_individu in list_var_groupby) | (var_individu is None))\n            &amp; (var_entreprise not in list_var_groupby)\n            &amp; (var_entreprise is not None)\n        ):\n            list_var_keep = np.unique(\n                list_var_groupby + list_var_of_interest_weights + [var_entreprise]\n            ).tolist()\n        else:\n            list_var_keep = np.unique(\n                list_var_groupby + list_var_of_interest_weights\n            ).tolist()\n\n        # Suppression des Nan\n        if dropna:\n            self.data_source = data_source[list_var_keep].copy().dropna(how=\"any\")\n        else:\n            self.data_source = data_source[list_var_keep].copy()\n\n    def iterate_with_total(\n        self, iterable_operations: Union[dict, List[str]]\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Computes descriptive statistics and returns results with subtotals and a grand total.\n\n        Parameters:\n        -----------\n        iterable_operations : iterable\n            Operations or functions to apply to the grouped data.\n\n        Returns:\n        --------\n        DataFrame\n            Descriptive statistics with subtotals and a grand total.\n        \"\"\"\n        # Disjonction de cas suivant la longueur de la liste de groupby\n        if len(self.list_var_groupby) == 0:\n            data_res = self.add_total(iterable_operations=iterable_operations)\n        elif len(self.list_var_groupby) == 1:\n            # It\u00e9ration des statistiques descriptives\n            data_stat_des = self.iterate_operations(\n                iterable_operations=iterable_operations,\n                data=self.data_source,\n                list_var_groupby=self.list_var_groupby,\n            )\n            # It\u00e9ration du total\n            data_total = self.add_total(iterable_operations=iterable_operations)\n            # Concat\u00e9nation des jeux de donn\u00e9es\n            data_res = pd.concat([data_stat_des, data_total], axis=0, join=\"outer\")\n        else:\n            # It\u00e9ration des statistiques descriptives\n            data_stat_des = self.iterate_operations(\n                iterable_operations=iterable_operations,\n                data=self.data_source,\n                list_var_groupby=self.list_var_groupby,\n            )\n            # It\u00e9ration des sous-totaux\n            data_sub_total = self.add_under_total(\n                iterable_operations=iterable_operations\n            )\n            # It\u00e9ration du total\n            data_total = self.add_total(iterable_operations=iterable_operations)\n            # Concat\u00e9nation des jeux de donn\u00e9es\n            data_res = pd.concat(\n                [data_stat_des, data_sub_total, data_total], axis=0, join=\"outer\"\n            )\n\n        # Tri de l'indice\n        data_res = _sort_index_with_total(data_source=data_res)\n\n        return data_res\n\n    def iterate_without_total(\n        self, iterable_operations: Union[dict, List[str]]\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Computes descriptive statistics without subtotals or a grand total and returns results.\n\n        Parameters:\n        -----------\n        iterable_operations : iterable\n            Operations or functions to apply to the grouped data.\n\n        Returns:\n        --------\n        DataFrame\n            Descriptive statistics without subtotals or a grand total.\n        \"\"\"\n        return self.iterate_operations(\n            iterable_operations=iterable_operations,\n            data=self.data_source,\n            list_var_groupby=self.list_var_groupby,\n        ).sort_index()\n\n    def add_under_total(\n        self, iterable_operations: Union[dict, List[str]]\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Generates and returns subtotals for combinations of grouping variables.\n\n        Parameters:\n        -----------\n        iterable_operations : iterable\n            Operations or functions to apply to the grouped data to compute subtotals.\n\n        Returns:\n        --------\n        DataFrame\n            Descriptive statistics with subtotals for combinations of grouping variables.\n        \"\"\"\n\n        # Parcours de toutes les combinaisons possibles de niveaux\n        # Cr\u00e9ation des combinaisons\n        list_combinations = []\n        for i in range(1, len(self.list_var_groupby)):\n            list_combinations += list(\n                combinations(np.arange(len(self.list_var_groupby)), r=i)\n            )\n        # Conversion en liste\n        list_combinations = [list(combination) for combination in list_combinations]\n        # Initialisation de la liste r\u00e9sultat\n        list_sub_total = []\n        for combination in list_combinations:\n            if len(combination) &gt; 0:\n                # Construction de la liste avec les sous-ensembles de variables\n                list_var_sub_groupby = [\n                    self.list_var_groupby[e]\n                    for e in range(len(self.list_var_groupby))\n                    if e not in combination\n                ]\n                # It\u00e9ration des statistiques descriptives\n                data_sub_total = self.iterate_operations(\n                    iterable_operations=iterable_operations,\n                    data=self.data_source,\n                    list_var_groupby=list_var_sub_groupby,\n                )\n                # Modification de l'index\n                sub_total_index_res = []\n                for sub_total_index in data_sub_total.index:\n                    for i_remove in combination:\n                        if isinstance(sub_total_index, tuple):\n                            sub_total_index = (\n                                list(sub_total_index)[:i_remove]\n                                + [\"Total\"]\n                                + list(sub_total_index[i_remove:])\n                            )\n                        elif isinstance(sub_total_index, list):\n                            sub_total_index = (\n                                sub_total_index[:i_remove]\n                                + [\"Total\"]\n                                + sub_total_index[i_remove:]\n                            )\n                        elif i_remove == 0:\n                            sub_total_index = [\"Total\"] + [sub_total_index]\n                        elif i_remove == 1:\n                            sub_total_index = [sub_total_index] + [\"Total\"]\n                        else:\n                            raise ValueError(\"Unknown index\")\n                    sub_total_index_res.append(sub_total_index)\n                data_sub_total.index = pd.MultiIndex.from_tuples(\n                    sub_total_index_res, names=self.list_var_groupby\n                )\n                # Ajout \u00e0 la liste r\u00e9sultat\n                list_sub_total.append(data_sub_total)\n        # Concat\u00e9nation des jeux de donn\u00e9es r\u00e9sultats\n        data_sub_total = pd.concat(list_sub_total, axis=0, join=\"outer\")\n\n        return data_sub_total\n\n    def add_total(self, iterable_operations: Union[dict, List[str]]) -&gt; pd.DataFrame:\n        \"\"\"\n        Perform a series of global aggregate operations on the data and concatenate the results.\n\n        The function goes through the specified operations (either as strings for basic operations or tuples for\n        operations with extra parameters) and applies them on the data_source. The result of each operation is\n        appended to a list. At the end, the results are concatenated and returned as a single DataFrame.\n\n        Parameters:\n        - iterable_operations (dict or list): Operations to be applied on the data. If a dictionary, the keys\n        represent the operation and values represent variables of interest. If a list, it contains either\n        operations as strings or tuples where the first element is the operation and the second is a dictionary\n        of parameters.\n\n        Returns:\n        - pd.DataFrame: A DataFrame containing aggregated results after applying all the operations, indexed by\n        either 'Total' or a MultiIndex version of it based on the length of list_var_groupby.\n\n        Raises:\n        - ValueError: If the provided type for iterable_operations is neither dict nor list.\n\n        Example:\n        ```python\n        # Example usage (assuming it's a method within a class)\n        aggregated_data = StatDesGroupby.add_total({'sum': ['var1', 'var2']})\n        ```\n\n        Notes:\n        - The function uses internal methods of the class, such as sum, mean, median, etc.\n        - The operations are performed without groupby, intended to get overall aggregates.\n        - The 'count_effectif' and 'max_sum_effectif' operations have special handling based on the presence\n        of var_individu and var_entreprise class attributes.\n        \"\"\"\n\n        # Initialisation de la liste r\u00e9sultat\n        list_total = []\n        # Parcours des diff\u00e9rentes op\u00e9rations\n        # Faire avec agg pour limiter le nombre de groupby\n        if isinstance(iterable_operations, dict):\n            for operation, list_var_of_interest_work in iterable_operations.items():\n                if isinstance(operation, str):\n                    if operation == \"sum\":\n                        list_total.append(\n                            self.sum(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif operation == \"mean\":\n                        list_total.append(\n                            self.mean(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif operation == \"median\":\n                        list_total.append(\n                            self.median(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif operation == \"nunique\":\n                        list_total.append(\n                            self.nunique(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                            )\n                        )\n                    elif operation == \"count\":\n                        list_total.append(\n                            self.count(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                            )\n                        )\n                    elif operation == \"any\":\n                        list_total.append(\n                            self.any(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                            )\n                        )\n                    elif operation == \"all\":\n                        list_total.append(\n                            self.all(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                            )\n                        )\n                    elif operation == \"majority\":\n                        list_total.append(\n                            self.majority(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif (\n                        (operation == \"count_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_total.append(\n                            self.nunique(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=[\n                                    self.var_individu,\n                                    self.var_entreprise,\n                                ],\n                            )\n                        )\n                    elif (\n                        (operation == \"count_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is None)\n                    ):\n                        list_total.append(\n                            self.nunique(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=[self.var_individu],\n                            )\n                        )\n                    elif (\n                        (operation == \"count_effectif\")\n                        &amp; (self.var_individu is None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_total.append(\n                            self.nunique(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=[self.var_entreprise],\n                            )\n                        )\n                    elif (\n                        (operation == \"max_sum_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_total.append(\n                            self.max_sum_effectif(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                                var_id=self.var_individu,\n                            )\n                        )\n                        list_total.append(\n                            self.max_sum_effectif(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                                var_id=self.var_entreprise,\n                            )\n                        )\n                    elif (\n                        (operation == \"max_sum_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is None)\n                    ):\n                        list_total.append(\n                            self.max_sum_effectif(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                                var_id=self.var_individu,\n                            )\n                        )\n                    elif (\n                        (operation == \"max_sum_effectif\")\n                        &amp; (self.var_individu is None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_total.append(\n                            self.max_sum_effectif(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                                var_id=self.var_entreprise,\n                            )\n                        )\n                elif isinstance(operation, tuple):\n                    # Le tuple est compos\u00e9 d'un string en premi\u00e8re position et d'un dictionnaire de param\u00e8tres en deuxi\u00e8me\n                    if operation[0] == \"quantile\":\n                        list_total.append(\n                            self.quantile(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                                q=operation[1][\"q\"],\n                            )\n                        )\n                    elif operation[0] == \"prop\":\n                        list_total.append(\n                            self.prop(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                                var_ref=operation[1][\"var_ref\"],\n                            )\n                        )\n                    elif operation[0] == \"inf_threshold\":\n                        list_total.append(\n                            self.inf_seuil(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_threshold=operation[1][\"var_threshold\"],\n                                seuil=operation[1][\"threshold\"],\n                            )\n                        )\n        elif isinstance(iterable_operations, list):\n            for operation in iterable_operations:\n                if isinstance(operation, str):\n                    if operation == \"sum\":\n                        list_total.append(\n                            self.sum(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif operation == \"mean\":\n                        list_total.append(\n                            self.mean(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif operation == \"median\":\n                        list_total.append(\n                            self.median(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif operation == \"nunique\":\n                        list_total.append(\n                            self.nunique(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                            )\n                        )\n                    elif operation == \"count\":\n                        list_total.append(\n                            self.count(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                            )\n                        )\n                    elif operation == \"any\":\n                        list_total.append(\n                            self.any(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                            )\n                        )\n                    elif operation == \"all\":\n                        list_total.append(\n                            self.all(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                            )\n                        )\n                    elif operation == \"majority\":\n                        list_total.append(\n                            self.majority(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif (\n                        (operation == \"count_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_total.append(\n                            self.nunique(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=[\n                                    self.var_individu,\n                                    self.var_entreprise,\n                                ],\n                            )\n                        )\n                    elif (\n                        (operation == \"count_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is None)\n                    ):\n                        list_total.append(\n                            self.nunique(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=[self.var_individu],\n                            )\n                        )\n                    elif (\n                        (operation == \"count_effectif\")\n                        &amp; (self.var_individu is None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_total.append(\n                            self.nunique(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=[self.var_entreprise],\n                            )\n                        )\n                    elif (\n                        (operation == \"max_sum_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_total.append(\n                            self.max_sum_effectif(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                                var_id=self.var_individu,\n                            )\n                        )\n                        list_total.append(\n                            self.max_sum_effectif(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                                var_id=self.var_entreprise,\n                            )\n                        )\n                    elif (\n                        (operation == \"max_sum_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is None)\n                    ):\n                        list_total.append(\n                            self.max_sum_effectif(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                                var_id=self.var_individu,\n                            )\n                        )\n                    elif (\n                        (operation == \"max_sum_effectif\")\n                        &amp; (self.var_individu is None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_total.append(\n                            self.max_sum_effectif(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                                var_id=self.var_entreprise,\n                            )\n                        )\n                elif isinstance(operation, tuple):\n                    # Le tuple est compos\u00e9 d'un string en premi\u00e8re position et d'un dictionnaire de param\u00e8tres en deuxi\u00e8me\n                    if operation[0] == \"quantile\":\n                        list_total.append(\n                            self.quantile(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                                q=operation[1][\"q\"],\n                            )\n                        )\n                    elif operation[0] == \"prop\":\n                        list_total.append(\n                            self.prop(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                                var_ref=operation[1][\"var_ref\"],\n                            )\n                        )\n                    elif operation[0] == \"inf_threshold\":\n                        list_total.append(\n                            self.inf_seuil(\n                                data=self.data_source,\n                                list_var_groupby=None,\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_threshold=operation[1][\"var_threshold\"],\n                                seuil=operation[1][\"threshold\"],\n                            )\n                        )\n        else:\n            raise ValueError(\"Unsupported type for iterable_operations\")\n\n        # Concat\u00e9nation des jeux de donn\u00e9es\n        data_total = pd.concat(list_total, axis=1, join=\"outer\")\n\n        if len(self.list_var_groupby) &gt; 1:\n            data_total.index = pd.MultiIndex.from_tuples(\n                [[\"Total\"] * len(self.list_var_groupby)], names=self.list_var_groupby\n            )\n        else:\n            data_total.index = [\"Total\"]\n\n        return data_total\n\n    def iterate_operations(\n        self,\n        iterable_operations: Union[dict, List[str]],\n        data: pd.DataFrame,\n        list_var_groupby: List[str],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Iteratively perform a set of operations on the data grouped by given variables.\n\n        This function first creates a nomenclature with categorical variables and then, based on the type\n        of operations provided, executes those operations on the dataset. It utilizes the internal methods\n        of the class to which this function belongs, such as sum, mean, median, etc.\n\n        Parameters:\n        - iterable_operations (dict or list): Operations to be applied on the data. If it's a dictionary, the\n        keys represent the operation and values represent the variables of interest. If it's a list, it only\n        contains operations.\n        - data (pd.DataFrame): Input dataset on which the operations need to be applied.\n        - list_var_groupby (list of str): List of variables based on which the data needs to be grouped.\n\n        Returns:\n        - pd.DataFrame: Dataset after applying all the operations, indexed by the list_var_groupby.\n\n        Raises:\n        - ValueError: If the provided type for iterable_operations is neither dict nor list.\n\n        Example:\n        ```python\n        # Example usage (assuming it's a method within a class)\n        result = StatDesGroupby.iterate_operations({'sum': ['var1', 'var2']}, data, ['group_var'])\n        ```\n\n        Notes:\n        - The function merges the resultant data with the data_nomenc to ensure the categorical variables are retained.\n        - The internal operations like sum, mean, etc., are methods of the same class and are not global functions.\n        \"\"\"\n        # Cr\u00e9ation d'une nomenclature avec variables cat\u00e9gorielles\n        data_nomenc = data[list_var_groupby].drop_duplicates().reset_index()\n        # Conversion de la colonne en variable cat\u00e9gorielle\n        data_nomenc[\"index\"] = pd.Categorical(data_nomenc[\"index\"])\n        # Ajout au jeu de donn\u00e9es\n        data_work = pd.merge(\n            left=data,\n            right=data_nomenc,\n            how=\"left\",\n            on=list_var_groupby,\n            validate=\"many_to_one\",\n        ).drop(list_var_groupby, axis=1)\n\n        # Initialisation de la liste r\u00e9sultat\n        list_res = []\n        # Parcours des diff\u00e9rentes op\u00e9rations\n        # Faire avec agg pour limiter le nombre de groupby\n        if isinstance(iterable_operations, dict):\n            for operation, list_var_of_interest_work in iterable_operations.items():\n                if isinstance(operation, str):\n                    if operation == \"sum\":\n                        list_res.append(\n                            self.sum(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif operation == \"mean\":\n                        list_res.append(\n                            self.mean(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif operation == \"median\":\n                        list_res.append(\n                            self.median(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif operation == \"nunique\":\n                        list_res.append(\n                            self.nunique(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                            )\n                        )\n                    elif operation == \"count\":\n                        list_res.append(\n                            self.count(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                            )\n                        )\n                    elif operation == \"any\":\n                        list_res.append(\n                            self.any(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                            )\n                        )\n                    elif operation == \"all\":\n                        list_res.append(\n                            self.all(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                            )\n                        )\n                    elif operation == \"majority\":\n                        list_res.append(\n                            self.majority(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif (\n                        (operation == \"count_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_res.append(\n                            self.nunique(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=[\n                                    self.var_individu,\n                                    self.var_entreprise,\n                                ],\n                            )\n                        )\n                    elif (\n                        (operation == \"count_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is None)\n                    ):\n                        list_res.append(\n                            self.nunique(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=[self.var_individu],\n                            )\n                        )\n                    elif (\n                        (operation == \"count_effectif\")\n                        &amp; (self.var_individu is None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_res.append(\n                            self.nunique(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=[self.var_entreprise],\n                            )\n                        )\n                    elif (\n                        (operation == \"max_sum_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_res.append(\n                            self.max_sum_effectif(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                                var_id=self.var_individu,\n                            )\n                        )\n                        list_res.append(\n                            self.max_sum_effectif(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                                var_id=self.var_entreprise,\n                            )\n                        )\n                    elif (\n                        (operation == \"max_sum_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is None)\n                    ):\n                        list_res.append(\n                            self.max_sum_effectif(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                                var_id=self.var_individu,\n                            )\n                        )\n                    elif (\n                        (operation == \"max_sum_effectif\")\n                        &amp; (self.var_individu is None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_res.append(\n                            self.max_sum_effectif(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                                var_id=self.var_entreprise,\n                            )\n                        )\n                elif isinstance(operation, tuple):\n                    # Le tuple est compos\u00e9 d'un string en premi\u00e8re position et d'un dictionnaire de param\u00e8tres en deuxi\u00e8me\n                    if operation[0] == \"quantile\":\n                        list_res.append(\n                            self.quantile(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                                q=operation[1][\"q\"],\n                            )\n                        )\n                    elif operation[0] == \"prop\":\n                        list_res.append(\n                            self.prop(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_weights=self.var_weights,\n                                var_ref=operation[1][\"var_ref\"],\n                            )\n                        )\n                    elif operation[0] == \"inf_threshold\":\n                        list_res.append(\n                            self.inf_seuil(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=np.intersect1d(\n                                    ar1=self.list_var_of_interest,\n                                    ar2=list_var_of_interest_work,\n                                ).tolist(),\n                                var_threshold=operation[1][\"var_threshold\"],\n                                seuil=operation[1][\"threshold\"],\n                            )\n                        )\n        elif isinstance(iterable_operations, list):\n            for operation in iterable_operations:\n                if isinstance(operation, str):\n                    if operation == \"sum\":\n                        list_res.append(\n                            self.sum(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif operation == \"mean\":\n                        list_res.append(\n                            self.mean(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif operation == \"median\":\n                        list_res.append(\n                            self.median(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif operation == \"nunique\":\n                        list_res.append(\n                            self.nunique(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                            )\n                        )\n                    elif operation == \"count\":\n                        list_res.append(\n                            self.count(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                            )\n                        )\n                    elif operation == \"any\":\n                        list_res.append(\n                            self.any(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                            )\n                        )\n                    elif operation == \"all\":\n                        list_res.append(\n                            self.all(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                            )\n                        )\n                    elif operation == \"majority\":\n                        list_res.append(\n                            self.majority(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                            )\n                        )\n                    elif (\n                        (operation == \"count_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_res.append(\n                            self.nunique(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=[\n                                    self.var_individu,\n                                    self.var_entreprise,\n                                ],\n                            )\n                        )\n                    elif (\n                        (operation == \"count_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is None)\n                    ):\n                        list_res.append(\n                            self.nunique(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=[self.var_individu],\n                            )\n                        )\n                    elif (\n                        (operation == \"count_effectif\")\n                        &amp; (self.var_individu is None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_res.append(\n                            self.nunique(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=[self.var_entreprise],\n                            )\n                        )\n                    elif (\n                        (operation == \"max_sum_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_res.append(\n                            self.max_sum_effectif(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                                var_id=self.var_individu,\n                            )\n                        )\n                        list_res.append(\n                            self.max_sum_effectif(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                                var_id=self.var_entreprise,\n                            )\n                        )\n                    elif (\n                        (operation == \"max_sum_effectif\")\n                        &amp; (self.var_individu is not None)\n                        &amp; (self.var_entreprise is None)\n                    ):\n                        list_res.append(\n                            self.max_sum_effectif(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                                var_id=self.var_individu,\n                            )\n                        )\n                    elif (\n                        (operation == \"max_sum_effectif\")\n                        &amp; (self.var_individu is None)\n                        &amp; (self.var_entreprise is not None)\n                    ):\n                        list_res.append(\n                            self.max_sum_effectif(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                                var_id=self.var_entreprise,\n                            )\n                        )\n                elif isinstance(operation, tuple):\n                    # Le tuple est compos\u00e9 d'un string en premi\u00e8re position et d'un dictionnaire de param\u00e8tres en deuxi\u00e8me\n                    if operation[0] == \"quantile\":\n                        list_res.append(\n                            self.quantile(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                                q=operation[1][\"q\"],\n                            )\n                        )\n                    elif operation[0] == \"prop\":\n                        list_res.append(\n                            self.prop(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_weights=self.var_weights,\n                                var_ref=operation[1][\"var_ref\"],\n                            )\n                        )\n                    elif operation[0] == \"inf_threshold\":\n                        list_res.append(\n                            self.inf_seuil(\n                                data=data_work,\n                                list_var_groupby=[\"index\"],\n                                list_var_of_interest=self.list_var_of_interest,\n                                var_threshold=operation[1][\"var_threshold\"],\n                                seuil=operation[1][\"threshold\"],\n                            )\n                        )\n        else:\n            raise ValueError(\"Unsupported type for iterable_operations\")\n\n        # Construction du jeu de donn\u00e9es r\u00e9sultat\n        data_res = (\n            pd.merge(\n                left=pd.concat(list_res, axis=1, join=\"outer\").reset_index(),\n                right=data_nomenc,\n                how=\"left\",\n                on=\"index\",\n                validate=\"one_to_one\",\n            )\n            .drop(\"index\", axis=1)\n            .set_index(list_var_groupby)\n        )\n\n        return data_res\n\n    def nunique(\n        self,\n        data: pd.DataFrame,\n        list_var_groupby: Union[List[str], None],\n        list_var_of_interest: List[str],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Calculate the number of unique values in the given columns of a DataFrame, optionally grouped by specific columns.\n\n        Parameters:\n        - data (pd.DataFrame): Input DataFrame to compute the number of unique values on.\n        - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed.\n        - list_var_of_interest (List[str]): List of column names for which the number of unique values is computed.\n\n        Returns:\n        - pd.DataFrame: A DataFrame containing the count of unique values. The output column names are appended with \"_nunique\".\n\n        Example:\n        ```\n        df = pd.DataFrame({\n            'A': ['a', 'b', 'a', 'c'],\n            'B': [1, 2, 3, 4]\n        })\n        nunique(df, list_var_groupby=['A'], list_var_of_interest=['B'])\n        ```\n\n        \"\"\"\n        if list_var_groupby is not None:\n            return (\n                data.groupby(list_var_groupby, as_index=True, observed=True)[\n                    list_var_of_interest\n                ]\n                .nunique()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=\"_nunique\"\n                    ),\n                    axis=1,\n                )\n            )\n        else:\n            return (\n                data[list_var_of_interest]\n                .nunique()\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=\"_nunique\"\n                    ),\n                    axis=1,\n                )\n            )\n\n    def count(\n        self,\n        data: pd.DataFrame,\n        list_var_groupby: Union[List[str], None],\n        list_var_of_interest: List[str],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Count the number of non-missing values in the given columns of a DataFrame, optionally grouped by specific columns.\n\n        Parameters:\n        - data (pd.DataFrame): Input DataFrame to compute the count on.\n        - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed.\n        - list_var_of_interest (List[str]): List of column names for which the count is computed.\n\n        Returns:\n        - pd.DataFrame: A DataFrame containing the count. The output column names are appended with \"_count\".\n\n        Example:\n        ```\n        df = pd.DataFrame({\n            'A': ['a', 'b', 'a', None],\n            'B': [1, 2, 3, 4]\n        })\n        count(df, list_var_groupby=['A'], list_var_of_interest=['B'])\n        ```\n\n        \"\"\"\n        if list_var_groupby is not None:\n            return (\n                data.groupby(list_var_groupby, as_index=True, observed=True)[\n                    list_var_of_interest\n                ]\n                .count()\n                .rename(\n                    create_dict_suffix(list_name=list_var_of_interest, suffix=\"_count\"),\n                    axis=1,\n                )\n            )\n        else:\n            return (\n                data[list_var_of_interest]\n                .count()\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(list_name=list_var_of_interest, suffix=\"_count\"),\n                    axis=1,\n                )\n            )\n\n    def any(\n        self,\n        data: pd.DataFrame,\n        list_var_groupby: Union[List[str], None],\n        list_var_of_interest: List[str],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Determine if any element in the given columns of a DataFrame is True, optionally grouped by specific columns.\n\n        Parameters:\n        - data (pd.DataFrame): Input DataFrame to check.\n        - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed.\n        - list_var_of_interest (List[str]): List of column names to check for any True values.\n\n        Returns:\n        - pd.DataFrame: A DataFrame indicating if any value is True. The output column names are appended with \"_any\".\n\n        Example:\n        ```\n        df = pd.DataFrame({\n            'A': ['a', 'b', 'a', 'c'],\n            'B': [True, False, True, False]\n        })\n        any(df, list_var_groupby=['A'], list_var_of_interest=['B'])\n        ```\n\n        \"\"\"\n        if list_var_groupby is not None:\n            return (\n                data.groupby(list_var_groupby, as_index=True, observed=True)[\n                    list_var_of_interest\n                ]\n                .any()\n                .rename(\n                    create_dict_suffix(list_name=list_var_of_interest, suffix=\"_any\"),\n                    axis=1,\n                )\n            )\n        else:\n            return (\n                data[list_var_of_interest]\n                .any()\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(list_name=list_var_of_interest, suffix=\"_any\"),\n                    axis=1,\n                )\n            )\n\n    def all(\n        self,\n        data: pd.DataFrame,\n        list_var_groupby: Union[List[str], None],\n        list_var_of_interest: List[str],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Determine if all elements in the given columns of a DataFrame are True, optionally grouped by specific columns.\n\n        Parameters:\n        - data (pd.DataFrame): Input DataFrame to check.\n        - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed.\n        - list_var_of_interest (List[str]): List of column names to check if all values are True.\n\n        Returns:\n        - pd.DataFrame: A DataFrame indicating if all values are True. The output column names are appended with \"_all\".\n\n        Example:\n        ```\n        df = pd.DataFrame({\n            'A': ['a', 'b', 'a', 'c'],\n            'B': [True, True, True, False]\n        })\n        all(df, list_var_groupby=['A'], list_var_of_interest=['B'])\n        ```\n\n        \"\"\"\n        if list_var_groupby is not None:\n            return (\n                data.groupby(list_var_groupby, as_index=True, observed=True)[\n                    list_var_of_interest\n                ]\n                .all()\n                .rename(\n                    create_dict_suffix(list_name=list_var_of_interest, suffix=\"_all\"),\n                    axis=1,\n                )\n            )\n        else:\n            return (\n                data[list_var_of_interest]\n                .all()\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(list_name=list_var_of_interest, suffix=\"_all\"),\n                    axis=1,\n                )\n            )\n\n    def sum(\n        self,\n        data: pd.DataFrame,\n        list_var_groupby: Union[List[str], None],\n        list_var_of_interest: List[str],\n        var_weights: Union[str, None],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Compute the sum aggregation on a given pandas DataFrame, with options for weighted sum.\n\n        Parameters:\n        - data (pd.DataFrame): The input DataFrame to compute aggregation on.\n        - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed.\n        - list_var_of_interest (List[str]): List of column names on which the aggregation is performed.\n        - var_weights (str, optional): Column name representing the weights for weighted sum.\n                                    If it is in list_var_of_interest, the function will compute both weighted\n                                    and non-weighted sum. If None, a simple sum is performed.\n\n        Returns:\n        - pd.DataFrame: A DataFrame containing the aggregated data. The output DataFrame will have a multi-index if\n                        list_var_groupby is provided and more than one type of aggregation (weighted and non-weighted)\n                        is performed. The column names in the output DataFrame will be appended with suffixes like \"_sum\".\n\n        Notes:\n        - The function uses helper functions like `create_pond_data` and `create_dict_suffix` which should be present in the\n        same context.\n        - It handles different combinations of list_var_groupby and var_weights, giving flexibility in the type of aggregation\n        performed.\n\n        Examples:\n        ```\n        df = pd.DataFrame({\n            'A': [1, 2, 3, 4],\n            'B': [10, 20, 30, 40],\n            'C': [100, 200, 300, 400],\n            'weights': [0.1, 0.2, 0.3, 0.4]\n        })\n\n        sum(df, list_var_groupby=['A'], list_var_of_interest=['B', 'C'], var_weights='weights')\n        ```\n\n        \"\"\"\n        # Initialisation de la liste \u00e0 retourner\n        list_data_return = []\n\n        # Calcul de la somme des poids non pond\u00e9r\u00e9e et suppression de la liste des variables d'int\u00e9r\u00eat\n        # Sans doute faire la m\u00eame chose pour les autres op\u00e9rations\n        # L'inconv\u00e9nient c'est qu'on ne peut plus faire de somme pond\u00e9r\u00e9e des poids (alors qu'avant on pouvait d\u00e9j\u00e0 le faire avec var_weights=None)\n        if (var_weights is not None) &amp; (var_weights in list_var_of_interest):\n            # Ajout de la somme des poids\n            if list_var_groupby is not None:\n                list_data_return.append(\n                    data.groupby(list_var_groupby, as_index=True, observed=True)[\n                        [var_weights]\n                    ]\n                    .sum()\n                    .rename(\n                        create_dict_suffix(list_name=[var_weights], suffix=\"_sum\"),\n                        axis=1,\n                    )\n                )\n            else:\n                list_data_return.append(\n                    data[[var_weights]]\n                    .sum()\n                    .to_frame()\n                    .transpose()\n                    .rename(\n                        create_dict_suffix(list_name=[var_weights], suffix=\"_sum\"),\n                        axis=1,\n                    )\n                )\n\n            # Initialisation de la liste de travail sans les poids\n            list_var_of_interest_work = [\n                var_of_interest\n                for var_of_interest in list_var_of_interest\n                if var_of_interest != var_weights\n            ]\n        else:\n            list_var_of_interest_work = list_var_of_interest\n\n        if (\n            (len(list_var_of_interest_work) &gt; 0)\n            &amp; (list_var_groupby is not None)\n            &amp; (var_weights is not None)\n        ):\n            data_pond = create_pond_data(\n                data=data,\n                list_var_of_interest=list_var_of_interest_work,\n                list_var_groupby=list_var_groupby,\n                var_weights=var_weights,\n            )\n            list_data_return.append(\n                data_pond.groupby(list_var_groupby, as_index=True, observed=True)[\n                    list_var_of_interest_work\n                ]\n                .sum()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest_work, suffix=\"_sum\"\n                    ),\n                    axis=1,\n                )\n            )\n        elif (\n            (len(list_var_of_interest_work) &gt; 0)\n            &amp; (list_var_groupby is not None)\n            &amp; (var_weights is None)\n        ):\n            list_data_return.append(\n                data.groupby(list_var_groupby, as_index=True, observed=True)[\n                    list_var_of_interest_work\n                ]\n                .sum()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest_work, suffix=\"_sum\"\n                    ),\n                    axis=1,\n                )\n            )\n        elif (\n            (len(list_var_of_interest_work) &gt; 0)\n            &amp; (list_var_groupby is None)\n            &amp; (var_weights is not None)\n        ):\n            data_pond = create_pond_data(\n                data=data,\n                list_var_of_interest=list_var_of_interest_work,\n                list_var_groupby=list_var_groupby,\n                var_weights=var_weights,\n            )\n            list_data_return.append(\n                data_pond.sum()\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest_work, suffix=\"_sum\"\n                    ),\n                    axis=1,\n                )\n            )\n        elif (\n            (len(list_var_of_interest_work) &gt; 0)\n            &amp; (list_var_groupby is None)\n            &amp; (var_weights is None)\n        ):\n            list_data_return.append(\n                data[list_var_of_interest_work]\n                .sum()\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest_work, suffix=\"_sum\"\n                    ),\n                    axis=1,\n                )\n            )\n\n        if list_var_groupby is not None:\n            return pd.concat(list_data_return, axis=1, join=\"outer\")\n        else:\n            return pd.concat(list_data_return, axis=0, join=\"outer\")\n\n    def mean(\n        self,\n        data: pd.DataFrame,\n        list_var_groupby: Union[List[str], None],\n        list_var_of_interest: List[str],\n        var_weights: Union[str, None],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Computes the weighted or unweighted mean of the specified variables,\n        possibly grouped by specified variables.\n\n        Parameters:\n        -----------\n        data : pd.DataFrame\n            The dataset containing the data to be processed.\n        list_var_groupby : list of str, optional\n            List of variable names to group by.\n        list_var_of_interest : list of str\n            List of variable names to compute the mean for.\n        var_weights : str, optional\n            Name of the column containing weights for weighted computation.\n\n        Returns:\n        --------\n        pd.Series or pd.DataFrame : The computed mean for the specified variables.\n        \"\"\"\n        if (list_var_groupby is not None) &amp; (var_weights is not None):\n            data_pond = create_pond_data(\n                data=data,\n                list_var_of_interest=list_var_of_interest,\n                list_var_groupby=list_var_groupby,\n                var_weights=var_weights,\n            )\n            return (\n                data_pond.groupby(list_var_groupby, as_index=True, observed=True)[\n                    list_var_of_interest\n                ]\n                .sum()\n                .divide(\n                    other=data.groupby(list_var_groupby, as_index=True, observed=True)[\n                        var_weights\n                    ].sum(),\n                    axis=0,\n                )\n                .rename(\n                    create_dict_suffix(list_name=list_var_of_interest, suffix=\"_mean\"),\n                    axis=1,\n                )\n            )\n        elif (list_var_groupby is not None) &amp; (var_weights is None):\n            return (\n                data.groupby(list_var_groupby, as_index=True, observed=True)[\n                    list_var_of_interest\n                ]\n                .mean()\n                .rename(\n                    create_dict_suffix(list_name=list_var_of_interest, suffix=\"_mean\"),\n                    axis=1,\n                )\n            )\n        elif (list_var_groupby is None) &amp; (var_weights is not None):\n            data_pond = create_pond_data(\n                data=data,\n                list_var_of_interest=list_var_of_interest,\n                list_var_groupby=list_var_groupby,\n                var_weights=var_weights,\n            )\n            return (\n                data_pond.sum()\n                .divide(other=data[var_weights].sum())\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(list_name=list_var_of_interest, suffix=\"_mean\"),\n                    axis=1,\n                )\n            )\n        elif (list_var_groupby is None) &amp; (var_weights is None):\n            return (\n                data[list_var_of_interest]\n                .mean()\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(list_name=list_var_of_interest, suffix=\"_mean\"),\n                    axis=1,\n                )\n            )\n\n    def median(\n        self,\n        data: pd.DataFrame,\n        list_var_groupby: Union[List[str], None],\n        list_var_of_interest: List[str],\n        var_weights: Union[str, None],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Computes the median value for the specified variables,\n        possibly grouped by specified variables.\n\n        Parameters are the same as the mean method.\n\n        Returns:\n        --------\n        pd.Series or pd.DataFrame\n            The median values for the specified variables.\n        \"\"\"\n        return self.quantile(\n            data=data,\n            list_var_groupby=list_var_groupby,\n            list_var_of_interest=list_var_of_interest,\n            var_weights=var_weights,\n            q=0.5,\n        )\n\n    def quantile(\n        self,\n        data: pd.DataFrame,\n        list_var_groupby: Union[List[str], None],\n        list_var_of_interest: List[str],\n        var_weights: Union[str, None],\n        q: float,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Computes the q-th quantile for the specified variables,\n        possibly grouped by specified variables.\n\n        Parameters:\n        -----------\n        q : float\n            Quantile to compute, which must be between 0 and 1 inclusive.\n\n        Other parameters are the same as the mean method.\n\n        Returns:\n        --------\n        pd.Series or pd.DataFrame\n            The q-th quantile for the specified variables.\n        \"\"\"\n        if (list_var_groupby is not None) &amp; (var_weights is not None):\n            return (\n                data.groupby(list_var_groupby, as_index=True, observed=True)[\n                    list_var_of_interest + [var_weights]\n                ]\n                .apply(\n                    func=lambda x: weighted_quantile(\n                        data=x,\n                        vars_of_interest=list_var_of_interest,\n                        var_weights=var_weights,\n                        q=q,\n                    )\n                )\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=\"_q\" + str(q)\n                    ),\n                    axis=1,\n                )\n            )\n        elif (list_var_groupby is not None) &amp; (var_weights is None):\n            return (\n                data.groupby(list_var_groupby, as_index=True, observed=True)[\n                    list_var_of_interest\n                ]\n                .quantile(q=q)\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=\"_q\" + str(q)\n                    ),\n                    axis=1,\n                )\n            )\n        elif (list_var_groupby is None) &amp; (var_weights is not None):\n            return (\n                weighted_quantile(\n                    data=data,\n                    vars_of_interest=list_var_of_interest,\n                    var_weights=var_weights,\n                    q=q,\n                )\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=\"_q\" + str(q)\n                    ),\n                    axis=1,\n                )\n            )\n        elif (list_var_groupby is None) &amp; (var_weights is None):\n            return (\n                data[list_var_of_interest]\n                .quantile(q=q)\n                .rename(0)\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=\"_q\" + str(q)\n                    ),\n                    axis=1,\n                )\n            )\n\n    def prop(\n        self,\n        data: pd.DataFrame,\n        list_var_groupby: Union[List[str], None],\n        list_var_of_interest: List[str],\n        var_weights: Union[str, None],\n        var_ref: str,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Computes the proportion of the specified variables based on a reference variable,\n        possibly grouped by specified variables.\n\n        Parameters:\n        -----------\n        var_ref : str\n            The reference variable used to compute the proportion.\n\n        Other parameters are the same as the mean method.\n\n        Returns:\n        --------\n        pd.Series or pd.DataFrame\n            The computed proportions for the specified variables.\n        \"\"\"\n        if (list_var_groupby is not None) &amp; (var_weights is not None):\n            data_pond = create_pond_data(\n                data=data,\n                list_var_of_interest=list_var_of_interest,\n                list_var_groupby=list_var_groupby,\n                var_weights=var_weights,\n            )\n            data_pond_ref = create_pond_data(\n                data=data,\n                list_var_of_interest=[var_ref],\n                list_var_groupby=list_var_groupby,\n                var_weights=var_weights,\n            )\n            return (\n                data_pond.groupby(list_var_groupby)\n                .sum()\n                .divide(data_pond_ref.groupby(list_var_groupby)[var_ref].sum(), axis=0)\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=f\"_{var_ref}_prop\"\n                    ),\n                    axis=1,\n                )\n            )\n        elif (list_var_groupby is not None) &amp; (var_weights is None):\n            return (\n                data.groupby(list_var_groupby)[list_var_of_interest]\n                .sum()\n                .divide(other=data.groupby(list_var_groupby)[var_ref].sum(), axis=0)\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=f\"_{var_ref}_prop\"\n                    ),\n                    axis=1,\n                )\n            )\n        elif (list_var_groupby is None) &amp; (var_weights is not None):\n            data_pond = create_pond_data(\n                data=data,\n                list_var_of_interest=list_var_of_interest,\n                list_var_groupby=list_var_groupby,\n                var_weights=var_weights,\n            )\n            data_pond_ref = create_pond_data(\n                data=data,\n                list_var_of_interest=[var_ref],\n                list_var_groupby=list_var_groupby,\n                var_weights=var_weights,\n            )\n            return (\n                data_pond.sum()\n                .divide(data_pond_ref[var_ref].sum())\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=f\"_{var_ref}_prop\"\n                    ),\n                    axis=1,\n                )\n            )\n        elif (list_var_groupby is None) &amp; (var_weights is None):\n            data_res = (\n                (data[list_var_of_interest].sum() / data[var_ref].sum())\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=f\"_{var_ref}_prop\"\n                    ),\n                    axis=1,\n                )\n            )\n            return data_res\n\n    def majority(\n        self,\n        data: pd.DataFrame,\n        list_var_groupby: Union[List[str], None],\n        list_var_of_interest: List[str],\n        var_weights: Union[str, None],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Determine the majority value for the specified variables, optionally weighted and grouped.\n\n        Parameters:\n            data (pd.DataFrame): The data frame containing the data.\n            list_var_groupby (list or None): The variables to group by. If None, no grouping is performed.\n            list_var_of_interest (list): The variables for which to determine the majority value.\n            var_weights (str or None): The variable to use for weighting. If None, no weighting is applied.\n\n        Returns:\n            pd.DataFrame: A data frame with the majority value for each specified variable of interest, optionally grouped and weighted.\n        \"\"\"\n        if (list_var_groupby is not None) &amp; (var_weights is not None):\n            return pd.concat(\n                [\n                    data.groupby(list_var_groupby)[[var_of_interest, var_weights]]\n                    .apply(\n                        func=lambda x: x.groupby(var_of_interest)[var_weights]\n                        .sum()\n                        .idxmax()\n                    )\n                    .to_frame()\n                    .rename({0: var_of_interest}, axis=1)\n                    for var_of_interest in list_var_of_interest\n                ],\n                axis=1,\n                join=\"outer\",\n            ).rename(\n                create_dict_suffix(list_name=list_var_of_interest, suffix=\"_majority\"),\n                axis=1,\n            )\n        elif (list_var_groupby is not None) &amp; (var_weights is None):\n            return pd.concat(\n                [\n                    data.groupby(list_var_groupby)[[var_of_interest]]\n                    .apply(func=lambda x: x[var_of_interest].value_counts().idxmax())\n                    .to_frame()\n                    .rename({0: var_of_interest}, axis=1)\n                    for var_of_interest in list_var_of_interest\n                ],\n                axis=1,\n                join=\"outer\",\n            ).rename(\n                create_dict_suffix(list_name=list_var_of_interest, suffix=\"_majority\"),\n                axis=1,\n            )\n        elif (list_var_groupby is None) &amp; (var_weights is not None):\n            return (\n                pd.Series(\n                    [\n                        data.groupby(var_of_interest)[var_weights].sum().idxmax()\n                        for var_of_interest in list_var_of_interest\n                    ],\n                    index=list_var_of_interest,\n                )\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=\"_majority\"\n                    ),\n                    axis=1,\n                )\n            )\n        elif (list_var_groupby is None) &amp; (var_weights is None):\n            return (\n                pd.Series(\n                    [\n                        data[var_of_interest].value_counts().idxmax()\n                        for var_of_interest in list_var_of_interest\n                    ],\n                    index=list_var_of_interest,\n                )\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=\"_majority\"\n                    ),\n                    axis=1,\n                )\n            )\n\n    def inf_threshold(\n        self,\n        data: pd.DataFrame,\n        list_var_groupby: Union[List[str], None],\n        list_var_of_interest: List[str],\n        var_threshold: str,\n        seuil: Union[int, float],\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Computes the proportion of unique values for the specified variables below a certain threshold.\n\n        Parameters:\n        -----------\n        var_threshold : str\n            The variable based on which the thresholding will be done.\n        seuil : int or float\n            The threshold value.\n\n        Other parameters are the same as the mean method.\n\n        Returns:\n        --------\n        pd.Series or pd.DataFrame\n            The proportion of unique values for the specified variables below the threshold.\n        \"\"\"\n        # A terminer en autorisant plusieurs op\u00e9rations\n        if list_var_groupby is not None:\n            return (\n                data.loc[data[var_threshold] &lt; seuil]\n                .groupby(list_var_groupby)[list_var_of_interest]\n                .nunique()\n                / data.groupby(list_var_groupby)[list_var_of_interest].nunique()\n            ).rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=\"_inf_\" + str(seuil)\n                ),\n                axis=1,\n            )\n        else:\n            return (\n                (\n                    data.loc[data[var_threshold] &lt; seuil][\n                        list_var_of_interest\n                    ].nunique()\n                    / data[list_var_of_interest].nunique()\n                )\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=\"_inf_\" + str(seuil)\n                    ),\n                    axis=1,\n                )\n            )\n\n    def max_sum_effectif(\n        self,\n        data: pd.DataFrame,\n        list_var_groupby: Union[List[str], None],\n        list_var_of_interest: List[str],\n        var_weights: Union[str, None],\n        var_id: str,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Computes the maximum proportion of a variable relative to its sum, possibly grouped by specified variables.\n\n        Parameters:\n        -----------\n        var_id : str\n            The identifier for renaming the resulting variables.\n\n        Other parameters are the same as the mean method.\n\n        Returns:\n        --------\n        pd.Series or pd.DataFrame\n            The computed maximum proportions for the specified variables relative to their sum.\n        \"\"\"\n        if (list_var_groupby is not None) &amp; (var_weights is not None):\n            data_pond = create_pond_data(\n                data=data,\n                list_var_of_interest=list_var_of_interest,\n                list_var_groupby=list_var_groupby,\n                var_weights=var_weights,\n            )\n            return (\n                data_pond.groupby(list_var_groupby)\n                .max()\n                .divide(other=data_pond.groupby(list_var_groupby).sum())\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=f\"_{var_id}_max/sum\"\n                    ),\n                    axis=1,\n                )\n            )\n        elif (list_var_groupby is not None) &amp; (var_weights is None):\n            return (\n                data.groupby(list_var_groupby)[list_var_of_interest]\n                .max()\n                .divide(data.groupby(list_var_groupby)[list_var_of_interest].sum())\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=f\"_{var_id}_max/sum\"\n                    ),\n                    axis=1,\n                )\n            )\n        elif (list_var_groupby is None) &amp; (var_weights is not None):\n            data_pond = create_pond_data(\n                data=data,\n                list_var_of_interest=list_var_of_interest,\n                list_var_groupby=list_var_groupby,\n                var_weights=var_weights,\n            )\n            return (\n                data_pond.max()\n                .divide(other=data_pond.sum())\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=f\"_{var_id}_max/sum\"\n                    ),\n                    axis=1,\n                )\n            )\n        elif (list_var_groupby is None) &amp; (var_weights is None):\n            return (\n                data[list_var_of_interest]\n                .max()\n                .divide(other=data[list_var_of_interest].sum())\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(\n                        list_name=list_var_of_interest, suffix=f\"_{var_id}_max/sum\"\n                    ),\n                    axis=1,\n                )\n            )\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.__init__","title":"<code>__init__(data_source, list_var_groupby, list_var_of_interest, var_individu=None, var_entreprise=None, var_weights=None, dropna=False)</code>","text":"<p>Initialize the StatDesGroupBy class with data and parameters.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.__init__--parameters","title":"Parameters:","text":"<p>data_source : DataFrame     Source data for computing statistics. list_var_groupby : list     List of variables to group data by. list_var_of_interest : list     List of variables for which descriptive statistics will be computed. var_individu : str, optional     Variable representing individual data. var_entreprise : str, optional     Variable representing enterprise data. var_weights : str, optional     Variable representing the weights for each data entry.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def __init__(\n    self,\n    data_source: pd.DataFrame,\n    list_var_groupby: List[str],\n    list_var_of_interest: List[str],\n    var_individu: Optional[Union[str, None]] = None,\n    var_entreprise: Optional[Union[str, None]] = None,\n    var_weights: Optional[Union[str, None]] = None,\n    dropna: Optional[bool] = False,\n) -&gt; None:\n    \"\"\"\n    Initialize the StatDesGroupBy class with data and parameters.\n\n    Parameters:\n    -----------\n    data_source : DataFrame\n        Source data for computing statistics.\n    list_var_groupby : list\n        List of variables to group data by.\n    list_var_of_interest : list\n        List of variables for which descriptive statistics will be computed.\n    var_individu : str, optional\n        Variable representing individual data.\n    var_entreprise : str, optional\n        Variable representing enterprise data.\n    var_weights : str, optional\n        Variable representing the weights for each data entry.\n    \"\"\"\n    # Initialisation des param\u00e8tres\n    self.list_var_groupby = list_var_groupby\n    self.list_var_of_interest = list_var_of_interest\n    self.var_individu = var_individu\n    self.var_entreprise = var_entreprise\n    self.var_weights = var_weights\n\n    # Initialisation de la liste des variables d'int\u00e9r\u00eat\n    if var_weights is not None:\n        list_var_of_interest_weights = list_var_of_interest + [var_weights]\n    else:\n        list_var_of_interest_weights = list_var_of_interest\n\n    # Copie ind\u00e9pendante du jeu de donn\u00e9es\n    if (\n        (var_individu not in list_var_groupby)\n        &amp; (var_individu is not None)\n        &amp; (var_entreprise not in list_var_groupby)\n        &amp; (var_entreprise is not None)\n    ):\n        list_var_keep = np.unique(\n            list_var_groupby\n            + list_var_of_interest_weights\n            + [var_individu, var_entreprise]\n        ).tolist()\n    elif (\n        (var_individu not in list_var_groupby)\n        &amp; (var_individu is not None)\n        &amp; ((var_entreprise in list_var_groupby) | (var_entreprise is None))\n    ):\n        list_var_keep = np.unique(\n            list_var_groupby + list_var_of_interest_weights + [var_individu]\n        ).tolist()\n    elif (\n        ((var_individu in list_var_groupby) | (var_individu is None))\n        &amp; (var_entreprise not in list_var_groupby)\n        &amp; (var_entreprise is not None)\n    ):\n        list_var_keep = np.unique(\n            list_var_groupby + list_var_of_interest_weights + [var_entreprise]\n        ).tolist()\n    else:\n        list_var_keep = np.unique(\n            list_var_groupby + list_var_of_interest_weights\n        ).tolist()\n\n    # Suppression des Nan\n    if dropna:\n        self.data_source = data_source[list_var_keep].copy().dropna(how=\"any\")\n    else:\n        self.data_source = data_source[list_var_keep].copy()\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.add_total","title":"<code>add_total(iterable_operations)</code>","text":"<p>Perform a series of global aggregate operations on the data and concatenate the results.</p> <p>The function goes through the specified operations (either as strings for basic operations or tuples for operations with extra parameters) and applies them on the data_source. The result of each operation is appended to a list. At the end, the results are concatenated and returned as a single DataFrame.</p> <p>Parameters: - iterable_operations (dict or list): Operations to be applied on the data. If a dictionary, the keys represent the operation and values represent variables of interest. If a list, it contains either operations as strings or tuples where the first element is the operation and the second is a dictionary of parameters.</p> <p>Returns: - pd.DataFrame: A DataFrame containing aggregated results after applying all the operations, indexed by either 'Total' or a MultiIndex version of it based on the length of list_var_groupby.</p> <p>Raises: - ValueError: If the provided type for iterable_operations is neither dict nor list.</p> <p>Example: <pre><code># Example usage (assuming it's a method within a class)\naggregated_data = StatDesGroupby.add_total({'sum': ['var1', 'var2']})\n</code></pre></p> <p>Notes: - The function uses internal methods of the class, such as sum, mean, median, etc. - The operations are performed without groupby, intended to get overall aggregates. - The 'count_effectif' and 'max_sum_effectif' operations have special handling based on the presence of var_individu and var_entreprise class attributes.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def add_total(self, iterable_operations: Union[dict, List[str]]) -&gt; pd.DataFrame:\n    \"\"\"\n    Perform a series of global aggregate operations on the data and concatenate the results.\n\n    The function goes through the specified operations (either as strings for basic operations or tuples for\n    operations with extra parameters) and applies them on the data_source. The result of each operation is\n    appended to a list. At the end, the results are concatenated and returned as a single DataFrame.\n\n    Parameters:\n    - iterable_operations (dict or list): Operations to be applied on the data. If a dictionary, the keys\n    represent the operation and values represent variables of interest. If a list, it contains either\n    operations as strings or tuples where the first element is the operation and the second is a dictionary\n    of parameters.\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing aggregated results after applying all the operations, indexed by\n    either 'Total' or a MultiIndex version of it based on the length of list_var_groupby.\n\n    Raises:\n    - ValueError: If the provided type for iterable_operations is neither dict nor list.\n\n    Example:\n    ```python\n    # Example usage (assuming it's a method within a class)\n    aggregated_data = StatDesGroupby.add_total({'sum': ['var1', 'var2']})\n    ```\n\n    Notes:\n    - The function uses internal methods of the class, such as sum, mean, median, etc.\n    - The operations are performed without groupby, intended to get overall aggregates.\n    - The 'count_effectif' and 'max_sum_effectif' operations have special handling based on the presence\n    of var_individu and var_entreprise class attributes.\n    \"\"\"\n\n    # Initialisation de la liste r\u00e9sultat\n    list_total = []\n    # Parcours des diff\u00e9rentes op\u00e9rations\n    # Faire avec agg pour limiter le nombre de groupby\n    if isinstance(iterable_operations, dict):\n        for operation, list_var_of_interest_work in iterable_operations.items():\n            if isinstance(operation, str):\n                if operation == \"sum\":\n                    list_total.append(\n                        self.sum(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif operation == \"mean\":\n                    list_total.append(\n                        self.mean(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif operation == \"median\":\n                    list_total.append(\n                        self.median(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif operation == \"nunique\":\n                    list_total.append(\n                        self.nunique(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                        )\n                    )\n                elif operation == \"count\":\n                    list_total.append(\n                        self.count(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                        )\n                    )\n                elif operation == \"any\":\n                    list_total.append(\n                        self.any(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                        )\n                    )\n                elif operation == \"all\":\n                    list_total.append(\n                        self.all(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                        )\n                    )\n                elif operation == \"majority\":\n                    list_total.append(\n                        self.majority(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif (\n                    (operation == \"count_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_total.append(\n                        self.nunique(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=[\n                                self.var_individu,\n                                self.var_entreprise,\n                            ],\n                        )\n                    )\n                elif (\n                    (operation == \"count_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is None)\n                ):\n                    list_total.append(\n                        self.nunique(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=[self.var_individu],\n                        )\n                    )\n                elif (\n                    (operation == \"count_effectif\")\n                    &amp; (self.var_individu is None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_total.append(\n                        self.nunique(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=[self.var_entreprise],\n                        )\n                    )\n                elif (\n                    (operation == \"max_sum_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_total.append(\n                        self.max_sum_effectif(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                            var_id=self.var_individu,\n                        )\n                    )\n                    list_total.append(\n                        self.max_sum_effectif(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                            var_id=self.var_entreprise,\n                        )\n                    )\n                elif (\n                    (operation == \"max_sum_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is None)\n                ):\n                    list_total.append(\n                        self.max_sum_effectif(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                            var_id=self.var_individu,\n                        )\n                    )\n                elif (\n                    (operation == \"max_sum_effectif\")\n                    &amp; (self.var_individu is None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_total.append(\n                        self.max_sum_effectif(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                            var_id=self.var_entreprise,\n                        )\n                    )\n            elif isinstance(operation, tuple):\n                # Le tuple est compos\u00e9 d'un string en premi\u00e8re position et d'un dictionnaire de param\u00e8tres en deuxi\u00e8me\n                if operation[0] == \"quantile\":\n                    list_total.append(\n                        self.quantile(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                            q=operation[1][\"q\"],\n                        )\n                    )\n                elif operation[0] == \"prop\":\n                    list_total.append(\n                        self.prop(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                            var_ref=operation[1][\"var_ref\"],\n                        )\n                    )\n                elif operation[0] == \"inf_threshold\":\n                    list_total.append(\n                        self.inf_seuil(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_threshold=operation[1][\"var_threshold\"],\n                            seuil=operation[1][\"threshold\"],\n                        )\n                    )\n    elif isinstance(iterable_operations, list):\n        for operation in iterable_operations:\n            if isinstance(operation, str):\n                if operation == \"sum\":\n                    list_total.append(\n                        self.sum(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif operation == \"mean\":\n                    list_total.append(\n                        self.mean(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif operation == \"median\":\n                    list_total.append(\n                        self.median(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif operation == \"nunique\":\n                    list_total.append(\n                        self.nunique(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                        )\n                    )\n                elif operation == \"count\":\n                    list_total.append(\n                        self.count(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                        )\n                    )\n                elif operation == \"any\":\n                    list_total.append(\n                        self.any(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                        )\n                    )\n                elif operation == \"all\":\n                    list_total.append(\n                        self.all(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                        )\n                    )\n                elif operation == \"majority\":\n                    list_total.append(\n                        self.majority(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif (\n                    (operation == \"count_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_total.append(\n                        self.nunique(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=[\n                                self.var_individu,\n                                self.var_entreprise,\n                            ],\n                        )\n                    )\n                elif (\n                    (operation == \"count_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is None)\n                ):\n                    list_total.append(\n                        self.nunique(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=[self.var_individu],\n                        )\n                    )\n                elif (\n                    (operation == \"count_effectif\")\n                    &amp; (self.var_individu is None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_total.append(\n                        self.nunique(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=[self.var_entreprise],\n                        )\n                    )\n                elif (\n                    (operation == \"max_sum_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_total.append(\n                        self.max_sum_effectif(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                            var_id=self.var_individu,\n                        )\n                    )\n                    list_total.append(\n                        self.max_sum_effectif(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                            var_id=self.var_entreprise,\n                        )\n                    )\n                elif (\n                    (operation == \"max_sum_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is None)\n                ):\n                    list_total.append(\n                        self.max_sum_effectif(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                            var_id=self.var_individu,\n                        )\n                    )\n                elif (\n                    (operation == \"max_sum_effectif\")\n                    &amp; (self.var_individu is None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_total.append(\n                        self.max_sum_effectif(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                            var_id=self.var_entreprise,\n                        )\n                    )\n            elif isinstance(operation, tuple):\n                # Le tuple est compos\u00e9 d'un string en premi\u00e8re position et d'un dictionnaire de param\u00e8tres en deuxi\u00e8me\n                if operation[0] == \"quantile\":\n                    list_total.append(\n                        self.quantile(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                            q=operation[1][\"q\"],\n                        )\n                    )\n                elif operation[0] == \"prop\":\n                    list_total.append(\n                        self.prop(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                            var_ref=operation[1][\"var_ref\"],\n                        )\n                    )\n                elif operation[0] == \"inf_threshold\":\n                    list_total.append(\n                        self.inf_seuil(\n                            data=self.data_source,\n                            list_var_groupby=None,\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_threshold=operation[1][\"var_threshold\"],\n                            seuil=operation[1][\"threshold\"],\n                        )\n                    )\n    else:\n        raise ValueError(\"Unsupported type for iterable_operations\")\n\n    # Concat\u00e9nation des jeux de donn\u00e9es\n    data_total = pd.concat(list_total, axis=1, join=\"outer\")\n\n    if len(self.list_var_groupby) &gt; 1:\n        data_total.index = pd.MultiIndex.from_tuples(\n            [[\"Total\"] * len(self.list_var_groupby)], names=self.list_var_groupby\n        )\n    else:\n        data_total.index = [\"Total\"]\n\n    return data_total\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.add_under_total","title":"<code>add_under_total(iterable_operations)</code>","text":"<p>Generates and returns subtotals for combinations of grouping variables.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.add_under_total--parameters","title":"Parameters:","text":"<p>iterable_operations : iterable     Operations or functions to apply to the grouped data to compute subtotals.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.add_under_total--returns","title":"Returns:","text":"<p>DataFrame     Descriptive statistics with subtotals for combinations of grouping variables.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def add_under_total(\n    self, iterable_operations: Union[dict, List[str]]\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generates and returns subtotals for combinations of grouping variables.\n\n    Parameters:\n    -----------\n    iterable_operations : iterable\n        Operations or functions to apply to the grouped data to compute subtotals.\n\n    Returns:\n    --------\n    DataFrame\n        Descriptive statistics with subtotals for combinations of grouping variables.\n    \"\"\"\n\n    # Parcours de toutes les combinaisons possibles de niveaux\n    # Cr\u00e9ation des combinaisons\n    list_combinations = []\n    for i in range(1, len(self.list_var_groupby)):\n        list_combinations += list(\n            combinations(np.arange(len(self.list_var_groupby)), r=i)\n        )\n    # Conversion en liste\n    list_combinations = [list(combination) for combination in list_combinations]\n    # Initialisation de la liste r\u00e9sultat\n    list_sub_total = []\n    for combination in list_combinations:\n        if len(combination) &gt; 0:\n            # Construction de la liste avec les sous-ensembles de variables\n            list_var_sub_groupby = [\n                self.list_var_groupby[e]\n                for e in range(len(self.list_var_groupby))\n                if e not in combination\n            ]\n            # It\u00e9ration des statistiques descriptives\n            data_sub_total = self.iterate_operations(\n                iterable_operations=iterable_operations,\n                data=self.data_source,\n                list_var_groupby=list_var_sub_groupby,\n            )\n            # Modification de l'index\n            sub_total_index_res = []\n            for sub_total_index in data_sub_total.index:\n                for i_remove in combination:\n                    if isinstance(sub_total_index, tuple):\n                        sub_total_index = (\n                            list(sub_total_index)[:i_remove]\n                            + [\"Total\"]\n                            + list(sub_total_index[i_remove:])\n                        )\n                    elif isinstance(sub_total_index, list):\n                        sub_total_index = (\n                            sub_total_index[:i_remove]\n                            + [\"Total\"]\n                            + sub_total_index[i_remove:]\n                        )\n                    elif i_remove == 0:\n                        sub_total_index = [\"Total\"] + [sub_total_index]\n                    elif i_remove == 1:\n                        sub_total_index = [sub_total_index] + [\"Total\"]\n                    else:\n                        raise ValueError(\"Unknown index\")\n                sub_total_index_res.append(sub_total_index)\n            data_sub_total.index = pd.MultiIndex.from_tuples(\n                sub_total_index_res, names=self.list_var_groupby\n            )\n            # Ajout \u00e0 la liste r\u00e9sultat\n            list_sub_total.append(data_sub_total)\n    # Concat\u00e9nation des jeux de donn\u00e9es r\u00e9sultats\n    data_sub_total = pd.concat(list_sub_total, axis=0, join=\"outer\")\n\n    return data_sub_total\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.all","title":"<code>all(data, list_var_groupby, list_var_of_interest)</code>","text":"<p>Determine if all elements in the given columns of a DataFrame are True, optionally grouped by specific columns.</p> <p>Parameters: - data (pd.DataFrame): Input DataFrame to check. - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed. - list_var_of_interest (List[str]): List of column names to check if all values are True.</p> <p>Returns: - pd.DataFrame: A DataFrame indicating if all values are True. The output column names are appended with \"_all\".</p> <p>Example: <pre><code>df = pd.DataFrame({\n    'A': ['a', 'b', 'a', 'c'],\n    'B': [True, True, True, False]\n})\nall(df, list_var_groupby=['A'], list_var_of_interest=['B'])\n</code></pre></p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def all(\n    self,\n    data: pd.DataFrame,\n    list_var_groupby: Union[List[str], None],\n    list_var_of_interest: List[str],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Determine if all elements in the given columns of a DataFrame are True, optionally grouped by specific columns.\n\n    Parameters:\n    - data (pd.DataFrame): Input DataFrame to check.\n    - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed.\n    - list_var_of_interest (List[str]): List of column names to check if all values are True.\n\n    Returns:\n    - pd.DataFrame: A DataFrame indicating if all values are True. The output column names are appended with \"_all\".\n\n    Example:\n    ```\n    df = pd.DataFrame({\n        'A': ['a', 'b', 'a', 'c'],\n        'B': [True, True, True, False]\n    })\n    all(df, list_var_groupby=['A'], list_var_of_interest=['B'])\n    ```\n\n    \"\"\"\n    if list_var_groupby is not None:\n        return (\n            data.groupby(list_var_groupby, as_index=True, observed=True)[\n                list_var_of_interest\n            ]\n            .all()\n            .rename(\n                create_dict_suffix(list_name=list_var_of_interest, suffix=\"_all\"),\n                axis=1,\n            )\n        )\n    else:\n        return (\n            data[list_var_of_interest]\n            .all()\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(list_name=list_var_of_interest, suffix=\"_all\"),\n                axis=1,\n            )\n        )\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.any","title":"<code>any(data, list_var_groupby, list_var_of_interest)</code>","text":"<p>Determine if any element in the given columns of a DataFrame is True, optionally grouped by specific columns.</p> <p>Parameters: - data (pd.DataFrame): Input DataFrame to check. - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed. - list_var_of_interest (List[str]): List of column names to check for any True values.</p> <p>Returns: - pd.DataFrame: A DataFrame indicating if any value is True. The output column names are appended with \"_any\".</p> <p>Example: <pre><code>df = pd.DataFrame({\n    'A': ['a', 'b', 'a', 'c'],\n    'B': [True, False, True, False]\n})\nany(df, list_var_groupby=['A'], list_var_of_interest=['B'])\n</code></pre></p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def any(\n    self,\n    data: pd.DataFrame,\n    list_var_groupby: Union[List[str], None],\n    list_var_of_interest: List[str],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Determine if any element in the given columns of a DataFrame is True, optionally grouped by specific columns.\n\n    Parameters:\n    - data (pd.DataFrame): Input DataFrame to check.\n    - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed.\n    - list_var_of_interest (List[str]): List of column names to check for any True values.\n\n    Returns:\n    - pd.DataFrame: A DataFrame indicating if any value is True. The output column names are appended with \"_any\".\n\n    Example:\n    ```\n    df = pd.DataFrame({\n        'A': ['a', 'b', 'a', 'c'],\n        'B': [True, False, True, False]\n    })\n    any(df, list_var_groupby=['A'], list_var_of_interest=['B'])\n    ```\n\n    \"\"\"\n    if list_var_groupby is not None:\n        return (\n            data.groupby(list_var_groupby, as_index=True, observed=True)[\n                list_var_of_interest\n            ]\n            .any()\n            .rename(\n                create_dict_suffix(list_name=list_var_of_interest, suffix=\"_any\"),\n                axis=1,\n            )\n        )\n    else:\n        return (\n            data[list_var_of_interest]\n            .any()\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(list_name=list_var_of_interest, suffix=\"_any\"),\n                axis=1,\n            )\n        )\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.count","title":"<code>count(data, list_var_groupby, list_var_of_interest)</code>","text":"<p>Count the number of non-missing values in the given columns of a DataFrame, optionally grouped by specific columns.</p> <p>Parameters: - data (pd.DataFrame): Input DataFrame to compute the count on. - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed. - list_var_of_interest (List[str]): List of column names for which the count is computed.</p> <p>Returns: - pd.DataFrame: A DataFrame containing the count. The output column names are appended with \"_count\".</p> <p>Example: <pre><code>df = pd.DataFrame({\n    'A': ['a', 'b', 'a', None],\n    'B': [1, 2, 3, 4]\n})\ncount(df, list_var_groupby=['A'], list_var_of_interest=['B'])\n</code></pre></p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def count(\n    self,\n    data: pd.DataFrame,\n    list_var_groupby: Union[List[str], None],\n    list_var_of_interest: List[str],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Count the number of non-missing values in the given columns of a DataFrame, optionally grouped by specific columns.\n\n    Parameters:\n    - data (pd.DataFrame): Input DataFrame to compute the count on.\n    - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed.\n    - list_var_of_interest (List[str]): List of column names for which the count is computed.\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing the count. The output column names are appended with \"_count\".\n\n    Example:\n    ```\n    df = pd.DataFrame({\n        'A': ['a', 'b', 'a', None],\n        'B': [1, 2, 3, 4]\n    })\n    count(df, list_var_groupby=['A'], list_var_of_interest=['B'])\n    ```\n\n    \"\"\"\n    if list_var_groupby is not None:\n        return (\n            data.groupby(list_var_groupby, as_index=True, observed=True)[\n                list_var_of_interest\n            ]\n            .count()\n            .rename(\n                create_dict_suffix(list_name=list_var_of_interest, suffix=\"_count\"),\n                axis=1,\n            )\n        )\n    else:\n        return (\n            data[list_var_of_interest]\n            .count()\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(list_name=list_var_of_interest, suffix=\"_count\"),\n                axis=1,\n            )\n        )\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.inf_threshold","title":"<code>inf_threshold(data, list_var_groupby, list_var_of_interest, var_threshold, seuil)</code>","text":"<p>Computes the proportion of unique values for the specified variables below a certain threshold.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.inf_threshold--parameters","title":"Parameters:","text":"<p>var_threshold : str     The variable based on which the thresholding will be done. seuil : int or float     The threshold value.</p> <p>Other parameters are the same as the mean method.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.inf_threshold--returns","title":"Returns:","text":"<p>pd.Series or pd.DataFrame     The proportion of unique values for the specified variables below the threshold.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def inf_threshold(\n    self,\n    data: pd.DataFrame,\n    list_var_groupby: Union[List[str], None],\n    list_var_of_interest: List[str],\n    var_threshold: str,\n    seuil: Union[int, float],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Computes the proportion of unique values for the specified variables below a certain threshold.\n\n    Parameters:\n    -----------\n    var_threshold : str\n        The variable based on which the thresholding will be done.\n    seuil : int or float\n        The threshold value.\n\n    Other parameters are the same as the mean method.\n\n    Returns:\n    --------\n    pd.Series or pd.DataFrame\n        The proportion of unique values for the specified variables below the threshold.\n    \"\"\"\n    # A terminer en autorisant plusieurs op\u00e9rations\n    if list_var_groupby is not None:\n        return (\n            data.loc[data[var_threshold] &lt; seuil]\n            .groupby(list_var_groupby)[list_var_of_interest]\n            .nunique()\n            / data.groupby(list_var_groupby)[list_var_of_interest].nunique()\n        ).rename(\n            create_dict_suffix(\n                list_name=list_var_of_interest, suffix=\"_inf_\" + str(seuil)\n            ),\n            axis=1,\n        )\n    else:\n        return (\n            (\n                data.loc[data[var_threshold] &lt; seuil][\n                    list_var_of_interest\n                ].nunique()\n                / data[list_var_of_interest].nunique()\n            )\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=\"_inf_\" + str(seuil)\n                ),\n                axis=1,\n            )\n        )\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.iterate_operations","title":"<code>iterate_operations(iterable_operations, data, list_var_groupby)</code>","text":"<p>Iteratively perform a set of operations on the data grouped by given variables.</p> <p>This function first creates a nomenclature with categorical variables and then, based on the type of operations provided, executes those operations on the dataset. It utilizes the internal methods of the class to which this function belongs, such as sum, mean, median, etc.</p> <p>Parameters: - iterable_operations (dict or list): Operations to be applied on the data. If it's a dictionary, the keys represent the operation and values represent the variables of interest. If it's a list, it only contains operations. - data (pd.DataFrame): Input dataset on which the operations need to be applied. - list_var_groupby (list of str): List of variables based on which the data needs to be grouped.</p> <p>Returns: - pd.DataFrame: Dataset after applying all the operations, indexed by the list_var_groupby.</p> <p>Raises: - ValueError: If the provided type for iterable_operations is neither dict nor list.</p> <p>Example: <pre><code># Example usage (assuming it's a method within a class)\nresult = StatDesGroupby.iterate_operations({'sum': ['var1', 'var2']}, data, ['group_var'])\n</code></pre></p> <p>Notes: - The function merges the resultant data with the data_nomenc to ensure the categorical variables are retained. - The internal operations like sum, mean, etc., are methods of the same class and are not global functions.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def iterate_operations(\n    self,\n    iterable_operations: Union[dict, List[str]],\n    data: pd.DataFrame,\n    list_var_groupby: List[str],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Iteratively perform a set of operations on the data grouped by given variables.\n\n    This function first creates a nomenclature with categorical variables and then, based on the type\n    of operations provided, executes those operations on the dataset. It utilizes the internal methods\n    of the class to which this function belongs, such as sum, mean, median, etc.\n\n    Parameters:\n    - iterable_operations (dict or list): Operations to be applied on the data. If it's a dictionary, the\n    keys represent the operation and values represent the variables of interest. If it's a list, it only\n    contains operations.\n    - data (pd.DataFrame): Input dataset on which the operations need to be applied.\n    - list_var_groupby (list of str): List of variables based on which the data needs to be grouped.\n\n    Returns:\n    - pd.DataFrame: Dataset after applying all the operations, indexed by the list_var_groupby.\n\n    Raises:\n    - ValueError: If the provided type for iterable_operations is neither dict nor list.\n\n    Example:\n    ```python\n    # Example usage (assuming it's a method within a class)\n    result = StatDesGroupby.iterate_operations({'sum': ['var1', 'var2']}, data, ['group_var'])\n    ```\n\n    Notes:\n    - The function merges the resultant data with the data_nomenc to ensure the categorical variables are retained.\n    - The internal operations like sum, mean, etc., are methods of the same class and are not global functions.\n    \"\"\"\n    # Cr\u00e9ation d'une nomenclature avec variables cat\u00e9gorielles\n    data_nomenc = data[list_var_groupby].drop_duplicates().reset_index()\n    # Conversion de la colonne en variable cat\u00e9gorielle\n    data_nomenc[\"index\"] = pd.Categorical(data_nomenc[\"index\"])\n    # Ajout au jeu de donn\u00e9es\n    data_work = pd.merge(\n        left=data,\n        right=data_nomenc,\n        how=\"left\",\n        on=list_var_groupby,\n        validate=\"many_to_one\",\n    ).drop(list_var_groupby, axis=1)\n\n    # Initialisation de la liste r\u00e9sultat\n    list_res = []\n    # Parcours des diff\u00e9rentes op\u00e9rations\n    # Faire avec agg pour limiter le nombre de groupby\n    if isinstance(iterable_operations, dict):\n        for operation, list_var_of_interest_work in iterable_operations.items():\n            if isinstance(operation, str):\n                if operation == \"sum\":\n                    list_res.append(\n                        self.sum(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif operation == \"mean\":\n                    list_res.append(\n                        self.mean(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif operation == \"median\":\n                    list_res.append(\n                        self.median(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif operation == \"nunique\":\n                    list_res.append(\n                        self.nunique(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                        )\n                    )\n                elif operation == \"count\":\n                    list_res.append(\n                        self.count(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                        )\n                    )\n                elif operation == \"any\":\n                    list_res.append(\n                        self.any(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                        )\n                    )\n                elif operation == \"all\":\n                    list_res.append(\n                        self.all(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                        )\n                    )\n                elif operation == \"majority\":\n                    list_res.append(\n                        self.majority(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif (\n                    (operation == \"count_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_res.append(\n                        self.nunique(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=[\n                                self.var_individu,\n                                self.var_entreprise,\n                            ],\n                        )\n                    )\n                elif (\n                    (operation == \"count_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is None)\n                ):\n                    list_res.append(\n                        self.nunique(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=[self.var_individu],\n                        )\n                    )\n                elif (\n                    (operation == \"count_effectif\")\n                    &amp; (self.var_individu is None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_res.append(\n                        self.nunique(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=[self.var_entreprise],\n                        )\n                    )\n                elif (\n                    (operation == \"max_sum_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_res.append(\n                        self.max_sum_effectif(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                            var_id=self.var_individu,\n                        )\n                    )\n                    list_res.append(\n                        self.max_sum_effectif(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                            var_id=self.var_entreprise,\n                        )\n                    )\n                elif (\n                    (operation == \"max_sum_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is None)\n                ):\n                    list_res.append(\n                        self.max_sum_effectif(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                            var_id=self.var_individu,\n                        )\n                    )\n                elif (\n                    (operation == \"max_sum_effectif\")\n                    &amp; (self.var_individu is None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_res.append(\n                        self.max_sum_effectif(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                            var_id=self.var_entreprise,\n                        )\n                    )\n            elif isinstance(operation, tuple):\n                # Le tuple est compos\u00e9 d'un string en premi\u00e8re position et d'un dictionnaire de param\u00e8tres en deuxi\u00e8me\n                if operation[0] == \"quantile\":\n                    list_res.append(\n                        self.quantile(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                            q=operation[1][\"q\"],\n                        )\n                    )\n                elif operation[0] == \"prop\":\n                    list_res.append(\n                        self.prop(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_weights=self.var_weights,\n                            var_ref=operation[1][\"var_ref\"],\n                        )\n                    )\n                elif operation[0] == \"inf_threshold\":\n                    list_res.append(\n                        self.inf_seuil(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=np.intersect1d(\n                                ar1=self.list_var_of_interest,\n                                ar2=list_var_of_interest_work,\n                            ).tolist(),\n                            var_threshold=operation[1][\"var_threshold\"],\n                            seuil=operation[1][\"threshold\"],\n                        )\n                    )\n    elif isinstance(iterable_operations, list):\n        for operation in iterable_operations:\n            if isinstance(operation, str):\n                if operation == \"sum\":\n                    list_res.append(\n                        self.sum(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif operation == \"mean\":\n                    list_res.append(\n                        self.mean(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif operation == \"median\":\n                    list_res.append(\n                        self.median(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif operation == \"nunique\":\n                    list_res.append(\n                        self.nunique(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                        )\n                    )\n                elif operation == \"count\":\n                    list_res.append(\n                        self.count(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                        )\n                    )\n                elif operation == \"any\":\n                    list_res.append(\n                        self.any(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                        )\n                    )\n                elif operation == \"all\":\n                    list_res.append(\n                        self.all(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                        )\n                    )\n                elif operation == \"majority\":\n                    list_res.append(\n                        self.majority(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                        )\n                    )\n                elif (\n                    (operation == \"count_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_res.append(\n                        self.nunique(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=[\n                                self.var_individu,\n                                self.var_entreprise,\n                            ],\n                        )\n                    )\n                elif (\n                    (operation == \"count_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is None)\n                ):\n                    list_res.append(\n                        self.nunique(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=[self.var_individu],\n                        )\n                    )\n                elif (\n                    (operation == \"count_effectif\")\n                    &amp; (self.var_individu is None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_res.append(\n                        self.nunique(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=[self.var_entreprise],\n                        )\n                    )\n                elif (\n                    (operation == \"max_sum_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_res.append(\n                        self.max_sum_effectif(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                            var_id=self.var_individu,\n                        )\n                    )\n                    list_res.append(\n                        self.max_sum_effectif(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                            var_id=self.var_entreprise,\n                        )\n                    )\n                elif (\n                    (operation == \"max_sum_effectif\")\n                    &amp; (self.var_individu is not None)\n                    &amp; (self.var_entreprise is None)\n                ):\n                    list_res.append(\n                        self.max_sum_effectif(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                            var_id=self.var_individu,\n                        )\n                    )\n                elif (\n                    (operation == \"max_sum_effectif\")\n                    &amp; (self.var_individu is None)\n                    &amp; (self.var_entreprise is not None)\n                ):\n                    list_res.append(\n                        self.max_sum_effectif(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                            var_id=self.var_entreprise,\n                        )\n                    )\n            elif isinstance(operation, tuple):\n                # Le tuple est compos\u00e9 d'un string en premi\u00e8re position et d'un dictionnaire de param\u00e8tres en deuxi\u00e8me\n                if operation[0] == \"quantile\":\n                    list_res.append(\n                        self.quantile(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                            q=operation[1][\"q\"],\n                        )\n                    )\n                elif operation[0] == \"prop\":\n                    list_res.append(\n                        self.prop(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_weights=self.var_weights,\n                            var_ref=operation[1][\"var_ref\"],\n                        )\n                    )\n                elif operation[0] == \"inf_threshold\":\n                    list_res.append(\n                        self.inf_seuil(\n                            data=data_work,\n                            list_var_groupby=[\"index\"],\n                            list_var_of_interest=self.list_var_of_interest,\n                            var_threshold=operation[1][\"var_threshold\"],\n                            seuil=operation[1][\"threshold\"],\n                        )\n                    )\n    else:\n        raise ValueError(\"Unsupported type for iterable_operations\")\n\n    # Construction du jeu de donn\u00e9es r\u00e9sultat\n    data_res = (\n        pd.merge(\n            left=pd.concat(list_res, axis=1, join=\"outer\").reset_index(),\n            right=data_nomenc,\n            how=\"left\",\n            on=\"index\",\n            validate=\"one_to_one\",\n        )\n        .drop(\"index\", axis=1)\n        .set_index(list_var_groupby)\n    )\n\n    return data_res\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.iterate_with_total","title":"<code>iterate_with_total(iterable_operations)</code>","text":"<p>Computes descriptive statistics and returns results with subtotals and a grand total.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.iterate_with_total--parameters","title":"Parameters:","text":"<p>iterable_operations : iterable     Operations or functions to apply to the grouped data.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.iterate_with_total--returns","title":"Returns:","text":"<p>DataFrame     Descriptive statistics with subtotals and a grand total.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def iterate_with_total(\n    self, iterable_operations: Union[dict, List[str]]\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Computes descriptive statistics and returns results with subtotals and a grand total.\n\n    Parameters:\n    -----------\n    iterable_operations : iterable\n        Operations or functions to apply to the grouped data.\n\n    Returns:\n    --------\n    DataFrame\n        Descriptive statistics with subtotals and a grand total.\n    \"\"\"\n    # Disjonction de cas suivant la longueur de la liste de groupby\n    if len(self.list_var_groupby) == 0:\n        data_res = self.add_total(iterable_operations=iterable_operations)\n    elif len(self.list_var_groupby) == 1:\n        # It\u00e9ration des statistiques descriptives\n        data_stat_des = self.iterate_operations(\n            iterable_operations=iterable_operations,\n            data=self.data_source,\n            list_var_groupby=self.list_var_groupby,\n        )\n        # It\u00e9ration du total\n        data_total = self.add_total(iterable_operations=iterable_operations)\n        # Concat\u00e9nation des jeux de donn\u00e9es\n        data_res = pd.concat([data_stat_des, data_total], axis=0, join=\"outer\")\n    else:\n        # It\u00e9ration des statistiques descriptives\n        data_stat_des = self.iterate_operations(\n            iterable_operations=iterable_operations,\n            data=self.data_source,\n            list_var_groupby=self.list_var_groupby,\n        )\n        # It\u00e9ration des sous-totaux\n        data_sub_total = self.add_under_total(\n            iterable_operations=iterable_operations\n        )\n        # It\u00e9ration du total\n        data_total = self.add_total(iterable_operations=iterable_operations)\n        # Concat\u00e9nation des jeux de donn\u00e9es\n        data_res = pd.concat(\n            [data_stat_des, data_sub_total, data_total], axis=0, join=\"outer\"\n        )\n\n    # Tri de l'indice\n    data_res = _sort_index_with_total(data_source=data_res)\n\n    return data_res\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.iterate_without_total","title":"<code>iterate_without_total(iterable_operations)</code>","text":"<p>Computes descriptive statistics without subtotals or a grand total and returns results.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.iterate_without_total--parameters","title":"Parameters:","text":"<p>iterable_operations : iterable     Operations or functions to apply to the grouped data.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.iterate_without_total--returns","title":"Returns:","text":"<p>DataFrame     Descriptive statistics without subtotals or a grand total.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def iterate_without_total(\n    self, iterable_operations: Union[dict, List[str]]\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Computes descriptive statistics without subtotals or a grand total and returns results.\n\n    Parameters:\n    -----------\n    iterable_operations : iterable\n        Operations or functions to apply to the grouped data.\n\n    Returns:\n    --------\n    DataFrame\n        Descriptive statistics without subtotals or a grand total.\n    \"\"\"\n    return self.iterate_operations(\n        iterable_operations=iterable_operations,\n        data=self.data_source,\n        list_var_groupby=self.list_var_groupby,\n    ).sort_index()\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.majority","title":"<code>majority(data, list_var_groupby, list_var_of_interest, var_weights)</code>","text":"<p>Determine the majority value for the specified variables, optionally weighted and grouped.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>The data frame containing the data.</p> required <code>list_var_groupby</code> <code>list or None</code> <p>The variables to group by. If None, no grouping is performed.</p> required <code>list_var_of_interest</code> <code>list</code> <p>The variables for which to determine the majority value.</p> required <code>var_weights</code> <code>str or None</code> <p>The variable to use for weighting. If None, no weighting is applied.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A data frame with the majority value for each specified variable of interest, optionally grouped and weighted.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def majority(\n    self,\n    data: pd.DataFrame,\n    list_var_groupby: Union[List[str], None],\n    list_var_of_interest: List[str],\n    var_weights: Union[str, None],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Determine the majority value for the specified variables, optionally weighted and grouped.\n\n    Parameters:\n        data (pd.DataFrame): The data frame containing the data.\n        list_var_groupby (list or None): The variables to group by. If None, no grouping is performed.\n        list_var_of_interest (list): The variables for which to determine the majority value.\n        var_weights (str or None): The variable to use for weighting. If None, no weighting is applied.\n\n    Returns:\n        pd.DataFrame: A data frame with the majority value for each specified variable of interest, optionally grouped and weighted.\n    \"\"\"\n    if (list_var_groupby is not None) &amp; (var_weights is not None):\n        return pd.concat(\n            [\n                data.groupby(list_var_groupby)[[var_of_interest, var_weights]]\n                .apply(\n                    func=lambda x: x.groupby(var_of_interest)[var_weights]\n                    .sum()\n                    .idxmax()\n                )\n                .to_frame()\n                .rename({0: var_of_interest}, axis=1)\n                for var_of_interest in list_var_of_interest\n            ],\n            axis=1,\n            join=\"outer\",\n        ).rename(\n            create_dict_suffix(list_name=list_var_of_interest, suffix=\"_majority\"),\n            axis=1,\n        )\n    elif (list_var_groupby is not None) &amp; (var_weights is None):\n        return pd.concat(\n            [\n                data.groupby(list_var_groupby)[[var_of_interest]]\n                .apply(func=lambda x: x[var_of_interest].value_counts().idxmax())\n                .to_frame()\n                .rename({0: var_of_interest}, axis=1)\n                for var_of_interest in list_var_of_interest\n            ],\n            axis=1,\n            join=\"outer\",\n        ).rename(\n            create_dict_suffix(list_name=list_var_of_interest, suffix=\"_majority\"),\n            axis=1,\n        )\n    elif (list_var_groupby is None) &amp; (var_weights is not None):\n        return (\n            pd.Series(\n                [\n                    data.groupby(var_of_interest)[var_weights].sum().idxmax()\n                    for var_of_interest in list_var_of_interest\n                ],\n                index=list_var_of_interest,\n            )\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=\"_majority\"\n                ),\n                axis=1,\n            )\n        )\n    elif (list_var_groupby is None) &amp; (var_weights is None):\n        return (\n            pd.Series(\n                [\n                    data[var_of_interest].value_counts().idxmax()\n                    for var_of_interest in list_var_of_interest\n                ],\n                index=list_var_of_interest,\n            )\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=\"_majority\"\n                ),\n                axis=1,\n            )\n        )\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.max_sum_effectif","title":"<code>max_sum_effectif(data, list_var_groupby, list_var_of_interest, var_weights, var_id)</code>","text":"<p>Computes the maximum proportion of a variable relative to its sum, possibly grouped by specified variables.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.max_sum_effectif--parameters","title":"Parameters:","text":"<p>var_id : str     The identifier for renaming the resulting variables.</p> <p>Other parameters are the same as the mean method.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.max_sum_effectif--returns","title":"Returns:","text":"<p>pd.Series or pd.DataFrame     The computed maximum proportions for the specified variables relative to their sum.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def max_sum_effectif(\n    self,\n    data: pd.DataFrame,\n    list_var_groupby: Union[List[str], None],\n    list_var_of_interest: List[str],\n    var_weights: Union[str, None],\n    var_id: str,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Computes the maximum proportion of a variable relative to its sum, possibly grouped by specified variables.\n\n    Parameters:\n    -----------\n    var_id : str\n        The identifier for renaming the resulting variables.\n\n    Other parameters are the same as the mean method.\n\n    Returns:\n    --------\n    pd.Series or pd.DataFrame\n        The computed maximum proportions for the specified variables relative to their sum.\n    \"\"\"\n    if (list_var_groupby is not None) &amp; (var_weights is not None):\n        data_pond = create_pond_data(\n            data=data,\n            list_var_of_interest=list_var_of_interest,\n            list_var_groupby=list_var_groupby,\n            var_weights=var_weights,\n        )\n        return (\n            data_pond.groupby(list_var_groupby)\n            .max()\n            .divide(other=data_pond.groupby(list_var_groupby).sum())\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=f\"_{var_id}_max/sum\"\n                ),\n                axis=1,\n            )\n        )\n    elif (list_var_groupby is not None) &amp; (var_weights is None):\n        return (\n            data.groupby(list_var_groupby)[list_var_of_interest]\n            .max()\n            .divide(data.groupby(list_var_groupby)[list_var_of_interest].sum())\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=f\"_{var_id}_max/sum\"\n                ),\n                axis=1,\n            )\n        )\n    elif (list_var_groupby is None) &amp; (var_weights is not None):\n        data_pond = create_pond_data(\n            data=data,\n            list_var_of_interest=list_var_of_interest,\n            list_var_groupby=list_var_groupby,\n            var_weights=var_weights,\n        )\n        return (\n            data_pond.max()\n            .divide(other=data_pond.sum())\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=f\"_{var_id}_max/sum\"\n                ),\n                axis=1,\n            )\n        )\n    elif (list_var_groupby is None) &amp; (var_weights is None):\n        return (\n            data[list_var_of_interest]\n            .max()\n            .divide(other=data[list_var_of_interest].sum())\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=f\"_{var_id}_max/sum\"\n                ),\n                axis=1,\n            )\n        )\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.mean","title":"<code>mean(data, list_var_groupby, list_var_of_interest, var_weights)</code>","text":"<p>Computes the weighted or unweighted mean of the specified variables, possibly grouped by specified variables.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.mean--parameters","title":"Parameters:","text":"<p>data : pd.DataFrame     The dataset containing the data to be processed. list_var_groupby : list of str, optional     List of variable names to group by. list_var_of_interest : list of str     List of variable names to compute the mean for. var_weights : str, optional     Name of the column containing weights for weighted computation.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.mean--returns","title":"Returns:","text":"<p>pd.Series or pd.DataFrame : The computed mean for the specified variables.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def mean(\n    self,\n    data: pd.DataFrame,\n    list_var_groupby: Union[List[str], None],\n    list_var_of_interest: List[str],\n    var_weights: Union[str, None],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Computes the weighted or unweighted mean of the specified variables,\n    possibly grouped by specified variables.\n\n    Parameters:\n    -----------\n    data : pd.DataFrame\n        The dataset containing the data to be processed.\n    list_var_groupby : list of str, optional\n        List of variable names to group by.\n    list_var_of_interest : list of str\n        List of variable names to compute the mean for.\n    var_weights : str, optional\n        Name of the column containing weights for weighted computation.\n\n    Returns:\n    --------\n    pd.Series or pd.DataFrame : The computed mean for the specified variables.\n    \"\"\"\n    if (list_var_groupby is not None) &amp; (var_weights is not None):\n        data_pond = create_pond_data(\n            data=data,\n            list_var_of_interest=list_var_of_interest,\n            list_var_groupby=list_var_groupby,\n            var_weights=var_weights,\n        )\n        return (\n            data_pond.groupby(list_var_groupby, as_index=True, observed=True)[\n                list_var_of_interest\n            ]\n            .sum()\n            .divide(\n                other=data.groupby(list_var_groupby, as_index=True, observed=True)[\n                    var_weights\n                ].sum(),\n                axis=0,\n            )\n            .rename(\n                create_dict_suffix(list_name=list_var_of_interest, suffix=\"_mean\"),\n                axis=1,\n            )\n        )\n    elif (list_var_groupby is not None) &amp; (var_weights is None):\n        return (\n            data.groupby(list_var_groupby, as_index=True, observed=True)[\n                list_var_of_interest\n            ]\n            .mean()\n            .rename(\n                create_dict_suffix(list_name=list_var_of_interest, suffix=\"_mean\"),\n                axis=1,\n            )\n        )\n    elif (list_var_groupby is None) &amp; (var_weights is not None):\n        data_pond = create_pond_data(\n            data=data,\n            list_var_of_interest=list_var_of_interest,\n            list_var_groupby=list_var_groupby,\n            var_weights=var_weights,\n        )\n        return (\n            data_pond.sum()\n            .divide(other=data[var_weights].sum())\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(list_name=list_var_of_interest, suffix=\"_mean\"),\n                axis=1,\n            )\n        )\n    elif (list_var_groupby is None) &amp; (var_weights is None):\n        return (\n            data[list_var_of_interest]\n            .mean()\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(list_name=list_var_of_interest, suffix=\"_mean\"),\n                axis=1,\n            )\n        )\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.median","title":"<code>median(data, list_var_groupby, list_var_of_interest, var_weights)</code>","text":"<p>Computes the median value for the specified variables, possibly grouped by specified variables.</p> <p>Parameters are the same as the mean method.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.median--returns","title":"Returns:","text":"<p>pd.Series or pd.DataFrame     The median values for the specified variables.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def median(\n    self,\n    data: pd.DataFrame,\n    list_var_groupby: Union[List[str], None],\n    list_var_of_interest: List[str],\n    var_weights: Union[str, None],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Computes the median value for the specified variables,\n    possibly grouped by specified variables.\n\n    Parameters are the same as the mean method.\n\n    Returns:\n    --------\n    pd.Series or pd.DataFrame\n        The median values for the specified variables.\n    \"\"\"\n    return self.quantile(\n        data=data,\n        list_var_groupby=list_var_groupby,\n        list_var_of_interest=list_var_of_interest,\n        var_weights=var_weights,\n        q=0.5,\n    )\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.nunique","title":"<code>nunique(data, list_var_groupby, list_var_of_interest)</code>","text":"<p>Calculate the number of unique values in the given columns of a DataFrame, optionally grouped by specific columns.</p> <p>Parameters: - data (pd.DataFrame): Input DataFrame to compute the number of unique values on. - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed. - list_var_of_interest (List[str]): List of column names for which the number of unique values is computed.</p> <p>Returns: - pd.DataFrame: A DataFrame containing the count of unique values. The output column names are appended with \"_nunique\".</p> <p>Example: <pre><code>df = pd.DataFrame({\n    'A': ['a', 'b', 'a', 'c'],\n    'B': [1, 2, 3, 4]\n})\nnunique(df, list_var_groupby=['A'], list_var_of_interest=['B'])\n</code></pre></p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def nunique(\n    self,\n    data: pd.DataFrame,\n    list_var_groupby: Union[List[str], None],\n    list_var_of_interest: List[str],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Calculate the number of unique values in the given columns of a DataFrame, optionally grouped by specific columns.\n\n    Parameters:\n    - data (pd.DataFrame): Input DataFrame to compute the number of unique values on.\n    - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed.\n    - list_var_of_interest (List[str]): List of column names for which the number of unique values is computed.\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing the count of unique values. The output column names are appended with \"_nunique\".\n\n    Example:\n    ```\n    df = pd.DataFrame({\n        'A': ['a', 'b', 'a', 'c'],\n        'B': [1, 2, 3, 4]\n    })\n    nunique(df, list_var_groupby=['A'], list_var_of_interest=['B'])\n    ```\n\n    \"\"\"\n    if list_var_groupby is not None:\n        return (\n            data.groupby(list_var_groupby, as_index=True, observed=True)[\n                list_var_of_interest\n            ]\n            .nunique()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=\"_nunique\"\n                ),\n                axis=1,\n            )\n        )\n    else:\n        return (\n            data[list_var_of_interest]\n            .nunique()\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=\"_nunique\"\n                ),\n                axis=1,\n            )\n        )\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.prop","title":"<code>prop(data, list_var_groupby, list_var_of_interest, var_weights, var_ref)</code>","text":"<p>Computes the proportion of the specified variables based on a reference variable, possibly grouped by specified variables.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.prop--parameters","title":"Parameters:","text":"<p>var_ref : str     The reference variable used to compute the proportion.</p> <p>Other parameters are the same as the mean method.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.prop--returns","title":"Returns:","text":"<p>pd.Series or pd.DataFrame     The computed proportions for the specified variables.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def prop(\n    self,\n    data: pd.DataFrame,\n    list_var_groupby: Union[List[str], None],\n    list_var_of_interest: List[str],\n    var_weights: Union[str, None],\n    var_ref: str,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Computes the proportion of the specified variables based on a reference variable,\n    possibly grouped by specified variables.\n\n    Parameters:\n    -----------\n    var_ref : str\n        The reference variable used to compute the proportion.\n\n    Other parameters are the same as the mean method.\n\n    Returns:\n    --------\n    pd.Series or pd.DataFrame\n        The computed proportions for the specified variables.\n    \"\"\"\n    if (list_var_groupby is not None) &amp; (var_weights is not None):\n        data_pond = create_pond_data(\n            data=data,\n            list_var_of_interest=list_var_of_interest,\n            list_var_groupby=list_var_groupby,\n            var_weights=var_weights,\n        )\n        data_pond_ref = create_pond_data(\n            data=data,\n            list_var_of_interest=[var_ref],\n            list_var_groupby=list_var_groupby,\n            var_weights=var_weights,\n        )\n        return (\n            data_pond.groupby(list_var_groupby)\n            .sum()\n            .divide(data_pond_ref.groupby(list_var_groupby)[var_ref].sum(), axis=0)\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=f\"_{var_ref}_prop\"\n                ),\n                axis=1,\n            )\n        )\n    elif (list_var_groupby is not None) &amp; (var_weights is None):\n        return (\n            data.groupby(list_var_groupby)[list_var_of_interest]\n            .sum()\n            .divide(other=data.groupby(list_var_groupby)[var_ref].sum(), axis=0)\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=f\"_{var_ref}_prop\"\n                ),\n                axis=1,\n            )\n        )\n    elif (list_var_groupby is None) &amp; (var_weights is not None):\n        data_pond = create_pond_data(\n            data=data,\n            list_var_of_interest=list_var_of_interest,\n            list_var_groupby=list_var_groupby,\n            var_weights=var_weights,\n        )\n        data_pond_ref = create_pond_data(\n            data=data,\n            list_var_of_interest=[var_ref],\n            list_var_groupby=list_var_groupby,\n            var_weights=var_weights,\n        )\n        return (\n            data_pond.sum()\n            .divide(data_pond_ref[var_ref].sum())\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=f\"_{var_ref}_prop\"\n                ),\n                axis=1,\n            )\n        )\n    elif (list_var_groupby is None) &amp; (var_weights is None):\n        data_res = (\n            (data[list_var_of_interest].sum() / data[var_ref].sum())\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=f\"_{var_ref}_prop\"\n                ),\n                axis=1,\n            )\n        )\n        return data_res\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.quantile","title":"<code>quantile(data, list_var_groupby, list_var_of_interest, var_weights, q)</code>","text":"<p>Computes the q-th quantile for the specified variables, possibly grouped by specified variables.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.quantile--parameters","title":"Parameters:","text":"<p>q : float     Quantile to compute, which must be between 0 and 1 inclusive.</p> <p>Other parameters are the same as the mean method.</p>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.quantile--returns","title":"Returns:","text":"<p>pd.Series or pd.DataFrame     The q-th quantile for the specified variables.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def quantile(\n    self,\n    data: pd.DataFrame,\n    list_var_groupby: Union[List[str], None],\n    list_var_of_interest: List[str],\n    var_weights: Union[str, None],\n    q: float,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Computes the q-th quantile for the specified variables,\n    possibly grouped by specified variables.\n\n    Parameters:\n    -----------\n    q : float\n        Quantile to compute, which must be between 0 and 1 inclusive.\n\n    Other parameters are the same as the mean method.\n\n    Returns:\n    --------\n    pd.Series or pd.DataFrame\n        The q-th quantile for the specified variables.\n    \"\"\"\n    if (list_var_groupby is not None) &amp; (var_weights is not None):\n        return (\n            data.groupby(list_var_groupby, as_index=True, observed=True)[\n                list_var_of_interest + [var_weights]\n            ]\n            .apply(\n                func=lambda x: weighted_quantile(\n                    data=x,\n                    vars_of_interest=list_var_of_interest,\n                    var_weights=var_weights,\n                    q=q,\n                )\n            )\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=\"_q\" + str(q)\n                ),\n                axis=1,\n            )\n        )\n    elif (list_var_groupby is not None) &amp; (var_weights is None):\n        return (\n            data.groupby(list_var_groupby, as_index=True, observed=True)[\n                list_var_of_interest\n            ]\n            .quantile(q=q)\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=\"_q\" + str(q)\n                ),\n                axis=1,\n            )\n        )\n    elif (list_var_groupby is None) &amp; (var_weights is not None):\n        return (\n            weighted_quantile(\n                data=data,\n                vars_of_interest=list_var_of_interest,\n                var_weights=var_weights,\n                q=q,\n            )\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=\"_q\" + str(q)\n                ),\n                axis=1,\n            )\n        )\n    elif (list_var_groupby is None) &amp; (var_weights is None):\n        return (\n            data[list_var_of_interest]\n            .quantile(q=q)\n            .rename(0)\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest, suffix=\"_q\" + str(q)\n                ),\n                axis=1,\n            )\n        )\n</code></pre>"},{"location":"api/StatDesGroupBy/#igf_toolbox.stats_des.base.StatDesGroupBy.sum","title":"<code>sum(data, list_var_groupby, list_var_of_interest, var_weights)</code>","text":"<p>Compute the sum aggregation on a given pandas DataFrame, with options for weighted sum.</p> <p>Parameters: - data (pd.DataFrame): The input DataFrame to compute aggregation on. - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed. - list_var_of_interest (List[str]): List of column names on which the aggregation is performed. - var_weights (str, optional): Column name representing the weights for weighted sum.                             If it is in list_var_of_interest, the function will compute both weighted                             and non-weighted sum. If None, a simple sum is performed.</p> <ul> <li>pd.DataFrame: A DataFrame containing the aggregated data. The output DataFrame will have a multi-index if                 list_var_groupby is provided and more than one type of aggregation (weighted and non-weighted)                 is performed. The column names in the output DataFrame will be appended with suffixes like \"_sum\".</li> </ul> <p>Notes: - The function uses helper functions like <code>create_pond_data</code> and <code>create_dict_suffix</code> which should be present in the same context. - It handles different combinations of list_var_groupby and var_weights, giving flexibility in the type of aggregation performed.</p> <p>Examples: <pre><code>df = pd.DataFrame({\n    'A': [1, 2, 3, 4],\n    'B': [10, 20, 30, 40],\n    'C': [100, 200, 300, 400],\n    'weights': [0.1, 0.2, 0.3, 0.4]\n})\n\nsum(df, list_var_groupby=['A'], list_var_of_interest=['B', 'C'], var_weights='weights')\n</code></pre></p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def sum(\n    self,\n    data: pd.DataFrame,\n    list_var_groupby: Union[List[str], None],\n    list_var_of_interest: List[str],\n    var_weights: Union[str, None],\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Compute the sum aggregation on a given pandas DataFrame, with options for weighted sum.\n\n    Parameters:\n    - data (pd.DataFrame): The input DataFrame to compute aggregation on.\n    - list_var_groupby (List[str], optional): List of column names to group by. If None, no grouping is performed.\n    - list_var_of_interest (List[str]): List of column names on which the aggregation is performed.\n    - var_weights (str, optional): Column name representing the weights for weighted sum.\n                                If it is in list_var_of_interest, the function will compute both weighted\n                                and non-weighted sum. If None, a simple sum is performed.\n\n    Returns:\n    - pd.DataFrame: A DataFrame containing the aggregated data. The output DataFrame will have a multi-index if\n                    list_var_groupby is provided and more than one type of aggregation (weighted and non-weighted)\n                    is performed. The column names in the output DataFrame will be appended with suffixes like \"_sum\".\n\n    Notes:\n    - The function uses helper functions like `create_pond_data` and `create_dict_suffix` which should be present in the\n    same context.\n    - It handles different combinations of list_var_groupby and var_weights, giving flexibility in the type of aggregation\n    performed.\n\n    Examples:\n    ```\n    df = pd.DataFrame({\n        'A': [1, 2, 3, 4],\n        'B': [10, 20, 30, 40],\n        'C': [100, 200, 300, 400],\n        'weights': [0.1, 0.2, 0.3, 0.4]\n    })\n\n    sum(df, list_var_groupby=['A'], list_var_of_interest=['B', 'C'], var_weights='weights')\n    ```\n\n    \"\"\"\n    # Initialisation de la liste \u00e0 retourner\n    list_data_return = []\n\n    # Calcul de la somme des poids non pond\u00e9r\u00e9e et suppression de la liste des variables d'int\u00e9r\u00eat\n    # Sans doute faire la m\u00eame chose pour les autres op\u00e9rations\n    # L'inconv\u00e9nient c'est qu'on ne peut plus faire de somme pond\u00e9r\u00e9e des poids (alors qu'avant on pouvait d\u00e9j\u00e0 le faire avec var_weights=None)\n    if (var_weights is not None) &amp; (var_weights in list_var_of_interest):\n        # Ajout de la somme des poids\n        if list_var_groupby is not None:\n            list_data_return.append(\n                data.groupby(list_var_groupby, as_index=True, observed=True)[\n                    [var_weights]\n                ]\n                .sum()\n                .rename(\n                    create_dict_suffix(list_name=[var_weights], suffix=\"_sum\"),\n                    axis=1,\n                )\n            )\n        else:\n            list_data_return.append(\n                data[[var_weights]]\n                .sum()\n                .to_frame()\n                .transpose()\n                .rename(\n                    create_dict_suffix(list_name=[var_weights], suffix=\"_sum\"),\n                    axis=1,\n                )\n            )\n\n        # Initialisation de la liste de travail sans les poids\n        list_var_of_interest_work = [\n            var_of_interest\n            for var_of_interest in list_var_of_interest\n            if var_of_interest != var_weights\n        ]\n    else:\n        list_var_of_interest_work = list_var_of_interest\n\n    if (\n        (len(list_var_of_interest_work) &gt; 0)\n        &amp; (list_var_groupby is not None)\n        &amp; (var_weights is not None)\n    ):\n        data_pond = create_pond_data(\n            data=data,\n            list_var_of_interest=list_var_of_interest_work,\n            list_var_groupby=list_var_groupby,\n            var_weights=var_weights,\n        )\n        list_data_return.append(\n            data_pond.groupby(list_var_groupby, as_index=True, observed=True)[\n                list_var_of_interest_work\n            ]\n            .sum()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest_work, suffix=\"_sum\"\n                ),\n                axis=1,\n            )\n        )\n    elif (\n        (len(list_var_of_interest_work) &gt; 0)\n        &amp; (list_var_groupby is not None)\n        &amp; (var_weights is None)\n    ):\n        list_data_return.append(\n            data.groupby(list_var_groupby, as_index=True, observed=True)[\n                list_var_of_interest_work\n            ]\n            .sum()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest_work, suffix=\"_sum\"\n                ),\n                axis=1,\n            )\n        )\n    elif (\n        (len(list_var_of_interest_work) &gt; 0)\n        &amp; (list_var_groupby is None)\n        &amp; (var_weights is not None)\n    ):\n        data_pond = create_pond_data(\n            data=data,\n            list_var_of_interest=list_var_of_interest_work,\n            list_var_groupby=list_var_groupby,\n            var_weights=var_weights,\n        )\n        list_data_return.append(\n            data_pond.sum()\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest_work, suffix=\"_sum\"\n                ),\n                axis=1,\n            )\n        )\n    elif (\n        (len(list_var_of_interest_work) &gt; 0)\n        &amp; (list_var_groupby is None)\n        &amp; (var_weights is None)\n    ):\n        list_data_return.append(\n            data[list_var_of_interest_work]\n            .sum()\n            .to_frame()\n            .transpose()\n            .rename(\n                create_dict_suffix(\n                    list_name=list_var_of_interest_work, suffix=\"_sum\"\n                ),\n                axis=1,\n            )\n        )\n\n    if list_var_groupby is not None:\n        return pd.concat(list_data_return, axis=1, join=\"outer\")\n    else:\n        return pd.concat(list_data_return, axis=0, join=\"outer\")\n</code></pre>"},{"location":"api/ThresholdExcluder/","title":"ThresholdExcluder","text":"<p>               Bases: <code>TransformerMixin</code>, <code>BaseEstimator</code></p> <p>Exclude observations from a DataFrame based on threshold criteria.</p> <p>This transformer filters rows of a DataFrame based on specified column threshold values. Multiple criteria can be applied simultaneously, and rows which don't satisfy all the conditions will be excluded from the result.</p> <ul> <li>list_dict_params (List[Dict]): A list of dictionaries specifying the exclusion criteria.     Each dictionary should contain:<ul> <li>'variable': The column on which to apply the criterion.</li> <li>'operator': The comparison operator, which can be one of the following: '&gt;', '&gt;=', '&lt;', '&lt;=', '!='.</li> <li>'threshold': The value to compare against.</li> </ul> </li> <li>drop (bool) : A boolean indicating whether to drop or fill with np.nan excluded observations</li> </ul> <p>Methods: - fit(X, y=None): Returns self. - transform(X, y=None): Exclude observations based on the criteria.</p> <p>Example:</p> <p>excluder = ThresholdExcluder([{'variable': 'A', 'operator': '&gt;', 'threshold': 5},                               {'variable': 'B', 'operator': '&lt;=', 'threshold': 10}]) df = pd.DataFrame({'A': [1, 6, 3, 7], 'B': [5, 10, 20, 8]}) df_transformed = excluder.transform(df)</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>class ThresholdExcluder(TransformerMixin, BaseEstimator):\n    \"\"\"\n    Exclude observations from a DataFrame based on threshold criteria.\n\n    This transformer filters rows of a DataFrame based on specified column threshold values.\n    Multiple criteria can be applied simultaneously, and rows which don't satisfy all the conditions\n    will be excluded from the result.\n\n    Attributes:\n    - list_dict_params (List[Dict]): A list of dictionaries specifying the exclusion criteria.\n        Each dictionary should contain:\n        - 'variable': The column on which to apply the criterion.\n        - 'operator': The comparison operator, which can be one of the following: '&gt;', '&gt;=', '&lt;', '&lt;=', '!='.\n        - 'threshold': The value to compare against.\n    - drop (bool) : A boolean indicating whether to drop or fill with np.nan excluded observations\n\n    Methods:\n    - fit(X, y=None): Returns self.\n    - transform(X, y=None): Exclude observations based on the criteria.\n\n    Example:\n    &gt;&gt;&gt; excluder = ThresholdExcluder([{'variable': 'A', 'operator': '&gt;', 'threshold': 5},\n    &gt;&gt;&gt;                               {'variable': 'B', 'operator': '&lt;=', 'threshold': 10}])\n    &gt;&gt;&gt; df = pd.DataFrame({'A': [1, 6, 3, 7], 'B': [5, 10, 20, 8]})\n    &gt;&gt;&gt; df_transformed = excluder.transform(df)\n    \"\"\"\n\n    def __init__(self, list_dict_params: List[Dict], drop: Optional[bool] = True):\n        \"\"\"\n        Initialize the ThresholdExcluder.\n\n        Parameters:\n        - list_dict_params (List[Dict]): A list of dictionaries specifying the exclusion criteria.\n        \"\"\"\n        # Initialisation de la liste du dictionnaire de param\u00e8tres\n        self.list_dict_params = list_dict_params\n\n    def fit(self, X, y=None) -&gt; None:\n        \"\"\"\n        Return self.\n\n        The fit method is implemented for compatibility with sklearn's TransformerMixin,\n        but doesn't perform any actual computation.\n\n        Parameters:\n        - X (pd.DataFrame): The input data. Not used, only needed for compatibility.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - self: The instance itself.\n        \"\"\"\n        return self\n\n    def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n        \"\"\"\n        Exclude observations based on the specified criteria.\n\n        Parameters:\n        - X (pd.DataFrame): The input data to transform.\n        - y (ignored): This parameter is ignored.\n\n        Returns:\n        - pd.DataFrame: The transformed data with observations not meeting the criteria excluded.\n        \"\"\"\n        # Disjonction suivant la suppression\n        if self.drop:\n            # Initialisation de la s\u00e9rie de bool\u00e9ens\n            list_data_boolean = []\n\n            # Parcours des dictionnaires de param\u00e8tres\n            for dict_params in self.list_dict_params:\n                if dict_params[\"operator\"] == \"&gt;\":\n                    list_data_boolean.append(\n                        (\n                            X[dict_params[\"variable\"]] &gt; dict_params[\"threshold\"]\n                        ).to_frame()\n                    )\n                elif dict_params[\"operator\"] == \"&gt;=\":\n                    list_data_boolean.append(\n                        (\n                            X[dict_params[\"variable\"]] &gt;= dict_params[\"threshold\"]\n                        ).to_frame()\n                    )\n                elif dict_params[\"operator\"] == \"&lt;\":\n                    list_data_boolean.append(\n                        (\n                            X[dict_params[\"variable\"]] &lt; dict_params[\"threshold\"]\n                        ).to_frame()\n                    )\n                elif dict_params[\"operator\"] == \"&lt;=\":\n                    list_data_boolean.append(\n                        (\n                            X[dict_params[\"variable\"]] &lt;= dict_params[\"threshold\"]\n                        ).to_frame()\n                    )\n                elif dict_params[\"operator\"] == \"!=\":\n                    list_data_boolean.append(\n                        (\n                            X[dict_params[\"variable\"]] != dict_params[\"threshold\"]\n                        ).to_frame()\n                    )\n            # Concat\u00e9nation des s\u00e9ries bool\u00e9ennes\n            data_boolean = pd.concat(list_data_boolean, axis=1, join=\"outer\")\n\n            # Restriction aux observations respectant tous les seuils\n            X_transformed = X.loc[data_boolean.all(axis=1)]\n        else:\n            # Copie ind\u00e9pendante du jeu de donn\u00e9es\n            X_transformed = X.copy()\n            # Parcours des dictionnaires de param\u00e8tres\n            for dict_params in self.list_dict_params:\n                if dict_params[\"operator\"] == \"&gt;\":\n                    X_transformed.loc[\n                        (X[dict_params[\"variable\"]] &gt; dict_params[\"threshold\"]),\n                        dict_params[\"variable\"],\n                    ] = np.nan\n                elif dict_params[\"operator\"] == \"&gt;=\":\n                    X_transformed.loc[\n                        (X[dict_params[\"variable\"]] &gt;= dict_params[\"threshold\"]),\n                        dict_params[\"variable\"],\n                    ] = np.nan\n                elif dict_params[\"operator\"] == \"&lt;\":\n                    X_transformed.loc[\n                        (X[dict_params[\"variable\"]] &lt; dict_params[\"threshold\"]),\n                        dict_params[\"variable\"],\n                    ] = np.nan\n                elif dict_params[\"operator\"] == \"&lt;=\":\n                    X_transformed.loc[\n                        (X[dict_params[\"variable\"]] &lt;= dict_params[\"threshold\"]),\n                        dict_params[\"variable\"],\n                    ] = np.nan\n                elif dict_params[\"operator\"] == \"!=\":\n                    X_transformed.loc[\n                        (X[dict_params[\"variable\"]] != dict_params[\"threshold\"]),\n                        dict_params[\"variable\"],\n                    ] = np.nan\n\n        return X_transformed\n</code></pre>"},{"location":"api/ThresholdExcluder/#igf_toolbox.preprocessing.excluders.ThresholdExcluder.__init__","title":"<code>__init__(list_dict_params, drop=True)</code>","text":"<p>Initialize the ThresholdExcluder.</p> <p>Parameters: - list_dict_params (List[Dict]): A list of dictionaries specifying the exclusion criteria.</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>def __init__(self, list_dict_params: List[Dict], drop: Optional[bool] = True):\n    \"\"\"\n    Initialize the ThresholdExcluder.\n\n    Parameters:\n    - list_dict_params (List[Dict]): A list of dictionaries specifying the exclusion criteria.\n    \"\"\"\n    # Initialisation de la liste du dictionnaire de param\u00e8tres\n    self.list_dict_params = list_dict_params\n</code></pre>"},{"location":"api/ThresholdExcluder/#igf_toolbox.preprocessing.excluders.ThresholdExcluder.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Return self.</p> <p>The fit method is implemented for compatibility with sklearn's TransformerMixin, but doesn't perform any actual computation.</p> <p>Parameters: - X (pd.DataFrame): The input data. Not used, only needed for compatibility. - y (ignored): This parameter is ignored.</p> <p>Returns: - self: The instance itself.</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>def fit(self, X, y=None) -&gt; None:\n    \"\"\"\n    Return self.\n\n    The fit method is implemented for compatibility with sklearn's TransformerMixin,\n    but doesn't perform any actual computation.\n\n    Parameters:\n    - X (pd.DataFrame): The input data. Not used, only needed for compatibility.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - self: The instance itself.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"api/ThresholdExcluder/#igf_toolbox.preprocessing.excluders.ThresholdExcluder.transform","title":"<code>transform(X, y=None)</code>","text":"<p>Exclude observations based on the specified criteria.</p> <p>Parameters: - X (pd.DataFrame): The input data to transform. - y (ignored): This parameter is ignored.</p> <p>Returns: - pd.DataFrame: The transformed data with observations not meeting the criteria excluded.</p> Source code in <code>igf_toolbox/preprocessing/excluders.py</code> <pre><code>def transform(self, X: pd.DataFrame, y=None) -&gt; pd.DataFrame:\n    \"\"\"\n    Exclude observations based on the specified criteria.\n\n    Parameters:\n    - X (pd.DataFrame): The input data to transform.\n    - y (ignored): This parameter is ignored.\n\n    Returns:\n    - pd.DataFrame: The transformed data with observations not meeting the criteria excluded.\n    \"\"\"\n    # Disjonction suivant la suppression\n    if self.drop:\n        # Initialisation de la s\u00e9rie de bool\u00e9ens\n        list_data_boolean = []\n\n        # Parcours des dictionnaires de param\u00e8tres\n        for dict_params in self.list_dict_params:\n            if dict_params[\"operator\"] == \"&gt;\":\n                list_data_boolean.append(\n                    (\n                        X[dict_params[\"variable\"]] &gt; dict_params[\"threshold\"]\n                    ).to_frame()\n                )\n            elif dict_params[\"operator\"] == \"&gt;=\":\n                list_data_boolean.append(\n                    (\n                        X[dict_params[\"variable\"]] &gt;= dict_params[\"threshold\"]\n                    ).to_frame()\n                )\n            elif dict_params[\"operator\"] == \"&lt;\":\n                list_data_boolean.append(\n                    (\n                        X[dict_params[\"variable\"]] &lt; dict_params[\"threshold\"]\n                    ).to_frame()\n                )\n            elif dict_params[\"operator\"] == \"&lt;=\":\n                list_data_boolean.append(\n                    (\n                        X[dict_params[\"variable\"]] &lt;= dict_params[\"threshold\"]\n                    ).to_frame()\n                )\n            elif dict_params[\"operator\"] == \"!=\":\n                list_data_boolean.append(\n                    (\n                        X[dict_params[\"variable\"]] != dict_params[\"threshold\"]\n                    ).to_frame()\n                )\n        # Concat\u00e9nation des s\u00e9ries bool\u00e9ennes\n        data_boolean = pd.concat(list_data_boolean, axis=1, join=\"outer\")\n\n        # Restriction aux observations respectant tous les seuils\n        X_transformed = X.loc[data_boolean.all(axis=1)]\n    else:\n        # Copie ind\u00e9pendante du jeu de donn\u00e9es\n        X_transformed = X.copy()\n        # Parcours des dictionnaires de param\u00e8tres\n        for dict_params in self.list_dict_params:\n            if dict_params[\"operator\"] == \"&gt;\":\n                X_transformed.loc[\n                    (X[dict_params[\"variable\"]] &gt; dict_params[\"threshold\"]),\n                    dict_params[\"variable\"],\n                ] = np.nan\n            elif dict_params[\"operator\"] == \"&gt;=\":\n                X_transformed.loc[\n                    (X[dict_params[\"variable\"]] &gt;= dict_params[\"threshold\"]),\n                    dict_params[\"variable\"],\n                ] = np.nan\n            elif dict_params[\"operator\"] == \"&lt;\":\n                X_transformed.loc[\n                    (X[dict_params[\"variable\"]] &lt; dict_params[\"threshold\"]),\n                    dict_params[\"variable\"],\n                ] = np.nan\n            elif dict_params[\"operator\"] == \"&lt;=\":\n                X_transformed.loc[\n                    (X[dict_params[\"variable\"]] &lt;= dict_params[\"threshold\"]),\n                    dict_params[\"variable\"],\n                ] = np.nan\n            elif dict_params[\"operator\"] == \"!=\":\n                X_transformed.loc[\n                    (X[dict_params[\"variable\"]] != dict_params[\"threshold\"]),\n                    dict_params[\"variable\"],\n                ] = np.nan\n\n    return X_transformed\n</code></pre>"},{"location":"api/_S3Connection/","title":"_S3Connection","text":"<p>A parent class for managing connections to an Amazon S3 bucket using different packages.</p> <p>This class allows you to connect to an S3 bucket using either 's3fs' or 'boto3' as the underlying package. 's3fs' is used for file system-like access, while 'boto3' provides a more comprehensive AWS SDK.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>The package to use for connecting to S3 ('s3fs' or 'boto3').</p> <code>'boto3'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the 'package' argument is not one of ['s3fs', 'boto3'].</p> <p>Attributes:</p> Name Type Description <code>package</code> <code>str</code> <p>The package used for S3 connectivity ('s3fs' or 'boto3').</p> <p>Methods:</p> Name Description <code>_connect</code> <p>Connects to the S3 bucket with the specified parameters and package. Returns the connected S3 client or file system.</p> <p>Example :</p> <p>s3_connection = _S3Connection(package='boto3') s3_client = s3_connection._connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')</p> Note <p>Make sure to set the required AWS environment variables for successful connections.</p> Source code in <code>igf_toolbox/s3/_connection.py</code> <pre><code>class _S3Connection:\n    \"\"\"\n    A parent class for managing connections to an Amazon S3 bucket using different packages.\n\n    This class allows you to connect to an S3 bucket using either 's3fs' or 'boto3' as the underlying package.\n    's3fs' is used for file system-like access, while 'boto3' provides a more comprehensive AWS SDK.\n\n    Args:\n        package (str): The package to use for connecting to S3 ('s3fs' or 'boto3').\n\n    Raises:\n        ValueError: If the 'package' argument is not one of ['s3fs', 'boto3'].\n\n    Attributes:\n        package (str): The package used for S3 connectivity ('s3fs' or 'boto3').\n\n    Methods:\n        _connect(endpoint_url, aws_access_key_id, aws_secret_access_key, aws_session_token, verify, **kwargs):\n            Connects to the S3 bucket with the specified parameters and package.\n            Returns the connected S3 client or file system.\n\n    Example :\n    &gt;&gt;&gt; s3_connection = _S3Connection(package='boto3')\n    &gt;&gt;&gt; s3_client = s3_connection._connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n\n    Note:\n        Make sure to set the required AWS environment variables for successful connections.\n    \"\"\"\n\n    def __init__(self, package: Optional[str] = \"boto3\") -&gt; None:\n        \"\"\"\n        Initialize the S3 connection class with the specified package.\n\n        Args:\n            package (str): The package to use for connecting to S3 ('s3fs' or 'boto3').\n        \"\"\"\n        # Initialisation du package utilis\u00e9 pour se connecter au bucket S3\n        # Deux valeurs sont valides pour ce param\u00e8tre 's3fs' et 'boto3'\n        if package not in [\"s3fs\", \"boto3\"]:\n            raise ValueError(\"'package' must be in ['s3fs', 'boto3']\")\n\n        self.package = package\n\n    def _connect(\n        self,\n        endpoint_url: Optional[Union[str, None]] = None,\n        aws_access_key_id: Optional[Union[str, None]] = None,\n        aws_secret_access_key: Optional[Union[str, None]] = None,\n        aws_session_token: Optional[Union[str, None]] = None,\n        verify: Optional[bool] = False,\n        **kwargs,\n    ) -&gt; None:\n        \"\"\"\n        Connects to the S3 bucket using the specified parameters and package.\n\n        Args:\n            endpoint_url (str): The S3 endpoint URL (optional).\n            aws_access_key_id (str): The AWS access key ID (optional).\n            aws_secret_access_key (str): The AWS secret access key (optional).\n            aws_session_token (str): The AWS session token (optional).\n            verify (bool): Whether to verify SSL certificates (default is False).\n            **kwargs: Additional keyword arguments specific to the chosen package.\n\n        Returns:\n            obj: The connected S3 client or file system.\n\n        Raises:\n            ValueError: If the 'package' argument is not one of ['s3fs', 'boto3'].\n\n        Example :\n        &gt;&gt;&gt; s3_connection = _S3Connection(package='boto3')\n        &gt;&gt;&gt; s3_client = s3_connection._connect(aws_access_key_id='your_access_key', aws_secret_access_key='your_secret_key')\n        \"\"\"\n        # D\u00e9sactive les warnings en raison de la non v\u00e9rification du certificat (non recommand\u00e9)\n        disable_warnings()\n\n        if self.package == \"boto3\":\n            self.s3 = client(\n                \"s3\",\n                endpoint_url=(\n                    endpoint_url\n                    if endpoint_url is not None\n                    else \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n                ),\n                aws_access_key_id=(\n                    aws_access_key_id\n                    if aws_access_key_id is not None\n                    else os.environ[\"AWS_ACCESS_KEY_ID\"]\n                ),\n                aws_secret_access_key=(\n                    aws_secret_access_key\n                    if aws_secret_access_key is not None\n                    else os.environ[\"AWS_SECRET_ACCESS_KEY\"]\n                ),\n                aws_session_token=(\n                    aws_session_token\n                    if aws_session_token is not None\n                    else os.environ[\"AWS_SESSION_TOKEN\"]\n                ),\n                verify=verify,\n                **kwargs,\n            )\n        elif self.package == \"s3fs\":\n            self.s3 = S3FileSystem(\n                client_kwargs={\n                    \"endpoint_url\": (\n                        endpoint_url\n                        if endpoint_url is not None\n                        else \"https://\" + os.environ[\"AWS_S3_ENDPOINT\"]\n                    )\n                },\n                **kwargs,\n            )\n        else:\n            raise ValueError(\"'package' must be in ['s3fs', 'boto3']\")\n\n        return self\n</code></pre>"},{"location":"api/_S3Connection/#igf_toolbox.s3._connection._S3Connection.__init__","title":"<code>__init__(package='boto3')</code>","text":"<p>Initialize the S3 connection class with the specified package.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str</code> <p>The package to use for connecting to S3 ('s3fs' or 'boto3').</p> <code>'boto3'</code> Source code in <code>igf_toolbox/s3/_connection.py</code> <pre><code>def __init__(self, package: Optional[str] = \"boto3\") -&gt; None:\n    \"\"\"\n    Initialize the S3 connection class with the specified package.\n\n    Args:\n        package (str): The package to use for connecting to S3 ('s3fs' or 'boto3').\n    \"\"\"\n    # Initialisation du package utilis\u00e9 pour se connecter au bucket S3\n    # Deux valeurs sont valides pour ce param\u00e8tre 's3fs' et 'boto3'\n    if package not in [\"s3fs\", \"boto3\"]:\n        raise ValueError(\"'package' must be in ['s3fs', 'boto3']\")\n\n    self.package = package\n</code></pre>"},{"location":"api/_assign_quantile_array/","title":"_assign_quantile_array","text":"<p>Assigns quantiles or thresholds to each observation in the input array.</p> <p>Parameters: - array_values (1D numpy.ndarray): Array containing values for which quantiles or thresholds are to be assigned. - quantiles (1D array-like): List or array of quantiles or thresholds. Does not need to be sorted. - is_threshold (bool): If True, the function assigns thresholds to each observation in <code>array_values</code>.                        If False, the function assigns quantile labels (e.g., 1 for first quantile, 2 for second, etc.).</p> <p>Returns: - numpy.ndarray: Array containing assigned quantiles or thresholds for each observation in <code>array_values</code>.</p> <p>Notes: This function is optimized with Numba's Just-in-Time (JIT) compiler for improved performance. Ensure that the <code>array_values</code> does not contain NaN values.</p> Source code in <code>igf_toolbox/stats_des/weighted.py</code> <pre><code>@njit\ndef _assign_quantile_array(\n    array_values: np.ndarray, quantiles: np.ndarray, is_threshold: bool\n) -&gt; np.ndarray:\n    \"\"\"\n    Assigns quantiles or thresholds to each observation in the input array.\n\n    Parameters:\n    - array_values (1D numpy.ndarray): Array containing values for which quantiles or thresholds are to be assigned.\n    - quantiles (1D array-like): List or array of quantiles or thresholds. Does not need to be sorted.\n    - is_threshold (bool): If True, the function assigns thresholds to each observation in `array_values`.\n                           If False, the function assigns quantile labels (e.g., 1 for first quantile, 2 for second, etc.).\n\n    Returns:\n    - numpy.ndarray: Array containing assigned quantiles or thresholds for each observation in `array_values`.\n\n    Notes:\n    This function is optimized with Numba's Just-in-Time (JIT) compiler for improved performance.\n    Ensure that the `array_values` does not contain NaN values.\n    \"\"\"\n\n    # Tri des quantiles\n    quantiles_array = np.sort(quantiles)\n\n    # Construction du jeu de donn\u00e9es r\u00e9sultat\n    if is_threshold:\n        array_res = np.full(shape=len(array_values), fill_value=np.min(array_values))\n    else:\n        array_res = np.zeros(shape=len(array_values))\n\n    # Application successive des quantiles (tri\u00e9s par ordre croissance)\n    for i, qi in enumerate(quantiles_array):\n        if is_threshold:\n            array_res[array_values &gt;= qi] = qi\n        else:\n            array_res[array_values &gt;= qi] = i + 1\n\n    return array_res\n</code></pre>"},{"location":"api/_fit_and_predict/","title":"_fit_and_predict","text":"<p>Internal auxiliary function to fit the model and predict on a given train-test split.</p>"},{"location":"api/_fit_and_predict/#igf_toolbox.model_selection.prediction._fit_and_predict--parameters","title":"Parameters","text":"<p>estimator : object     An estimator instance. X : array-like or pd.DataFrame     The data to fit. y : array-like or None     The target variable to try to predict. train : array-like     The indices of the training samples. test : array-like     The indices of the testing samples. verbose : int     The verbosity level. compute_confidence_interval : bool     Whether to compute confidence intervals for the predictions. bootstrap_size : int or None     The number of bootstrap samples to use. n_iterations_boostrap : int or None     The number of bootstrap iterations. alpha : float or None     Alpha level for the confidence interval. fit_params : dict or None     Parameters to pass to the fit method of the estimator. method : str     The prediction method to use ('predict', 'predict_proba', etc.).</p>"},{"location":"api/_fit_and_predict/#igf_toolbox.model_selection.prediction._fit_and_predict--returns","title":"Returns","text":"<p>predictions : ndarray     Predicted values for each test sample. If <code>compute_confidence_interval</code> is True,     it also returns the lower and upper bounds for each prediction.</p>"},{"location":"api/_fit_and_predict/#igf_toolbox.model_selection.prediction._fit_and_predict--notes","title":"Notes","text":"<p>This is an internal function, users should use <code>crossval_predict</code> instead.</p> Source code in <code>igf_toolbox/model_selection/prediction.py</code> <pre><code>def _fit_and_predict(\n    estimator,\n    X,\n    y,\n    train,\n    test,\n    verbose: int,\n    compute_confidence_interval: bool,\n    bootstrap_size: Union[int, None],\n    n_iterations_boostrap: Union[int, None],\n    alpha: Union[float, None],\n    fit_params: Union[dict, None],\n    method: str,\n):\n    \"\"\"\n    Internal auxiliary function to fit the model and predict on a given train-test split.\n\n    Parameters\n    ----------\n    estimator : object\n        An estimator instance.\n    X : array-like or pd.DataFrame\n        The data to fit.\n    y : array-like or None\n        The target variable to try to predict.\n    train : array-like\n        The indices of the training samples.\n    test : array-like\n        The indices of the testing samples.\n    verbose : int\n        The verbosity level.\n    compute_confidence_interval : bool\n        Whether to compute confidence intervals for the predictions.\n    bootstrap_size : int or None\n        The number of bootstrap samples to use.\n    n_iterations_boostrap : int or None\n        The number of bootstrap iterations.\n    alpha : float or None\n        Alpha level for the confidence interval.\n    fit_params : dict or None\n        Parameters to pass to the fit method of the estimator.\n    method : str\n        The prediction method to use ('predict', 'predict_proba', etc.).\n\n    Returns\n    -------\n    predictions : ndarray\n        Predicted values for each test sample. If `compute_confidence_interval` is True,\n        it also returns the lower and upper bounds for each prediction.\n\n    Notes\n    -----\n    This is an internal function, users should use `crossval_predict` instead.\n    \"\"\"\n    # Ajuste la longeur des vecteurs de poids sur les observations\n    fit_params = fit_params if fit_params is not None else {}\n    # fit_params = _check_method_params(X, fit_params, train)\n\n    # Extraction des observations correspondant au Train\n    X_train, y_train = _safe_split(estimator, X, y, train)\n\n    # Extraction des observations correspondant au Test\n    X_test, _ = _safe_split(estimator, X, y, test, train)\n\n    # Entrainement de l'Estimateur\n    if y_train is None:\n        estimator.fit(X_train, **fit_params)\n    else:\n        estimator.fit(X_train, y_train, **fit_params)\n    func = getattr(estimator, method)\n    predictions = func(X_test)\n\n    encode = (\n        method in [\"decision_function\", \"predict_proba\", \"predict_log_proba\"]\n        and y is not None\n    )\n\n    if encode:\n        if isinstance(predictions, list):\n            predictions = [\n                _enforce_prediction_order(\n                    estimator.classes_[i_label],\n                    predictions[i_label],\n                    n_classes=len(set(y[:, i_label])),\n                    method=method,\n                )\n                for i_label in range(len(predictions))\n            ]\n        else:\n            # A 2D y array should be a binary label indicator matrix\n            n_classes = len(set(y)) if y.ndim == 1 else y.shape[1]\n            predictions = _enforce_prediction_order(\n                estimator.classes_, predictions, n_classes, method\n            )\n\n    # Computation de l'intervalle de confiance\n    # Normalement pour pr\u00e9diction on pourrait prendre la m\u00e9diane plutot que la valeur pr\u00e9dite sur l'ensemble\n    # L'algorithme ne marche sans doute pas dans le cas d'un y de dimension sup\u00e9rieure \u00e0 1, faire des np.array et chercher les std en s\u00e9prarant composante par composante\n    # Pour le Bootstrap cela vaudrait peut \u00eatre vraiment le coup d'impl\u00e9menter avec numba ou en C++\n    if (\n        bootstrap_size is not None\n        and n_iterations_boostrap is not None\n        and alpha is not None\n    ):\n        # ScoreBootstrap = []\n        list_predictions_boostrap = []\n        for _ in range(n_iterations_boostrap):\n            if y_train is None:\n                X_trainBootstrap = resample(\n                    X_train, replace=True, n_samples=len(X_train) * bootstrap_size\n                )\n                estimator.fit(X_trainBootstrap, **fit_params)\n            else:\n                X_trainBootstrap, y_trainBootstrap = resample(\n                    X_train,\n                    y_train,\n                    replace=True,\n                    n_samples=int(len(X_train) * bootstrap_size),\n                )\n                estimator.fit(X_trainBootstrap, y_trainBootstrap, **fit_params)\n\n            func = getattr(estimator, method)\n            predictionsBootstrap = func(X_test)\n\n            encode = (\n                method in [\"decision_function\", \"predict_proba\", \"predict_log_proba\"]\n                and y is not None\n            )\n\n            if encode:\n                if isinstance(predictionsBootstrap, list):\n                    predictionsBootstrap = [\n                        _enforce_prediction_order(\n                            estimator.classes_[i_label],\n                            predictionsBootstrap[i_label],\n                            n_classes=len(set(y[:, i_label])),\n                            method=method,\n                        )\n                        for i_label in range(len(predictionsBootstrap))\n                    ]\n                else:\n                    # A 2D y array should be a binary label indicator matrix\n                    n_classes = len(set(y)) if y.ndim == 1 else y.shape[1]\n                    predictionsBootstrap = _enforce_prediction_order(\n                        estimator.classes_, predictionsBootstrap, n_classes, method\n                    )\n\n            # Ajout de la pr\u00e9diction\n            list_predictions_boostrap.append(predictionsBootstrap)\n\n        if compute_confidence_interval:\n            # Computation des limites de l'intervalle de confiance\n            lower_bound_conf = np.quantile(\n                a=np.vstack(list_predictions_boostrap), q=(1 - alpha) / 2, axis=0\n            )\n            upper_bound_conf = np.quantile(\n                a=np.vstack(list_predictions_boostrap), q=(1 + alpha) / 2, axis=0\n            )\n\n            predictions = np.vstack(\n                [predictions, lower_bound_conf, upper_bound_conf]\n            )  # Peut \u00eatre faut-il le changer en hstack, essayer dans le crossval pr\u00e9dict\n\n    return np.transpose(predictions)\n</code></pre>"},{"location":"api/_init_logger/","title":"_init_logger","text":"<p>Initializes the logger for logging to a file.</p> <p>Parameters: - filename (os.PathLike): Path to the log file.</p> <p>Returns: - logging.Logger: Initialized logger object.</p> <p>Note: - This function configures logging to output messages to both console and a file.</p> Source code in <code>igf_toolbox/utils/logger.py</code> <pre><code>def _init_logger(filename: os.PathLike) -&gt; logging.Logger:\n    \"\"\"\n    Initializes the logger for logging to a file.\n\n    Parameters:\n    - filename (os.PathLike): Path to the log file.\n\n    Returns:\n    - logging.Logger: Initialized logger object.\n\n    Note:\n    - This function configures logging to output messages to both console and a file.\n    \"\"\"\n    # Configuration de logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(message)s\",\n        encoding=\"utf-8\",\n        level=logging.INFO,\n    )\n\n    # V\u00e9rification de l'existance du dossier pour le fichier de log\n    log_directory = os.path.dirname(filename)\n\n    if (not os.path.exists(log_directory)) &amp; (log_directory != \"\"):\n        os.makedirs(log_directory)\n\n    # Configuration du fichier de logs\n    file_handler = logging.FileHandler(filename)\n    file_handler.setLevel(logging.INFO)\n\n    # Initialisation du logger\n    logger = logging.getLogger()\n    logger.addHandler(file_handler)\n\n    return logger\n</code></pre>"},{"location":"api/_separate_category_modality_from_dummy/","title":"_separate_category_modality_from_dummy","text":"<p>Auxiliary function to separate category names and their modalities from dummy variable names.</p> <p>Given a dummy variable name typically in the format \"category - modality\", this function separates the name into its individual category and modality components.</p>"},{"location":"api/_separate_category_modality_from_dummy/#igf_toolbox.utils._auxiliary._separate_category_modality_from_dummy--parameters","title":"Parameters:","text":"<p>value : dict     Dictionary containing a key 'dummies' whose associated value is the dummy variable name to be split.</p>"},{"location":"api/_separate_category_modality_from_dummy/#igf_toolbox.utils._auxiliary._separate_category_modality_from_dummy--returns","title":"Returns:","text":"<p>dict     Updated dictionary with two new keys:     'categories' : the extracted category name     'modalities' : the extracted modality name</p>"},{"location":"api/_separate_category_modality_from_dummy/#igf_toolbox.utils._auxiliary._separate_category_modality_from_dummy--examples","title":"Examples:","text":"<p>value = {'dummies': 'Color - Blue'} _separate_category_modality_from_dummy(value) {'dummies': 'Color - Blue', 'categories': 'Color', 'modalities': 'Blue'}</p>"},{"location":"api/_separate_category_modality_from_dummy/#igf_toolbox.utils._auxiliary._separate_category_modality_from_dummy--notes","title":"Notes:","text":"<p>The function assumes the dummy variable name contains only one ' - ' separator and splits only at the first occurrence.</p> Source code in <code>igf_toolbox/utils/_auxiliary.py</code> <pre><code>def _separate_category_modality_from_dummy(value: Dict[str, str]) -&gt; Dict[str, str]:\n    \"\"\"\n    Auxiliary function to separate category names and their modalities from dummy variable names.\n\n    Given a dummy variable name typically in the format \"category - modality\",\n    this function separates the name into its individual category and modality components.\n\n    Parameters:\n    -----------\n    value : dict\n        Dictionary containing a key 'dummies' whose associated value is the dummy variable name to be split.\n\n    Returns:\n    --------\n    dict\n        Updated dictionary with two new keys:\n        'categories' : the extracted category name\n        'modalities' : the extracted modality name\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; value = {'dummies': 'Color - Blue'}\n    &gt;&gt;&gt; _separate_category_modality_from_dummy(value)\n    {'dummies': 'Color - Blue', 'categories': 'Color', 'modalities': 'Blue'}\n\n    Notes:\n    ------\n    The function assumes the dummy variable name contains only one ' - ' separator and splits only at the first occurrence.\n    \"\"\"\n\n    # S\u00e9paration selon la premi\u00e8re occurence du caract\u00e8re ' - ' de la cat\u00e9gorie de ses modalit\u00e9s\n    res = value[\"dummies\"].split(\" - \", 1)\n\n    # Compl\u00e9tion du jeu de donn\u00e9es\n    value[\"categories\"] = res[0]\n    value[\"modalities\"] = res[1]\n\n    return value\n</code></pre>"},{"location":"api/_sort_index_with_total/","title":"_sort_index_with_total","text":"<p>Sorts the index of a DataFrame, moving the 'Total' label to the end of each level.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>DataFrame</code> <p>The DataFrame to be sorted.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The sorted DataFrame with 'Total' labels moved to the end of each index level.</p> Note <p>This function is intended for internal use and is used to sort the index of a DataFrame by moving 'Total' labels to the end.</p> Source code in <code>igf_toolbox/utils/_auxiliary.py</code> <pre><code>def _sort_index_with_total(data_source: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"\n    Sorts the index of a DataFrame, moving the 'Total' label to the end of each level.\n\n    Parameters:\n        data_source (pd.DataFrame): The DataFrame to be sorted.\n\n    Returns:\n        pd.DataFrame: The sorted DataFrame with 'Total' labels moved to the end of each index level.\n\n    Note:\n        This function is intended for internal use and is used to sort the index of a DataFrame by moving 'Total' labels to the end.\n    \"\"\"\n\n    # Tri de chaque index\n    for level in range(data_source.index.nlevels):\n        if level == 0:\n            data_source = _sort_level_with_total(data_source=data_source, level=level)\n        else:\n            data_source = data_source.groupby(level=np.arange(level).tolist()).apply(\n                func=lambda x: _sort_level_with_total(data_source=x, level=level)\n            )\n    return data_source\n</code></pre>"},{"location":"api/_sort_level_with_total/","title":"_sort_level_with_total","text":"<p>Sorts a specific index level of a DataFrame, moving the 'Total' label to the end of that level.</p> <p>Parameters:</p> Name Type Description Default <code>data_source</code> <code>DataFrame</code> <p>The DataFrame to be sorted.</p> required <code>level</code> <code>int</code> <p>The index level to be sorted.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The DataFrame with the specified index level sorted, moving 'Total' labels to the end.</p> Note <p>This function is intended for internal use and is used to sort a specific index level of a DataFrame.</p> Source code in <code>igf_toolbox/utils/_auxiliary.py</code> <pre><code>def _sort_level_with_total(data_source: pd.DataFrame, level: int) -&gt; pd.DataFrame:\n    \"\"\"\n    Sorts a specific index level of a DataFrame, moving the 'Total' label to the end of that level.\n\n    Parameters:\n        data_source (pd.DataFrame): The DataFrame to be sorted.\n        level (int): The index level to be sorted.\n\n    Returns:\n        pd.DataFrame: The DataFrame with the specified index level sorted, moving 'Total' labels to the end.\n\n    Note:\n        This function is intended for internal use and is used to sort a specific index level of a DataFrame.\n    \"\"\"\n\n    # Extraction des donn\u00e9es de total\n    mask_total = data_source.index.get_level_values(level).map(\n        mapper=lambda x: x == \"Total\"\n    )\n    # Tri des donn\u00e9es qui ne sont pas des totaux et concat\u00e9nation\n    if level == 0:\n        data_source = pd.concat(\n            [\n                data_source.loc[~mask_total].sort_index(level=level),\n                data_source.loc[mask_total],\n            ],\n            axis=0,\n        )\n    else:\n        data_source = pd.concat(\n            [\n                data_source.loc[~mask_total]\n                .sort_index(level=level)\n                .droplevel(level=np.arange(level).tolist()),\n                data_source.loc[mask_total].droplevel(level=np.arange(level).tolist()),\n            ],\n            axis=0,\n        )\n\n    return data_source\n</code></pre>"},{"location":"api/_weighted_quantile_array/","title":"_weighted_quantile_array","text":"<p>Calculates weighted quantiles for a given 2D array with values and weights.</p> <p>Parameters: - array (2D numpy.ndarray): Input array containing values and their corresponding weights. - values_pos (int): Column index in <code>array</code> where the values are stored. - weights_pos (int): Column index in <code>array</code> where the weights are stored. - q (1D array-like): List or array of quantiles to compute. Values must be between 0 and 1.</p> <p>Returns: - numpy.ndarray: Array containing computed weighted quantiles corresponding to <code>q</code>.</p> <p>Notes: This function is optimized with Numba's Just-in-Time (JIT) compiler for improved performance. Ensure that the <code>array</code> does not contain NaN values.</p> Source code in <code>igf_toolbox/stats_des/weighted.py</code> <pre><code>@njit\ndef _weighted_quantile_array(\n    array: np.ndarray, values_pos: int, weights_pos: int, q: np.ndarray\n) -&gt; np.ndarray:\n    \"\"\"\n    Calculates weighted quantiles for a given 2D array with values and weights.\n\n    Parameters:\n    - array (2D numpy.ndarray): Input array containing values and their corresponding weights.\n    - values_pos (int): Column index in `array` where the values are stored.\n    - weights_pos (int): Column index in `array` where the weights are stored.\n    - q (1D array-like): List or array of quantiles to compute. Values must be between 0 and 1.\n\n    Returns:\n    - numpy.ndarray: Array containing computed weighted quantiles corresponding to `q`.\n\n    Notes:\n    This function is optimized with Numba's Just-in-Time (JIT) compiler for improved performance.\n    Ensure that the `array` does not contain NaN values.\n    \"\"\"\n    # Tri selon les valeurs (en premi\u00e8re colonne)\n    array_work = array[array[:, values_pos].argsort()]\n    # Somme cumulative des poids\n    array_work[:, weights_pos] = np.cumsum(array_work[:, weights_pos]) / np.sum(\n        array_work[:, weights_pos]\n    )\n    # Initialisation de l'array r\u00e9sultat\n    res_value = np.zeros(len(q))\n    for i, qi in enumerate(q):\n        res_value[i] = array_work[array_work[:, weights_pos] &lt;= qi, values_pos][-1]\n\n    return res_value\n</code></pre>"},{"location":"api/apply_stars/","title":"apply_stars","text":"<p>Append asterisks to a coefficient based on its p-value.</p> <p>Parameters: - coef (str): The coefficient to which asterisks will be appended. - p_value (float): The p-value corresponding to the coefficient.</p> <ul> <li>str: The coefficient with appended asterisks indicating its significance level.   Three asterisks () for p &lt; 0.01, two asterisks () for p &lt; 0.05,   one asterisk () for p &lt; 0.1, and no asterisks for p &gt;= 0.1.</li> </ul> <p>Example:</p> <p>apply_stars('0.25', 0.02) '0.25 (**)'</p> Source code in <code>igf_toolbox/utils/base.py</code> <pre><code>def apply_stars(coef: str, p_value: float) -&gt; str:\n    \"\"\"\n    Append asterisks to a coefficient based on its p-value.\n\n    Parameters:\n    - coef (str): The coefficient to which asterisks will be appended.\n    - p_value (float): The p-value corresponding to the coefficient.\n\n    Returns:\n    - str: The coefficient with appended asterisks indicating its significance level.\n      Three asterisks (***) for p &lt; 0.01, two asterisks (**) for p &lt; 0.05,\n      one asterisk (*) for p &lt; 0.1, and no asterisks for p &gt;= 0.1.\n\n    Example:\n    &gt;&gt;&gt; apply_stars('0.25', 0.02)\n    '0.25 (**)'\n    \"\"\"\n\n    if p_value &lt; 0.01:\n        return coef + \" (***)\"\n    elif p_value &lt; 0.05:\n        return coef + \" (**)\"\n    elif p_value &lt; 0.1:\n        return coef + \" (*)\"\n    else:\n        return coef + \" ()\"\n</code></pre>"},{"location":"api/assign_quantile/","title":"assign_quantile","text":"<p>Assigns quantiles or thresholds to each observation in the input series.</p> <p>Parameters: - serie (pd.Series): Serie containing values for which quantiles or thresholds are to be assigned. - quantiles (1D array-like): List or array of quantiles or thresholds. Does not need to be sorted. - is_threshold (bool): If True, the function assigns thresholds to each observation in <code>array_values</code>.                        If False, the function assigns quantile labels (e.g., 1 for first quantile, 2 for second, etc.).</p> <p>Returns: - pd.Series: Series containing assigned quantiles or thresholds for each observation in <code>serie</code>.</p> <p>Notes: This function relies on <code>_assign_quantile_array</code> optimized with Numba's Just-in-Time (JIT) compiler for improved performance. Ensure that the <code>array_values</code> does not contain NaN values.</p> Source code in <code>igf_toolbox/stats_des/weighted.py</code> <pre><code>def assign_quantile(\n    serie: pd.Series, quantiles: List[Union[int, float]], is_threshold: bool\n) -&gt; pd.Series:\n    \"\"\"\n    Assigns quantiles or thresholds to each observation in the input series.\n\n    Parameters:\n    - serie (pd.Series): Serie containing values for which quantiles or thresholds are to be assigned.\n    - quantiles (1D array-like): List or array of quantiles or thresholds. Does not need to be sorted.\n    - is_threshold (bool): If True, the function assigns thresholds to each observation in `array_values`.\n                           If False, the function assigns quantile labels (e.g., 1 for first quantile, 2 for second, etc.).\n\n    Returns:\n    - pd.Series: Series containing assigned quantiles or thresholds for each observation in `serie`.\n\n    Notes:\n    This function relies on `_assign_quantile_array` optimized with Numba's Just-in-Time (JIT) compiler for improved performance.\n    Ensure that the `array_values` does not contain NaN values.\n    \"\"\"\n\n    return pd.Series(\n        data=_assign_quantile_array(\n            array_values=np.asarray(serie),\n            quantiles=np.asarray(quantiles),\n            is_threshold=is_threshold,\n        ),\n        index=serie.index,\n    )\n</code></pre>"},{"location":"api/convert_pvalues_to_stars/","title":"convert_pvalues_to_stars","text":"<p>Convert p-values to asterisks and append them to the coefficients in a dataframe.</p> <p>This function takes in a dataframe and based on provided coefficient and p-value columns, it will convert the p-values to asterisks using the <code>apply_stars</code> function and append them to the coefficients. The coefficients can also be converted to percentage if required.</p> <p>Parameters: - data_source (pd.DataFrame): Source dataframe containing the coefficient and p-value columns. - col_coef (str): Name of the column containing coefficients. - col_pvalues (str): Name of the column containing p-values. - is_percent (bool): If True, coefficients are converted to percentages. Otherwise, they remain as is.</p> <ul> <li>pd.DataFrame: A dataframe with a new column 'Coefficients - P-valeurs' containing coefficients   appended with asterisks indicating significance level based on p-values.   The coefficients are rounded and the decimal points are replaced with commas.</li> </ul> <p>Example:</p> <p>df = pd.DataFrame({'coef': [0.25, 0.1], 'p_value': [0.02, 0.5]}) convert_pvalues_to_stars(df, 'coef', 'p_value', True)    coef  p_value Coefficients - P-valeurs 0  0.25     0.02                   25,0% (**) 1  0.10     0.50                   10,0% ()</p> Source code in <code>igf_toolbox/utils/base.py</code> <pre><code>def convert_pvalues_to_stars(\n    data_source: pd.DataFrame, col_coef: str, col_pvalues: str, is_percent: bool\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Convert p-values to asterisks and append them to the coefficients in a dataframe.\n\n    This function takes in a dataframe and based on provided coefficient and p-value columns,\n    it will convert the p-values to asterisks using the `apply_stars` function and append them\n    to the coefficients. The coefficients can also be converted to percentage if required.\n\n    Parameters:\n    - data_source (pd.DataFrame): Source dataframe containing the coefficient and p-value columns.\n    - col_coef (str): Name of the column containing coefficients.\n    - col_pvalues (str): Name of the column containing p-values.\n    - is_percent (bool): If True, coefficients are converted to percentages. Otherwise, they remain as is.\n\n    Returns:\n    - pd.DataFrame: A dataframe with a new column 'Coefficients - P-valeurs' containing coefficients\n      appended with asterisks indicating significance level based on p-values.\n      The coefficients are rounded and the decimal points are replaced with commas.\n\n    Example:\n    &gt;&gt;&gt; df = pd.DataFrame({'coef': [0.25, 0.1], 'p_value': [0.02, 0.5]})\n    &gt;&gt;&gt; convert_pvalues_to_stars(df, 'coef', 'p_value', True)\n       coef  p_value Coefficients - P-valeurs\n    0  0.25     0.02                   25,0% (**)\n    1  0.10     0.50                   10,0% ()\n    \"\"\"\n\n    # Copie ind\u00e9pendante du jeu de donn\u00e9es\n    data_res = data_source.copy()\n\n    # Ajout de la colonne r\u00e9sultats\n    if is_percent:\n        data_res[\"Coefficients - P-valeurs\"] = (\n            np.round(data_res[col_coef] * 100, decimals=1)\n            .astype(str)\n            .apply(lambda x: x + \"%\")\n        )\n    else:\n        data_res[\"Coefficients - P-valeurs\"] = np.round(\n            data_res[col_coef], decimals=2\n        ).astype(str)\n\n    # Correction des points\n    data_res[\"Coefficients - P-valeurs\"] = data_res[\n        \"Coefficients - P-valeurs\"\n    ].str.replace(\".\", \",\", regex=False)\n\n    # Ajout des petites \u00e9toiles\n    data_res[\"Coefficients - P-valeurs\"] = data_res[\n        [\"Coefficients - P-valeurs\", col_pvalues]\n    ].apply(\n        lambda x: apply_stars(\n            coef=x[\"Coefficients - P-valeurs\"], p_value=x[col_pvalues]\n        ),\n        axis=1,\n    )\n\n    return data_res\n</code></pre>"},{"location":"api/count_effectif_modalite/","title":"count_effectif_modalite","text":"<p>Counts the number of unique individuals within a reference modality for a given list of variables.</p>"},{"location":"api/count_effectif_modalite/#igf_toolbox.utils.base.count_effectif_modalite--parameters","title":"Parameters","text":"<p>liste_fix_no_trap : list of tuples     List of variable-modalities pairs. Each tuple consists of a variable name and its reference modality. data_source : pd.DataFrame     Source DataFrame containing the data. var_id : str     Name of the variable that identifies unique individuals within <code>data_source</code>.</p>"},{"location":"api/count_effectif_modalite/#igf_toolbox.utils.base.count_effectif_modalite--returns","title":"Returns","text":"<p>data_res : pd.DataFrame     DataFrame containing:     - 'variable': Variable names from <code>liste_fix_no_trap</code>.     - 'modalite_ref': Corresponding modalities from <code>liste_fix_no_trap</code>.     - 'nombre_individus': Count of unique individuals within each modality.     The last row contains the intersection count for all modalities provided in <code>liste_fix_no_trap</code>.</p>"},{"location":"api/count_effectif_modalite/#igf_toolbox.utils.base.count_effectif_modalite--notes","title":"Notes","text":"<p>If a modality for a given variable is not found in <code>data_source</code>, a warning is issued, and the modality is skipped.</p> Source code in <code>igf_toolbox/utils/base.py</code> <pre><code>def count_effectif_modalite(\n    liste_fix_no_trap: List[Tuple[str, str]], data_source: pd.DataFrame, var_id: str\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Counts the number of unique individuals within a reference modality for a given list of variables.\n\n    Parameters\n    ----------\n    liste_fix_no_trap : list of tuples\n        List of variable-modalities pairs. Each tuple consists of a variable name and its reference modality.\n    data_source : pd.DataFrame\n        Source DataFrame containing the data.\n    var_id : str\n        Name of the variable that identifies unique individuals within `data_source`.\n\n    Returns\n    -------\n    data_res : pd.DataFrame\n        DataFrame containing:\n        - 'variable': Variable names from `liste_fix_no_trap`.\n        - 'modalite_ref': Corresponding modalities from `liste_fix_no_trap`.\n        - 'nombre_individus': Count of unique individuals within each modality.\n        The last row contains the intersection count for all modalities provided in `liste_fix_no_trap`.\n\n    Notes\n    -----\n    If a modality for a given variable is not found in `data_source`, a warning is issued, and the modality is skipped.\n    \"\"\"\n\n    # Initialisation du jeu de donn\u00e9es r\u00e9sultat\n    data_res = pd.DataFrame(\n        data=liste_fix_no_trap, columns=[\"variable\", \"modalite_ref\"]\n    )\n\n    # Initialisation du nombre d'individus correspondant \u00e0 chaque modalit\u00e9\n    data_res[\"nombre_individus\"] = 0\n    data_intersect = data_source.copy()\n\n    # Remplissage du jeu de donn\u00e9es r\u00e9sultats\n    for i, (variable, modalite_ref) in enumerate(liste_fix_no_trap):\n        # Ajout du nombre d'individus\n        data_res.loc[i, \"nombre_individus\"] = data_source.loc[\n            data_source[variable] == modalite_ref, var_id\n        ].nunique()\n        # R\u00e9duction de l'intersection\n        try:\n            data_intersect = data_intersect.loc[\n                data_intersect[variable] == modalite_ref\n            ]\n        except:\n            warnings.warn(\n                \"Could not find the modality {} of the variable {}\".format(\n                    variable, modalite_ref\n                )\n            )\n            pass\n\n    # Ajout de l'intersection\n    data_res = pd.concat(\n        [\n            data_res,\n            pd.DataFrame(\n                [[\"Intersection\", \"Intersection\", data_intersect[var_id].nunique()]],\n                columns=[\"variable\", \"modalite_ref\", \"nombre_individus\"],\n            ),\n        ],\n        axis=0,\n        join=\"outer\",\n        ignore_index=True,\n    )\n\n    return data_res\n</code></pre>"},{"location":"api/create_dict_suffix/","title":"create_dict_suffix","text":"<p>Creates a dictionary where each key from the given list has a corresponding value with the specified suffix.</p>"},{"location":"api/create_dict_suffix/#igf_toolbox.utils._auxiliary.create_dict_suffix--parameters","title":"Parameters:","text":"<p>list_name : list     List of strings that will serve as keys in the resulting dictionary. suffix : str     Suffix to append to each string in the list_name to create corresponding dictionary values.</p>"},{"location":"api/create_dict_suffix/#igf_toolbox.utils._auxiliary.create_dict_suffix--returns","title":"Returns:","text":"<p>dict     Dictionary with keys from list_name and values as original strings appended with the provided suffix.</p>"},{"location":"api/create_dict_suffix/#igf_toolbox.utils._auxiliary.create_dict_suffix--example","title":"Example:","text":"<p>create_dict_suffix(['apple', 'banana'], '_fruit') {'apple': 'apple_fruit', 'banana': 'banana_fruit'}</p> Source code in <code>igf_toolbox/utils/_auxiliary.py</code> <pre><code>def create_dict_suffix(list_name: List[str], suffix: str) -&gt; Dict[str, str]:\n    \"\"\"\n    Creates a dictionary where each key from the given list has a corresponding value with the specified suffix.\n\n    Parameters:\n    -----------\n    list_name : list\n        List of strings that will serve as keys in the resulting dictionary.\n    suffix : str\n        Suffix to append to each string in the list_name to create corresponding dictionary values.\n\n    Returns:\n    --------\n    dict\n        Dictionary with keys from list_name and values as original strings appended with the provided suffix.\n\n    Example:\n    --------\n    &gt;&gt;&gt; create_dict_suffix(['apple', 'banana'], '_fruit')\n    {'apple': 'apple_fruit', 'banana': 'banana_fruit'}\n    \"\"\"\n    # Initialisation du dictionnaire r\u00e9sultat\n    dict_suffix = {}\n    # Remplissage du dictionnaire\n    for name in list_name:\n        dict_suffix[name] = name + suffix\n    return dict_suffix\n</code></pre>"},{"location":"api/create_pond_data/","title":"create_pond_data","text":"<p>Create a weighted dataset by multiplying the variables of interest with the specified weights.</p>"},{"location":"api/create_pond_data/#igf_toolbox.stats_des.weighted.create_pond_data--parameters","title":"Parameters","text":"<p>data : pandas.DataFrame     The source dataset that contains the variables of interest, grouping variables, and weights. list_var_of_interest : list of str     The list of column names in <code>data</code> that are of interest for weighting. list_var_groupby : list of str or None     The list of column names in <code>data</code> used for grouping. If None, no grouping will be performed. var_weights : str     The column name in <code>data</code> that contains the weights for the variables of interest.</p>"},{"location":"api/create_pond_data/#igf_toolbox.stats_des.weighted.create_pond_data--returns","title":"Returns","text":"<p>pandas.DataFrame     A DataFrame with the variables of interest weighted by <code>var_weights</code>. If <code>list_var_groupby</code> is provided,     the returned DataFrame will also contain the groupby variables.</p>"},{"location":"api/create_pond_data/#igf_toolbox.stats_des.weighted.create_pond_data--notes","title":"Notes","text":"<p>The function resets the index of the input dataframe to ensure safe merging.</p>"},{"location":"api/create_pond_data/#igf_toolbox.stats_des.weighted.create_pond_data--examples","title":"Examples","text":"<p>data = pd.DataFrame({ ...     'A': [1, 2, 3], ...     'B': [4, 5, 6], ...     'weights': [0.5, 1, 1.5] ... }) create_pond_data(data, ['A'], None, 'weights')    A 0  0.5 1  2.0 2  4.5</p> Source code in <code>igf_toolbox/stats_des/weighted.py</code> <pre><code>def create_pond_data(\n    data: pd.DataFrame,\n    list_var_of_interest: List[str],\n    list_var_groupby: Union[List[str], None],\n    var_weights: str,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Create a weighted dataset by multiplying the variables of interest with the specified weights.\n\n    Parameters\n    ----------\n    data : pandas.DataFrame\n        The source dataset that contains the variables of interest, grouping variables, and weights.\n    list_var_of_interest : list of str\n        The list of column names in `data` that are of interest for weighting.\n    list_var_groupby : list of str or None\n        The list of column names in `data` used for grouping. If None, no grouping will be performed.\n    var_weights : str\n        The column name in `data` that contains the weights for the variables of interest.\n\n    Returns\n    -------\n    pandas.DataFrame\n        A DataFrame with the variables of interest weighted by `var_weights`. If `list_var_groupby` is provided,\n        the returned DataFrame will also contain the groupby variables.\n\n    Notes\n    -----\n    The function resets the index of the input dataframe to ensure safe merging.\n\n    Examples\n    --------\n    &gt;&gt;&gt; data = pd.DataFrame({\n    ...     'A': [1, 2, 3],\n    ...     'B': [4, 5, 6],\n    ...     'weights': [0.5, 1, 1.5]\n    ... })\n    &gt;&gt;&gt; create_pond_data(data, ['A'], None, 'weights')\n       A\n    0  0.5\n    1  2.0\n    2  4.5\n    \"\"\"\n\n    # R\u00e9initialisation de l'index pour pouvoir faire un merge \"safe\"\n    data_work = data.copy().reset_index()\n\n    # Multiplication des variables d'int\u00e9r\u00eat par des poids\n    if list_var_groupby is not None:\n        data_pond = pd.merge(\n            left=data_work[list_var_groupby],\n            right=data_work[list_var_of_interest].multiply(\n                data_work[var_weights], axis=0\n            ),\n            how=\"outer\",\n            left_index=True,\n            right_index=True,\n            validate=\"one_to_one\",\n        )\n    else:\n        data_pond = data_work[list_var_of_interest].multiply(\n            data_work[var_weights], axis=0\n        )\n\n    return data_pond\n</code></pre>"},{"location":"api/crossval_predict/","title":"crossval_predict","text":"<p>Perform cross-validation on an estimator and obtain prediction results. This version is adapted for online learning scenarios and returns a pandas DataFrame.</p>"},{"location":"api/crossval_predict/#igf_toolbox.model_selection.prediction.crossval_predict--parameters","title":"Parameters","text":"<p>estimator : estimator object     The object to use to fit the data. X : array-like or pd.DataFrame     The data to fit. y : array-like, optional, default: None     The target variable to try to predict. groups : array-like, optional, default: None     Group labels for the samples. cv : int, cross-validation generator or 'warn', optional (default='warn')     Determines the cross-validation splitting strategy. n_jobs : int, optional (default=2)     Number of jobs to run in parallel. verbose : int, optional (default=0)     Verbosity level. compute_confidence_interval : bool, optional (default=False)     Whether to compute confidence interval for the prediction. bootstrap_size : int, optional (default=None)     Size of bootstrap sample. n_iterations_boostrap : int, optional (default=None)     Number of bootstrap iterations. alpha : float, optional (default=None)     Alpha value for intervals. fit_params : dict, optional (default=None)     Parameters to pass to the fit method. pre_dispatch : int, or string, optional (default='2*n_jobs')     Controls the number of jobs that get dispatched during parallel execution. method : string, optional (default='predict')     Invokes the passed method name of the passed estimator.</p>"},{"location":"api/crossval_predict/#igf_toolbox.model_selection.prediction.crossval_predict--returns","title":"Returns","text":"<p>predictions : pd.DataFrame     The predictions obtained from cross-validation.</p>"},{"location":"api/crossval_predict/#igf_toolbox.model_selection.prediction.crossval_predict--notes","title":"Notes","text":"<p>This function is adapted for online learning scenarios and is designed to prevent cloning the initial estimator during cross-validation.</p>"},{"location":"api/crossval_predict/#igf_toolbox.model_selection.prediction.crossval_predict--examples","title":"Examples","text":"<p>from sklearn.datasets import load_iris from sklearn.linear_model import LogisticRegression X, y = load_iris(return_X_y=True) crossval_predict(LogisticRegression(), X, y, cv=3)</p> Source code in <code>igf_toolbox/model_selection/prediction.py</code> <pre><code>def crossval_predict(\n    estimator,\n    X,\n    y=None,\n    groups=None,\n    cv=\"warn\",\n    n_jobs: Optional[int] = 2,\n    verbose: Optional[int] = 0,\n    compute_confidence_interval: Optional[bool] = False,\n    bootstrap_size: Optional[Union[int, None]] = None,\n    n_iterations_boostrap: Optional[Union[int, None]] = None,\n    alpha: Optional[Union[float, None]] = None,\n    fit_params: Optional[Union[dict, None]] = None,\n    pre_dispatch: Optional[Union[str, int]] = \"2*n_jobs\",\n    method: Optional[str] = \"predict\",\n):\n    \"\"\"\n    Perform cross-validation on an estimator and obtain prediction results.\n    This version is adapted for online learning scenarios and returns a pandas DataFrame.\n\n    Parameters\n    ----------\n    estimator : estimator object\n        The object to use to fit the data.\n    X : array-like or pd.DataFrame\n        The data to fit.\n    y : array-like, optional, default: None\n        The target variable to try to predict.\n    groups : array-like, optional, default: None\n        Group labels for the samples.\n    cv : int, cross-validation generator or 'warn', optional (default='warn')\n        Determines the cross-validation splitting strategy.\n    n_jobs : int, optional (default=2)\n        Number of jobs to run in parallel.\n    verbose : int, optional (default=0)\n        Verbosity level.\n    compute_confidence_interval : bool, optional (default=False)\n        Whether to compute confidence interval for the prediction.\n    bootstrap_size : int, optional (default=None)\n        Size of bootstrap sample.\n    n_iterations_boostrap : int, optional (default=None)\n        Number of bootstrap iterations.\n    alpha : float, optional (default=None)\n        Alpha value for intervals.\n    fit_params : dict, optional (default=None)\n        Parameters to pass to the fit method.\n    pre_dispatch : int, or string, optional (default='2*n_jobs')\n        Controls the number of jobs that get dispatched during parallel execution.\n    method : string, optional (default='predict')\n        Invokes the passed method name of the passed estimator.\n\n    Returns\n    -------\n    predictions : pd.DataFrame\n        The predictions obtained from cross-validation.\n\n    Notes\n    -----\n    This function is adapted for online learning scenarios and is designed to prevent cloning the initial estimator during cross-validation.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from sklearn.datasets import load_iris\n    &gt;&gt;&gt; from sklearn.linear_model import LogisticRegression\n    &gt;&gt;&gt; X, y = load_iris(return_X_y=True)\n    &gt;&gt;&gt; crossval_predict(LogisticRegression(), X, y, cv=3)\n\n    \"\"\"\n    # Le score rmse est ici \u00e9quivalent \u00e0 la valeur absolue de l'erreur\n    X, y, groups = indexable(X, y, groups)\n\n    cv = check_cv(cv, y, classifier=is_classifier(estimator))\n\n    # We clone the estimator to make sure that all the folds are\n    # independent, and that it is pickle-able.\n    parallel = Parallel(n_jobs=n_jobs, verbose=verbose, pre_dispatch=pre_dispatch)\n    prediction_blocks = parallel(\n        delayed(_fit_and_predict)(\n            clone(estimator),\n            X,\n            y,\n            train,\n            test,\n            verbose,\n            compute_confidence_interval,\n            bootstrap_size,\n            n_iterations_boostrap,\n            alpha,\n            fit_params,\n            method,\n        )\n        for train, test in cv.split(X, y, groups)\n    )\n\n    # Concatenate the predictions\n    predictions = [pred_block_i for pred_block_i, _ in prediction_blocks]\n    predictions = np.hstack(predictions)\n\n    # Wrap results in pd.DataFrame\n    test_indices = np.concatenate([indices_i for _, indices_i in prediction_blocks])\n    test_index = [y.index[_] for _ in test_indices]\n\n    return pd.DataFrame(np.transpose(predictions), index=test_index)\n</code></pre>"},{"location":"api/divide_by_total/","title":"divide_by_total","text":"<p>Divide values in the dataset by their respective total, based on specified groupby and divide-by columns.</p> <p>Parameters: - data_stat_des (pd.DataFrame): Input dataset containing statistics. - list_var_groupby (list of str): List of columns to group data by. - list_var_divide (list of str): List of columns used to identify the 'Total' or other reference rows. - list_var_of_interest (list of str): List of columns containing the values to be divided. - modality (str or dict, optional): Value used to identify the 'Total' or reference rows in the <code>list_var_divide</code> columns.                                      Default is 'Total'. If it's a dictionary, keys should be columns from <code>list_var_divide</code> and                                      values are the respective modalities to be used as reference for each column.</p> <p>Returns: - pd.DataFrame: The resulting dataset after dividing the specified values by their respective totals.</p> <p>Notes: This function is useful to compute relative statistics or proportions. Ensure that the <code>data_stat_des</code> does not contain NaN values in the specified columns, and the 'Total' or other reference rows are unique for each combination in <code>list_var_groupby</code>.</p> Source code in <code>igf_toolbox/utils/base.py</code> <pre><code>def divide_by_total(\n    data_stat_des: pd.DataFrame,\n    list_var_groupby: List[str],\n    list_var_divide: List[str],\n    list_var_of_interest: List[str],\n    modality: Optional[str] = \"Total\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Divide values in the dataset by their respective total, based on specified groupby and divide-by columns.\n\n    Parameters:\n    - data_stat_des (pd.DataFrame): Input dataset containing statistics.\n    - list_var_groupby (list of str): List of columns to group data by.\n    - list_var_divide (list of str): List of columns used to identify the 'Total' or other reference rows.\n    - list_var_of_interest (list of str): List of columns containing the values to be divided.\n    - modality (str or dict, optional): Value used to identify the 'Total' or reference rows in the `list_var_divide` columns.\n                                         Default is 'Total'. If it's a dictionary, keys should be columns from `list_var_divide` and\n                                         values are the respective modalities to be used as reference for each column.\n\n    Returns:\n    - pd.DataFrame: The resulting dataset after dividing the specified values by their respective totals.\n\n    Notes:\n    This function is useful to compute relative statistics or proportions. Ensure that the `data_stat_des` does not contain\n    NaN values in the specified columns, and the 'Total' or other reference rows are unique for each combination\n    in `list_var_groupby`.\n    \"\"\"\n    # Tol\u00e9rer aussi que modality soit un dictionnaire\n\n    # Le groupby ne doit pas \u00eatre en index\n\n    # Initialisation du jeu de donn\u00e9es r\u00e9sultat\n    data_res = data_stat_des.copy()\n\n    # Construction du jeu de donn\u00e9es par lequel diviser\n    data_condition = pd.concat(\n        [\n            data_res[col].apply(func=lambda x: True if x == modality else False)\n            for col in list_var_divide\n        ],\n        axis=1,\n        join=\"outer\",\n    )\n    data_divide = data_res.loc[\n        data_condition.all(axis=1), list_var_groupby + list_var_of_interest\n    ].set_index(list_var_groupby)\n\n    data_res = data_res.set_index(list_var_groupby + list_var_divide) / data_divide\n\n    return data_res\n</code></pre>"},{"location":"api/estimate_summarize/","title":"estimate_summarize","text":"<p>Estimate a regression model and return its summary and R-squared value.</p> <p>This function allows for optional transformation and exclusion of data before fitting the model. It also allows for optional weighting of observations.</p>"},{"location":"api/estimate_summarize/#igf_toolbox.model_selection.inference.estimate_summarize--parameters","title":"Parameters:","text":"<p>estimator : object     A regression estimator with fit, summary, and rsquared methods.</p> DataFrame <p>The feature matrix. Missing values in the DataFrame should be removed or imputed before passing to this function.</p> Series <p>The target variable.</p> TransformerMixin, default=None <p>An optional transformer object that has a fit_transform method. If provided, it will be used to transform X.</p> TransformerMixin, default=None <p>An optional column excluder object with a fit_transform method. If provided, it will be used to exclude certain columns from the data.</p> Series, default=None <p>Optional series of sample weights. If provided, these will be used to weight the observations during model fitting.</p>"},{"location":"api/estimate_summarize/#igf_toolbox.model_selection.inference.estimate_summarize--returns","title":"Returns:","text":"<p>tuple     A tuple containing:     - model summary (as returned by estimator.summary())     - R-squared value (as returned by estimator.rsquared())</p>"},{"location":"api/estimate_summarize/#igf_toolbox.model_selection.inference.estimate_summarize--examples","title":"Examples:","text":"<p>est = OLS(...) X = ... y = ... transformer = ... summary, r2 = estimate_summarize(est, X, y, transformer=transformer)</p> Source code in <code>igf_toolbox/model_selection/inference.py</code> <pre><code>def estimate_summarize(\n    estimator,\n    X: pd.DataFrame,\n    y: pd.Series,\n    transformer: Optional[Union[TransformerMixin, None]] = None,\n    excluder: Optional[Union[TransformerMixin, None]] = None,\n    sample_weight: Optional[Union[pd.Series, None]] = None,\n):\n    \"\"\"\n    Estimate a regression model and return its summary and R-squared value.\n\n    This function allows for optional transformation and exclusion of data\n    before fitting the model. It also allows for optional weighting of observations.\n\n    Parameters:\n    -----------\n    estimator : object\n        A regression estimator with fit, summary, and rsquared methods.\n\n    X : DataFrame\n        The feature matrix.\n        Missing values in the DataFrame should be removed or imputed before passing to this function.\n\n    y : Series\n        The target variable.\n\n    transformer : TransformerMixin, default=None\n        An optional transformer object that has a fit_transform method.\n        If provided, it will be used to transform X.\n\n    excluder : TransformerMixin, default=None\n        An optional column excluder object with a fit_transform method.\n        If provided, it will be used to exclude certain columns from the data.\n\n    sample_weight : Series, default=None\n        Optional series of sample weights. If provided, these will be used to weight\n        the observations during model fitting.\n\n    Returns:\n    --------\n    tuple\n        A tuple containing:\n        - model summary (as returned by estimator.summary())\n        - R-squared value (as returned by estimator.rsquared())\n\n    Examples:\n    ---------\n    &gt;&gt;&gt; est = OLS(...)\n    &gt;&gt;&gt; X = ...\n    &gt;&gt;&gt; y = ...\n    &gt;&gt;&gt; transformer = ...\n    &gt;&gt;&gt; summary, r2 = estimate_summarize(est, X, y, transformer=transformer)\n    \"\"\"\n\n    # Transformation des donn\u00e9es\n    if transformer is not None:\n        X = transformer.fit_transform(X.dropna())\n\n    # Appariement des donn\u00e9es et suppression des Nan\n    data_work = pd.concat([X, y.to_frame()], axis=1, join=\"inner\").dropna()\n\n    # Exclusion de certaines donn\u00e9es\n    if excluder is not None:\n        data_work = excluder.fit_transform(data_work)\n\n    if sample_weight is not None:\n        # Appariement avec les poids et suppression des Nan\n        data_work = pd.concat(\n            [data_work, sample_weight.loc[sample_weight &gt; 0].to_frame()],\n            axis=1,\n            join=\"inner\",\n        ).dropna()\n        # Initialisation et estimation du mod\u00e8le\n        model = estimator.fit(\n            X=data_work[X.columns],\n            y=data_work[y.name],\n            sample_weight=data_work[sample_weight.name],\n        )\n    else:\n        # Initialisation et estimation du mod\u00e8le\n        model = estimator.fit(X=data_work[X.columns], y=data_work[y.name])\n\n    # Description des r\u00e9sultats\n    return model.summary(), model.rsquared()\n</code></pre>"},{"location":"api/get_scalar_mappable/","title":"get_scalar_mappable","text":"<p>This function creates a ScalarMappable object from the IGF ColorMap, scaling it according to the minimum and maximum values in the data.</p> <p>Parameters: data (numpy.ndarray): A numpy array containing the data to be mapped.</p> <p>Returns: scalar_mappable (matplotlib.cm.ScalarMappable): A ScalarMappable object that can be used to map data values to colors using the IGF ColorMap.</p> <p>The function first defines the IGF ColorMap using the LinearSegmentedColormap class from matplotlib. It then determines the minimum and maximum values in the data. These values are used to normalize the ColorMap using the Normalize class from matplotlib. Finally, a ScalarMappable object is created using the normalized ColorMap and returned by the function.</p> Source code in <code>igf_toolbox/graphs/styles.py</code> <pre><code>def get_scalar_mappable(data: Union[pd.DataFrame, np.ndarray]) -&gt; ScalarMappable:\n    \"\"\"\n    This function creates a ScalarMappable object from the IGF ColorMap,\n    scaling it according to the minimum and maximum values in the data.\n\n    Parameters:\n    data (numpy.ndarray): A numpy array containing the data to be mapped.\n\n    Returns:\n    scalar_mappable (matplotlib.cm.ScalarMappable): A ScalarMappable object\n    that can be used to map data values to colors using the IGF ColorMap.\n\n    The function first defines the IGF ColorMap using the LinearSegmentedColormap\n    class from matplotlib. It then determines the minimum and maximum values\n    in the data. These values are used to normalize the ColorMap using the\n    Normalize class from matplotlib. Finally, a ScalarMappable object is\n    created using the normalized ColorMap and returned by the function.\n    \"\"\"\n    # D\u00e9finition de la ColorMap de l'IGF\n    cmap_igf = LinearSegmentedColormap.from_list(\n        \"charte\", [\"#096c45\", \"#737c24\", \"#d69a00\", \"#e17d18\", \"#9f0025\"], N=256\n    )\n\n    # D\u00e9finition des valeurs minimales et maximales des donn\u00e9es\n    value_min = data.min()\n    value_max = data.max()\n    # Normalisation des valeurs de la ColorMap\n    norm = Normalize(vmin=value_min, vmax=value_max)\n    # Cr\u00e9ation du ScalarMappable\n    scalar_mappable = ScalarMappable(norm=norm, cmap=cmap_igf)\n\n    return scalar_mappable\n</code></pre>"},{"location":"api/nest_groupby/","title":"nest_groupby","text":"<p>Refine a dataset based on successive groupby operations. If there is only one modality in addition to the \"Total\" in the previous groupby, only the \"Total\" row is preserved.</p> <p>Parameters: - data_stat_des (pd.DataFrame): Input dataset to refine. - list_var_groupby (list of str): List of columns to perform successive groupby operations on. - modality (str, optional): Reference modality to identify specific rows. Default is 'Total'.</p> <p>Returns: - pd.DataFrame: Refined dataset after successive groupby operations.</p> <p>Notes: This function helps in data summarization where, for each group defined by the previous groupby columns, if only one unique value exists in addition to the 'Total' or specified modality, then only the 'Total' or specified modality row is retained. Ensure that there are no NaN values in the specified columns of the input dataframe. The function makes use of a helper <code>StatDesGroupBy</code> class to perform the iterative groupby operations.</p> Source code in <code>igf_toolbox/stats_des/base.py</code> <pre><code>def nest_groupby(\n    data_stat_des: pd.DataFrame,\n    list_var_groupby: List[str],\n    modality: Optional[str] = \"Total\",\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Refine a dataset based on successive groupby operations. If there is only one modality in addition to the \"Total\" in the previous\n    groupby, only the \"Total\" row is preserved.\n\n    Parameters:\n    - data_stat_des (pd.DataFrame): Input dataset to refine.\n    - list_var_groupby (list of str): List of columns to perform successive groupby operations on.\n    - modality (str, optional): Reference modality to identify specific rows. Default is 'Total'.\n\n    Returns:\n    - pd.DataFrame: Refined dataset after successive groupby operations.\n\n    Notes:\n    This function helps in data summarization where, for each group defined by the previous groupby columns, if only one unique\n    value exists in addition to the 'Total' or specified modality, then only the 'Total' or specified modality row is retained.\n    Ensure that there are no NaN values in the specified columns of the input dataframe. The function makes use of a helper\n    `StatDesGroupBy` class to perform the iterative groupby operations.\n    \"\"\"\n    # Copie ind\u00e9pendante du jeu de donn\u00e9es\n    data_res = data_stat_des.copy()\n\n    if len(list_var_groupby) &gt; 1:\n        for i in range(1, len(list_var_groupby)):\n            # Restriction au sous ensemble de variable de groupby\n            list_var_groupby_work = list_var_groupby[:i]\n            # Identification de la variable d'int\u00e9r\u00eat\n            var_of_interest = list_var_groupby[i]\n            # Cr\u00e9ation d'un jeu de donn\u00e9es auxiliaire pour l'impl\u00e9mentation du crit\u00e8re\n            data_criteria = data_stat_des[\n                list_var_groupby_work + [var_of_interest]\n            ].copy()\n            # Ajout du test si la modalit\u00e9 est pr\u00e9sente dans le jeu de donn\u00e9es\n            data_criteria[\"is_modality\"] = data_criteria[var_of_interest].apply(\n                func=lambda x: True if x == modality else False\n            )\n            # D\u00e9nombrement des items de groupby et de pr\u00e9sence de la modalit\u00e9\n            # Initialisation du module de statistiques descriptives\n            data_source = data_criteria.copy()\n            list_var_of_interest = [var_of_interest, \"is_modality\"]\n            var_individu = None\n            var_entreprise = None\n            var_weights = None\n            iterable_operations = {\"nunique\": [var_of_interest], \"any\": [\"is_modality\"]}\n            stat_des_generator = StatDesGroupBy(\n                data_source=data_source,\n                list_var_groupby=list_var_groupby_work,\n                list_var_of_interest=list_var_of_interest,\n                var_individu=var_individu,\n                var_entreprise=var_entreprise,\n                var_weights=var_weights,\n            )\n            data_stat_aux = (\n                stat_des_generator.iterate_without_total(\n                    iterable_operations=iterable_operations\n                )\n                .reset_index()\n                .rename({\"index\": list_var_groupby_work[0]}, axis=1)\n            )\n            data_criteria = pd.merge(\n                left=data_criteria,\n                right=data_stat_aux,\n                on=list_var_groupby_work,\n                how=\"left\",\n                validate=\"many_to_one\",\n            )\n            # Restriction aux lignes d'int\u00e9r\u00eat\n            # Les index de data_criteria et data_res sont identiques du fait du left_merge\n            data_res.loc[\n                (data_criteria[var_of_interest + \"_nunique\"] == 2)\n                &amp; (data_criteria[\"is_modality_any\"]),\n                var_of_interest,\n            ] = modality\n    elif len(list_var_groupby) == 1:\n        if (data_res[list_var_groupby[0]].nunique() == 2) &amp; (\n            modality in data_res[list_var_groupby[0]]\n        ):\n            data_res[list_var_groupby[0]] = modality\n\n    # Supression des doublons en indice\n    data_res.drop_duplicates(subset=list_var_groupby, inplace=True)\n\n    return data_res\n</code></pre>"},{"location":"api/set_igf_style/","title":"set_igf_style","text":"<p>Sets the default style for all graphs according to igf chart</p> Source code in <code>igf_toolbox/graphs/styles.py</code> <pre><code>def set_igf_style() -&gt; None:\n    \"\"\"Sets the default style for all graphs according to igf chart\"\"\"\n\n    # Installation de la police Cambria\n    fontManager.addfont(\n        os.path.join(os.path.dirname(os.path.abspath(__file__)), \"Cambria.ttf\")\n    )\n    # fontManager.addfont(\"Cambria.ttf\")\n\n    # Figure size\n    plt.rcParams[\"figure.figsize\"] = (15, 7)\n\n    # Line plot styles\n    plt.rcParams[\"lines.linewidth\"] = 2\n    plt.rcParams[\"lines.markersize\"] = 8\n\n    # Axis labels and ticks\n    plt.rcParams[\"font.family\"] = \"Cambria\"\n    plt.rcParams[\"axes.labelsize\"] = 16\n    plt.rcParams[\"xtick.labelsize\"] = 16\n    plt.rcParams[\"ytick.labelsize\"] = 16\n\n    # Legend\n    plt.rcParams[\"legend.fontsize\"] = 16\n    plt.rcParams[\"legend.title_fontsize\"] = 16\n    plt.rcParams[\"legend.framealpha\"] = 0\n\n    plt.rcParams[\"legend.loc\"] = \"upper center\"\n    # add bboc8to_anchor=(0.5, -0.15) and ncol=... params when calling plt.legend(bboc_to)\n\n    # Remove top and right spines\n    plt.rcParams[\"axes.edgecolor\"] = \"black\"\n    plt.rcParams[\"axes.spines.top\"] = False\n    plt.rcParams[\"axes.spines.right\"] = False\n    plt.rcParams[\"axes.spines.left\"] = True\n    plt.rcParams[\"axes.spines.bottom\"] = True\n\n    # Set custom colormap\n    plt.rcParams[\"axes.prop_cycle\"] = cycler(\n        \"color\", [\"#096c45\", \"#737c24\", \"#d69a00\", \"#e17d18\", \"#9f0025\", \"#ae535c\"]\n    )\n</code></pre>"},{"location":"api/weighted_quantile/","title":"weighted_quantile","text":"<p>Compute the weighted quantile(s) for multiple variables of interest from the given dataset.</p> <p>Parameters: - data (pd.DataFrame): The input dataset containing the values and their respective weights. - vars_of_interest (list of str): List of column names in <code>data</code> representing the variables for which quantiles will be computed. - var_weights (str or None): Column name in <code>data</code> representing the weights of the values.                              If None, simple quantile will be computed for each variable of interest. - q (Number or array-like): Quantile or sequence of quantiles to compute, which must be between 0 and 1 inclusive.</p> <ul> <li>pd.Series: The computed weighted quantile(s) for each variable of interest. The index of the series corresponds to              the variables of interest, and the values are the computed quantiles.</li> </ul> <p>Notes: This function relies on the <code>_weighted_quantile_array</code> compiled function. Ensure that the <code>data</code> does not contain NaN values in the specified columns.</p> Source code in <code>igf_toolbox/stats_des/weighted.py</code> <pre><code>def weighted_quantile(\n    data: pd.DataFrame,\n    vars_of_interest: Union[List[str], str],\n    var_weights: Union[str, None],\n    q: Union[int, float, List[Union[int, float]]],\n) -&gt; pd.Series:\n    \"\"\"\n    Compute the weighted quantile(s) for multiple variables of interest from the given dataset.\n\n    Parameters:\n    - data (pd.DataFrame): The input dataset containing the values and their respective weights.\n    - vars_of_interest (list of str): List of column names in `data` representing the variables for which quantiles will be computed.\n    - var_weights (str or None): Column name in `data` representing the weights of the values.\n                                 If None, simple quantile will be computed for each variable of interest.\n    - q (Number or array-like): Quantile or sequence of quantiles to compute, which must be between 0 and 1 inclusive.\n\n    Returns:\n    - pd.Series: The computed weighted quantile(s) for each variable of interest. The index of the series corresponds to\n                 the variables of interest, and the values are the computed quantiles.\n\n    Notes:\n    This function relies on the `_weighted_quantile_array` compiled function.\n    Ensure that the `data` does not contain NaN values in the specified columns.\n    \"\"\"\n\n    # Retraitement des variables d'int\u00e9r\u00eat et conversion en liste\n    if isinstance(vars_of_interest, str):\n        vars_of_interest = [vars_of_interest]\n\n    # Retraitement des quantiles\n    if isinstance(q, Number):\n        q_work = np.array([q])\n    else:\n        q_work = np.asarray(q)\n\n    # Initialisation de la liste r\u00e9sultat\n    list_res = []\n    # Parcours des variables d'int\u00e9r\u00eat\n    for var_of_interest in vars_of_interest:\n        # Distinction de la m\u00e9thode a appliquer suivant que\n        if var_weights is not None:\n            res_value = _weighted_quantile_array(\n                array=data[[var_of_interest, var_weights]].values,\n                values_pos=0,\n                weights_pos=1,\n                q=q_work,\n            )\n            if len(res_value) == 1:\n                res_value = res_value[0]\n        else:\n            res_value = data[var_of_interest].quantile(q)\n        # Ajout \u00e0 la liste r\u00e9sultat\n        list_res.append(res_value)\n\n    return pd.Series(data=list_res, index=vars_of_interest)\n</code></pre>"},{"location":"api/weighted_std/","title":"weighted_std","text":"<p>Computes the weighted standard deviation for a given set of values.</p> <p>Parameters: - values (array-like): Input data for which the weighted standard deviation is computed. - axis (int, optional): Axis along which the weighted standard deviation is computed.     - If axis=0, compute the standard deviation for each column.     - If axis=1, compute the standard deviation for each row.     - Default is None, which computes the standard deviation of the flattened array. - weights (array-like, optional): An array of weights of the same shape as <code>values</code>. Default is None, which gives equal weight to all values.</p> <p>Returns: - pd.Series: Series containing the computed weighted standard deviation.</p> <p>Raises: - ValueError: If the input values contain NaN.</p> <p>Notes: The function expects no NaN values in the input. Ensure NaN values are handled before calling this function.</p> Source code in <code>igf_toolbox/stats_des/weighted.py</code> <pre><code>def weighted_std(\n    values: Union[np.ndarray, pd.DataFrame, pd.Series],\n    axis: Union[int, None] = None,\n    weights: Union[List[Union[int, float]], None] = None,\n) -&gt; pd.Series:\n    \"\"\"\n    Computes the weighted standard deviation for a given set of values.\n\n    Parameters:\n    - values (array-like): Input data for which the weighted standard deviation is computed.\n    - axis (int, optional): Axis along which the weighted standard deviation is computed.\n        - If axis=0, compute the standard deviation for each column.\n        - If axis=1, compute the standard deviation for each row.\n        - Default is None, which computes the standard deviation of the flattened array.\n    - weights (array-like, optional): An array of weights of the same shape as `values`. Default is None, which gives equal weight to all values.\n\n    Returns:\n    - pd.Series: Series containing the computed weighted standard deviation.\n\n    Raises:\n    - ValueError: If the input values contain NaN.\n\n    Notes:\n    The function expects no NaN values in the input. Ensure NaN values are handled before calling this function.\n    \"\"\"\n\n    if np.isnan(values).any():\n        raise ValueError(\n            \"Input values contain NaN. Please remove them before computing the weighted standard deviation.\"\n        )\n\n    # Calcul de la moyenne pond\u00e9r\u00e9e\n    average = np.average(values, axis=axis, weights=weights)\n    # Calcul de la d\u00e9viation standard\n    std = np.sqrt(\n        np.average(np.subtract(values, average) ** 2, axis=axis, weights=weights)\n    )\n    # Mise sous forme de Series\n    if axis == 0:\n        std = pd.Series(data=std, index=values.columns)\n    elif axis == 1:\n        std = pd.Series(data=std, index=values.index)\n\n    return std\n</code></pre>"}]}